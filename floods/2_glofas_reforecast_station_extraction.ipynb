{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49878cdb-9641-4730-8b1c-4e055b7ff377",
   "metadata": {},
   "source": [
    "Script to extract GloFAS reforecast data at station locations stored in an s3 bucket. Metadata file is used to identify which station points to extract (use Lisflood x and y coordinates if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a2f3091-fffe-4fca-971d-c7cd953b0962",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import dask\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from hip.analysis.compute.utils import persist_with_progress_bar\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20bae077-d6b2-4d65-90ac-ed55e3903e20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "country = 'mozambique'  # define country of interest\n",
    "#directory = Path(f'/s3/scratch/jamie.towner/flood_aa/{country}')  # define main working directory\n",
    "directory = Path(r\"C:\\Users\\15133\\Documents\\WFP\\flood_hazard\\flood_aa\\MOZ_training\")  # define main working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd13594b-de40-4cfc-b677-236ed8b29e77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15133\\AppData\\Local\\Temp\\ipykernel_12680\\3494039067.py:15: FutureWarning: In a future version of xarray decode_timedelta will default to False rather than None. To silence this warning, set decode_timedelta to True, False, or a 'CFTimedeltaCoder' instance.\n",
      "  ds = xr.open_mfdataset(store, decode_coords=\"all\", engine=\"zarr\")\n",
      "C:\\Users\\15133\\AppData\\Local\\Temp\\ipykernel_12680\\3494039067.py:15: FutureWarning: In a future version of xarray decode_timedelta will default to False rather than None. To silence this warning, set decode_timedelta to True, False, or a 'CFTimedeltaCoder' instance.\n",
      "  ds = xr.open_mfdataset(store, decode_coords=\"all\", engine=\"zarr\")\n"
     ]
    }
   ],
   "source": [
    "# # Set up the S3 path for the Zarr files\n",
    "# store = f\"s3://wfp-seasmon/input/cds/glofas-reforecast/saf/*/*.zarr\"\n",
    "\n",
    "# # Set up connection to s3 store\n",
    "# s3 = s3fs.S3FileSystem.current()\n",
    "\n",
    "# # Define mapper object for multiple files\n",
    "# remote_files = s3.glob(store)\n",
    "# store = [s3fs.S3Map(root=f\"s3://{file}\", s3=s3, check=False) for file in remote_files]\n",
    "\n",
    "store = list(Path.glob(directory / \"data/forecasts/glofas_reforecasts/\", \"*.zarr\"))\n",
    "\n",
    "# Configure dask to avoid creating large chunks and open the dataset\n",
    "with dask.config.set(**{\"array.slicing.split_large_chunks\": True}):\n",
    "    ds = xr.open_mfdataset(store, decode_coords=\"all\", engine=\"zarr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dfe0e26-b8b8-4626-a3ea-35af1ae7342a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the CSV file containing station information (i.e., station name, lat, lon)\n",
    "# define paths to data\n",
    "station_info = pd.read_csv(directory / \"data/metadata/metadata_observations.csv\")\n",
    "\n",
    "# select only chokwe station for training\n",
    "station_info = station_info[station_info['station name'] == 'Limpopo_em_Chokwe']\n",
    "\n",
    "# Define the output directory\n",
    "out_dir = directory / \"data/forecasts/glofas_reforecasts/stations\"\n",
    "Path(out_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b53067d4-f09d-484b-9a6e-640dc57e1cc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Data:   0%|                                                                           | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 234.67 s\n",
      "Data at point dimensions: ('number', 'time', 'step')\n",
      "Data at point shape: (11, 1052, 46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Data: 100%|██████████████████████████████████████████████████████████████████| 1/1 [03:59<00:00, 239.55s/it]\n"
     ]
    }
   ],
   "source": [
    "# Initialize tqdm progress bar\n",
    "pbar = tqdm(total=len(station_info), desc=\"Extracting Data\")\n",
    "\n",
    "# Loop over each station in the metadata file\n",
    "for index, row in station_info.iterrows():\n",
    "\n",
    "    point_name = row['station name']\n",
    "    latitude = row['lisflood_y']\n",
    "    longitude = row['lisflood_x']\n",
    "    \n",
    "    if np.isnan(latitude) or np.isnan(longitude):\n",
    "        latitude = row['latitude']\n",
    "        longitude = row['longitude']\n",
    "    \n",
    "    # Sanitize the station name for use in a file path (e.g., remove spaces, special characters)\n",
    "    station_name = \"\".join(c for c in point_name if c.isalnum() or c in (' ', '_')).replace(' ', '_')\n",
    "    \n",
    "    # Define expected output file path\n",
    "    nc_file_name = os.path.join(out_dir, f\"{station_name}.nc\")\n",
    "\n",
    "    # Skip if file already exists\n",
    "    if os.path.exists(nc_file_name):\n",
    "        print(f\"Skipping {station_name} (already exists)\")\n",
    "        pbar.update(1)\n",
    "        continue\n",
    "\n",
    "    # Find the nearest latitude and longitude in the dataset\n",
    "    lat_index = ds['latitude'].sel(latitude=latitude, method='nearest')\n",
    "    lon_index = ds['longitude'].sel(longitude=longitude, method='nearest')\n",
    "\n",
    "    # Extract river discharge data for the nearest point and for all ensemble members\n",
    "    data_at_point = persist_with_progress_bar(ds['dis24'].sel(latitude=lat_index, longitude=lon_index))\n",
    "\n",
    "    # Print the actual dimensions for diagnostics\n",
    "    print(\"Data at point dimensions:\", data_at_point.dims)\n",
    "    print(\"Data at point shape:\", data_at_point.shape)\n",
    "\n",
    "    # Extract dates, ensemble member, and step\n",
    "    dates = ds.time.values\n",
    "    ensemble_members = ds['dis24'].coords['number'].values\n",
    "    steps = ds.step.values\n",
    "\n",
    "    # Create a new xarray dataset for storing data in NetCDF format\n",
    "    station_ds = xr.Dataset(\n",
    "        {\n",
    "            \"dis24\": ([\"number\", \"time\", \"step\"], data_at_point.values)\n",
    "        },\n",
    "        coords={\n",
    "            \"number\": ensemble_members,\n",
    "            \"time\": dates,\n",
    "            \"step\": steps\n",
    "        },\n",
    "        attrs={\n",
    "            \"description\": f\"River discharge forecasts for {point_name}\",\n",
    "            \"units\": \"m^3/s\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Save the dataset as a NetCDF file\n",
    "    nc_file_name = os.path.join(out_dir, f\"{station_name}.nc\")\n",
    "    station_ds.to_netcdf(nc_file_name)\n",
    "\n",
    "    # Update the progress bar\n",
    "    pbar.update(1)\n",
    "\n",
    "# Close the progress bar\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bdc366-2dc1-429c-b1e5-670296d1cddc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Pixi)",
   "language": "python",
   "name": "pixi-kernel-python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
