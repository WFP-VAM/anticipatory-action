{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49878cdb-9641-4730-8b1c-4e055b7ff377",
   "metadata": {},
   "source": [
    "Script to extract GloFAS reforecast data at station locations stored in an s3 bucket. Metadata file is used to identify which station points to extract (use Lisflood x and y coordinates if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a2f3091-fffe-4fca-971d-c7cd953b0962",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import dask\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from hip.analysis.compute.utils import persist_with_progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20bae077-d6b2-4d65-90ac-ed55e3903e20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "country = 'mozambique'  # define country of interest\n",
    "directory = '/s3/scratch/jamie.towner/flood_aa'  # define main working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd13594b-de40-4cfc-b677-236ed8b29e77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the S3 path for the Zarr files\n",
    "store = f\"s3://wfp-seasmon/input/cds/glofas-reforecast/saf/*/*.zarr\"\n",
    "\n",
    "# Set up connection to s3 store\n",
    "s3 = s3fs.S3FileSystem.current()\n",
    "\n",
    "# Define mapper object for multiple files\n",
    "remote_files = s3.glob(store)\n",
    "store = [s3fs.S3Map(root=f\"s3://{file}\", s3=s3, check=False) for file in remote_files]\n",
    "\n",
    "# Configure dask to avoid creating large chunks and open the dataset\n",
    "with dask.config.set(**{\"array.slicing.split_large_chunks\": True}):\n",
    "    ds = xr.open_mfdataset(store, decode_coords=\"all\", engine=\"zarr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dfe0e26-b8b8-4626-a3ea-35af1ae7342a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the CSV file containing station information (i.e., station name, lat, lon)\n",
    "# define paths to data\n",
    "metadata_directory = os.path.join(directory, country, \"data/metadata\")\n",
    "station_info_file = \"metadata_observations.csv\"\n",
    "station_info_path = os.path.join(metadata_directory, station_info_file)\n",
    "station_info = pd.read_csv(station_info_path)\n",
    "\n",
    "# Define the output directory\n",
    "out_dir = os.path.join(directory, country, \"data/forecasts/glofas_reforecasts/stations\")\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53067d4-f09d-484b-9a6e-640dc57e1cc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize tqdm progress bar\n",
    "pbar = tqdm(total=len(station_info), desc=\"Extracting Data\")\n",
    "\n",
    "# Loop over each station in the metadata file\n",
    "for index, row in station_info.iterrows():\n",
    "\n",
    "    point_name = row['station name']\n",
    "    latitude = row['lisflood_y']\n",
    "    longitude = row['lisflood_x']\n",
    "    \n",
    "    if np.isnan(latitude) or np.isnan(longitude):\n",
    "        latitude = row['latitude']\n",
    "        longitude = row['longitude']\n",
    "    \n",
    "    # Sanitize the station name for use in a file path (e.g., remove spaces, special characters)\n",
    "    station_name = \"\".join(c for c in point_name if c.isalnum() or c in (' ', '_')).replace(' ', '_')\n",
    "    \n",
    "    # Define expected output file path\n",
    "    nc_file_name = os.path.join(out_dir, f\"{station_name}.nc\")\n",
    "\n",
    "    # Skip if file already exists\n",
    "    if os.path.exists(nc_file_name):\n",
    "        print(f\"Skipping {station_name} (already exists)\")\n",
    "        pbar.update(1)\n",
    "        continue\n",
    "\n",
    "    # Find the nearest latitude and longitude in the dataset\n",
    "    lat_index = ds['latitude'].sel(latitude=latitude, method='nearest')\n",
    "    lon_index = ds['longitude'].sel(longitude=longitude, method='nearest')\n",
    "\n",
    "    # Extract river discharge data for the nearest point and for all ensemble members\n",
    "    data_at_point = persist_with_progress_bar(ds['dis24'].sel(latitude=lat_index, longitude=lon_index))\n",
    "\n",
    "    # Print the actual dimensions for diagnostics\n",
    "    print(\"Data at point dimensions:\", data_at_point.dims)\n",
    "    print(\"Data at point shape:\", data_at_point.shape)\n",
    "\n",
    "    # Extract dates, ensemble member, and step\n",
    "    dates = ds.time.values\n",
    "    ensemble_members = ds['dis24'].coords['number'].values\n",
    "    steps = ds.step.values\n",
    "\n",
    "    # Create a new xarray dataset for storing data in NetCDF format\n",
    "    station_ds = xr.Dataset(\n",
    "        {\n",
    "            \"dis24\": ([\"number\", \"time\", \"step\"], data_at_point.values)\n",
    "        },\n",
    "        coords={\n",
    "            \"number\": ensemble_members,\n",
    "            \"time\": dates,\n",
    "            \"step\": steps\n",
    "        },\n",
    "        attrs={\n",
    "            \"description\": f\"River discharge forecasts for {point_name}\",\n",
    "            \"units\": \"m^3/s\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Save the dataset as a NetCDF file\n",
    "    nc_file_name = os.path.join(out_dir, f\"{station_name}.nc\")\n",
    "    station_ds.to_netcdf(nc_file_name)\n",
    "\n",
    "    # Update the progress bar\n",
    "    pbar.update(1)\n",
    "\n",
    "# Close the progress bar\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118f650-42c3-41f1-a30d-69177e2c3086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hdc",
   "language": "python",
   "name": "conda-env-hdc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
