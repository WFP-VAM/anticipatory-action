# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.16.1
#   kernelspec:
#     display_name: Python 3 (ipykernel)
#     language: python
#     name: python3
# ---

# %% [markdown] id="UTbokLLWp_9o"
# # Introduction
#
# This colab facilitates reading the Google Runoff Reanalysis & Reforecast dataset (GRRR). The dataset consists of hydrologic predictions by Google's SOTA hydrologic model, an improved version of the model described in the recent [Nature publication](https://www.nature.com/articles/s41586-024-07145-1), with full global coverage (based on the HydroBasins dataset) in daily resolution.
#
# It includes:
# *   Streamflow reanalysis for over 1M hybas locations, for 1980-2023.
# *   Streamflow reforecasts for over 1M hybas locations, for 2016-2022, with lead-times 0 to 7 days.
# *   Return period values based on the reanalysis data, which can be used as severity threshold levels.
#
# The dataset is available under a CC-BY-4.0 license: https://creativecommons.org/licenses/by/4.0/
#
# ## Indexing
# The dataset is indexed by "gauge_id", which refers to a specific HydroSheds basin (representing *all* area that drains to the basins outlet, and not just the sub-basin area).
#
# For each basin, the reanalysis includes a time series (indexed by "time"). The time T refers to the average daily streamflow at the outlet of the modeled basin (in the range [T, T+1d]; 1d=one day). In other words, the dataset is "left labeled".
#
# The reforecasts are indexed by "issue_time" and "lead_time". The reforecast at issue-time T should be thought of as the forecast that would be generated at the instant time T (the issue time) by the model, if it had all input information that was available until that moment. More specifically, the model receives CPC and IMERG precipitation estimates that refer to times no later than T, and uses HRES and GraphCast weather forecasts that were issued until time T. We ignore the realistic delay of several hours in obtaining the data which exists in an operational setting.
#
# The "lead_time" index (which is a time-delta of 0, 1, ..., 7 days) refers to the end of the forecasted daily window. So the reforecasted value at issue_time=T, lead-time=L, refers to the daily discharge averaged over the time period [T+L-1d, T+L]. In other words, the lead_time index is "right-labeled".
#
# Following the last definitions, note the following relationship of the reanalysis and reforecast datasets:
#
# Reanalysis[time=T] == Reforecast[time=T+1, lead_time=0]
#
# ## Appendix - basin outlets
# As an appendix, we include location estimates to the outlet points of the modeled basins. Note that the predictions apply to specific points in rivers, as opposed to many global model approaches where the model output is linked to a pixel of several square kilometers. See more information in the appendix below.

# %% [markdown] id="3EBT09_DoleZ"
# # Setup

# %% id="mQzGeIqM2y0p" outputId="6fab4982-c8e5-431f-fa82-315ee6557217" colab={"base_uri": "https://localhost:8080/"}
# !pip install zarr xarray

# %% id="l5KyZE7E1h6h"
import datetime
import os
import pandas as pd
import xarray as xr
import matplotlib.pyplot as plt


# %% [markdown] id="wANq7C_Ua4qG"
# # Appendix - HydroBasins outlet locations
#
# We add a mapping from hybas id to its lat-long location. This data was generated by Prof. Bernhard Lehner, the creator of the HydroSheds dataset, but is not an official part of the dataset as of today (October 2024). We add it as a means to help users of Google's Streamflow dataset find the approximate location for which the predictions apply, which are the outlet points of the respective HydroBasins watershed.
#
# **Important Note** For coastal basins, the outlet is not as well defined as in non-coastal basins. The dataset generated Prof. Lehner includes many possible  outlet points in such cases. For simplicity, we arbitrarily select one of them. Please use those with even more caution, and only as an approximate estimate to the location to the point for which the hydrologic prediction is produced.

# %% id="Rbho7CibFzEt"
def open_zarr(path):
  return xr.open_zarr(
      store=path,
      chunks=None,
      storage_options=dict(token='anon')
      )


# %% id="9FX9S-BqdlqB" outputId="4672594e-d7c3-4916-c19c-a68a478886dd" colab={"base_uri": "https://localhost:8080/", "height": 223}
base_directory = 'gs://flood-forecasting/hydrologic_predictions/model_id_8583a5c2_v0/'

outlets_path = os.path.join(base_directory, 'hybas_outlet_locations_UNOFFICIAL.zarr/')
outlets_ds = open_zarr(outlets_path)
outlets_ds

# %% id="EKdlsS8vhwww"
import numpy as np
from pyproj import Proj, transform
from scipy.spatial.distance import cdist

# %% id="mOy4hogr3_q-" outputId="821e73ab-f6bb-4953-aa4f-8c03757bff7e" colab={"base_uri": "https://localhost:8080/", "height": 455}
df = outlets_ds.to_pandas()
df


# %% id="jVDYc6ZD4Cbe"
def get_gauge_id(x, y, df):
    # get the closest data point to the clicked point on map
    # convert from web mercator to epsg:4326
    #x, y = transform(Proj(init="epsg:3857"), Proj(init="epsg:4326"), x, y)

    distance = cdist(
        df[["longitude", "latitude"]], [[x, y]], metric="euclidean"
    )
    x, y = df[["longitude", "latitude"]].to_numpy()[distance.argmin(axis=0)][0]

    id = df[(df['longitude']==x) & (df['latitude']==y)].index.item()

    return x, y, id



# %% [markdown] id="jnUzm6_F4XhM"
# ### get google ids of stations of interest from their lat lon locations

# %% id="yuAkhrba4Ii-" outputId="2b1fb454-30ad-4be0-d585-41437f96c54a" colab={"base_uri": "https://localhost:8080/"}
for lat,lon in [[-22.22,29.98],[-21.07,30.38],[-16.74,32.29],[-19.22,32.02],[-20.05,31.04],[-21.13,31.27],[-21.13,31.52],[-19.9,32.13],[-18.4,32.78],[-20.27,32.67],[-18.22,33.03]]:
  x,y,id = get_gauge_id(lon, lat, df)
  print(x,y,id)

# %% id="JQXuVFM-FUD7"
reforecast_path = os.path.join(base_directory, 'reforecast/streamflow.zarr/')
reanalysis_path = os.path.join(base_directory, 'reanalysis/streamflow.zarr/')
return_periods_path = os.path.join(base_directory, 'return_periods.zarr/')

# %% colab={"base_uri": "https://localhost:8080/"} id="Afjkwfv-FucF" outputId="dd77a14b-0b73-4145-b8ba-1036a617f729"
reforecast_ds = open_zarr(reforecast_path)
reanalysis_ds = open_zarr(reanalysis_path)
return_periods_ds = open_zarr(return_periods_path)

print(f'{reforecast_ds.sizes = }')
print(f'{reanalysis_ds.sizes = }')
print(f'{return_periods_ds.sizes = }')

# %% [markdown] id="M-OFNUCrGFT2"
# # Visualize some hybas_id

# %% id="Nla5GKP6FIeV" outputId="ac7ddb53-95af-4266-c138-88ef00026230" colab={"base_uri": "https://localhost:8080/", "height": 223}
hybas_ids = ['hybas_1121548490','hybas_1121530890','hybas_1122213940','hybas_1121502180','hybas_1121515070','hybas_1121532060',
             'hybas_1121532310','hybas_1121512000','hybas_1122227230','hybas_1122240700','hybas_1122226070'] # Danube, Vienna
gauge_reforecast_ds = reforecast_ds.sel(gauge_id=hybas_ids).compute()
gauge_reanalysis_ds = reanalysis_ds.sel(gauge_id=hybas_ids).compute()
gauge_return_periods_ds = return_periods_ds.sel(gauge_id=hybas_ids).compute()
gauge_reanalysis_ds

# %% id="9g6YSfhX46TT" outputId="07c89b3a-aa96-49f8-fd41-4ee1f9b76084" colab={"base_uri": "https://localhost:8080/", "height": 455}
data_reanalysis = gauge_reanalysis_ds.to_dataframe()
data_reforecast = gauge_reforecast_ds.to_dataframe()
data_return_periods = gauge_return_periods_ds.to_dataframe()
data_reanalysis

# %% id="Yw7kG0NU5Bpt"
from google.colab import files

# %% id="ALoUqpha5BtE" outputId="42cd6390-5468-40ba-f6ce-bb9da48cbdc9" colab={"base_uri": "https://localhost:8080/", "height": 17}
data_reanalysis.to_csv('ZWE_google_reanalysis.csv')
files.download('ZWE_google_reanalysis.csv')

data_reforecast.to_csv('ZWE_google_reforecast.csv')
files.download('ZWE_google_reforecast.csv')

data_return_periods.to_csv('ZWE_google_return_periods.csv')
files.download('ZWE_google_return_periods.csv')
