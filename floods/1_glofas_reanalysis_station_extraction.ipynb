{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "138d0500-3f61-462b-94dc-4b86d3f30f0f",
   "metadata": {},
   "source": [
    "Script to extract GloFAS reanalysis data at station locations stored in an s3 bucket. Metadata file is used to identify which station points to extract (use Lisflood x and y coordinates if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58d1358f-e55f-48c3-98a2-2c2302e82ccb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import dask\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6d2b84c-7d8b-479a-b5ed-19302848c37a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "country = 'zimbabwe'  # define country of interest\n",
    "directory = '/s3/scratch/jamie.towner/flood_aa'  # define main working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd9bec4a-06f2-4d85-a712-f0b539cf789f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the S3 path for the Zarr files\n",
    "store = f\"s3://wfp-seasmon/input/cds/glofas-historical/saf/01/*.zarr\"\n",
    "\n",
    "# Set up connection to s3 store\n",
    "s3 = s3fs.S3FileSystem.current()\n",
    "\n",
    "# Fetch list of .zarr stores (files)\n",
    "remote_files = s3.glob(store)\n",
    "store = [\n",
    "    s3fs.S3Map(root=f\"s3://{file}\", s3=s3, check=False) for file in remote_files\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b554a8cb-61d0-456f-99b4-48890331fc92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Data:   0%|          | 0/630 [01:14<?, ?it/s]\u001b[A\n",
      "\n",
      "Extracting Data:   7%|▋         | 45/630 [00:43<09:19,  1.05it/s]\u001b[A\n",
      "Extracting Data:  14%|█▍        | 90/630 [00:57<05:17,  1.70it/s]\u001b[A\n",
      "Extracting Data:  21%|██▏       | 135/630 [01:12<03:49,  2.16it/s]\u001b[A\n",
      "Extracting Data:  29%|██▊       | 180/630 [01:25<03:00,  2.49it/s]\u001b[A\n",
      "Extracting Data:  36%|███▌      | 225/630 [01:39<02:29,  2.72it/s]\u001b[A\n",
      "Extracting Data:  43%|████▎     | 270/630 [01:53<02:05,  2.87it/s]\u001b[A\n",
      "Extracting Data:  50%|█████     | 315/630 [02:07<01:45,  2.99it/s]\u001b[A\n",
      "Extracting Data:  57%|█████▋    | 360/630 [02:21<01:27,  3.08it/s]\u001b[A\n",
      "Extracting Data:  64%|██████▍   | 405/630 [02:34<01:11,  3.13it/s]\u001b[A\n",
      "Extracting Data:  71%|███████▏  | 450/630 [02:48<00:56,  3.17it/s]\u001b[A\n",
      "Extracting Data:  79%|███████▊  | 495/630 [03:02<00:42,  3.20it/s]\u001b[A\n",
      "Extracting Data:  86%|████████▌ | 540/630 [03:16<00:28,  3.21it/s]\u001b[A\n",
      "Extracting Data:  93%|█████████▎| 585/630 [03:30<00:13,  3.22it/s]\u001b[A\n",
      "Extracting Data: 100%|██████████| 630/630 [03:43<00:00,  2.81it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file containing station information (i.e., station name, lat, lon)\n",
    "# define paths to data\n",
    "metadata_directory = os.path.join(directory, country, \"data/metadata\")\n",
    "station_info_file = \"metadata_observations.csv\"\n",
    "station_info_path = os.path.join(metadata_directory, station_info_file)\n",
    "station_info = pd.read_csv(station_info_path)\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "out_dir = os.path.join(directory, country, \"data/forecasts/glofas_reanalysis\")\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "# Initialize a dictionary to store data for each station\n",
    "station_data = {}\n",
    "\n",
    "# Initialize tqdm with the total number of iterations to track progress\n",
    "total_iterations = len(remote_files) * len(station_info)\n",
    "pbar = tqdm(total=total_iterations, desc=\"Extracting Data\")\n",
    "\n",
    "# Open multiple .zarr files with dask and xarray, setting chunk configuration\n",
    "with dask.config.set(**{\"array.slicing.split_large_chunks\": True}):\n",
    "    ds = xr.open_mfdataset(\n",
    "        store,\n",
    "        decode_coords=\"all\",\n",
    "        engine=\"zarr\",\n",
    "        parallel=True,  # Enable parallel processing for speed-up\n",
    "        combine=\"by_coords\"\n",
    "    )\n",
    "\n",
    "    # Loop over each station in the station_info CSV\n",
    "    for index, row in station_info.iterrows():\n",
    "        point_name = row['station name']\n",
    "        latitude = row['latitude']\n",
    "        longitude = row['longitude']\n",
    "\n",
    "        # Replace 'lat' and 'lon' with 'latitude' and 'longitude'\n",
    "        lat_index = ds['latitude'].sel(latitude=latitude, method='nearest').values\n",
    "        lon_index = ds['longitude'].sel(longitude=longitude, method='nearest').values\n",
    "\n",
    "        # Extract river discharge data for the nearest point\n",
    "        data_at_point = ds['dis24'].sel(latitude=lat_index, longitude=lon_index).values\n",
    "        dates = ds.time.values\n",
    "\n",
    "        # Convert dates to DD/MM/YYYY format\n",
    "        formatted_dates = pd.to_datetime(dates).strftime('%d/%m/%Y')\n",
    "\n",
    "        # Create a DataFrame for the extracted data\n",
    "        extracted_df = pd.DataFrame({'date': formatted_dates, 'river discharge': data_at_point})\n",
    "\n",
    "        # Append the data to the station's DataFrame within the station_data dictionary\n",
    "        if point_name not in station_data:\n",
    "            station_data[point_name] = extracted_df\n",
    "        else:\n",
    "            # Merge with the existing data for the same station\n",
    "            station_data[point_name] = pd.concat([station_data[point_name], extracted_df])\n",
    "        \n",
    "        pbar.update(len(remote_files))  # Update tqdm progress by number of files processed\n",
    "\n",
    "# Close the tqdm progress bar\n",
    "pbar.close()\n",
    "\n",
    "# Save extracted data for each station to CSV files\n",
    "for station, data in station_data.items():\n",
    "    csv_file_name = os.path.join(out_dir, f\"{station}.csv\")\n",
    "    data.to_csv(csv_file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc36032-5fde-4cf8-8a34-2c2cf1527464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa-env",
   "language": "python",
   "name": "conda-env-aa-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
