{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3549cdc8-4dde-43ea-b719-4f516c17734c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Preseason flood anticipatory action script to process observed/reanalysis and \n",
    "forecast data, calculate contigency metrics and choose the 'best' trigger for \n",
    "operational use based on quality criteria (e.g., f1 score, hit rate and false alarm rates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f64bf4d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import relevant packages \n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03603971",
   "metadata": {
    "cell_marker": "#######################################################"
   },
   "source": [
    "Section 1: Define variables, paths and read in data \n",
    "\n",
    "In this section we begin by defining our country of interest, our main working directory and whether we are using observed or reanalysis data. We then define the paths to load the data and perform visual checks to ensure the data is what we are expecting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fa48348-a091-4695-aaae-65aa18fae052",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "country = 'mozambique'  # define country of interest\n",
    "directory = '/s3/scratch/jamie.towner/flood_aa'  # define main working directory\n",
    "benchmark = 'observations'  # choose 'observations' or 'glofas_reanalysis' as the benchmark\n",
    "\n",
    "# define paths to data\n",
    "forecast_data_directory = os.path.join(directory, country, \"data/forecasts/glofas_reforecasts\")\n",
    "metadata_directory = os.path.join(directory, country, \"data/metadata\")\n",
    "output_directory = os.path.join(directory, country, \"outputs/triggers\")\n",
    "\n",
    "# create output directory if it does not exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# set observed data and metadata directory and filenames based on benchmark choice\n",
    "if benchmark == 'observations':\n",
    "    observed_data_directory = os.path.join(directory, country, \"data/observations/gauging_stations/all_stations\")\n",
    "    observed_data_file = \"observations.csv\"\n",
    "    station_info_file = \"metadata_observations.csv\"\n",
    "elif benchmark == 'glofas_reanalysis':\n",
    "    observed_data_directory = os.path.join(directory, country, \"data/forecasts/glofas_reanalysis/all_stations\")\n",
    "    observed_data_file = \"glofas_reanalysis.csv\"\n",
    "    station_info_file = 'metadata_glofas_reanalysis.csv'\n",
    "else:\n",
    "    raise ValueError(\"invalid benchmark choice. choose 'observations' or 'glofas_reanalysis'.\")\n",
    "\n",
    "# load the observed or reanalysis data and gauging stations metadata\n",
    "observed_data_path = os.path.join(observed_data_directory, observed_data_file)\n",
    "station_info_path = os.path.join(metadata_directory, station_info_file)\n",
    "\n",
    "observed_data = pd.read_csv(observed_data_path)\n",
    "station_info = pd.read_csv(station_info_path)\n",
    "# format station name\n",
    "station_info['station name'] = [\"\".join(c for c in name if c.isalnum() or c in (' ', '_')).replace(' ', '_') for name in station_info['station name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be228527-43db-49c2-800a-1883d30ccec5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Limpopo_em_Mapai</th>\n",
       "      <th>Limpopo__Combomune</th>\n",
       "      <th>Limpopo_em_Chokwe</th>\n",
       "      <th>Limpopo_em_Sicacate</th>\n",
       "      <th>Limpopo_em_Mabalane</th>\n",
       "      <th>Limpopo_em_XaiXai</th>\n",
       "      <th>Changane_em_Chibuto</th>\n",
       "      <th>Limpopo_em_Macaretane</th>\n",
       "      <th>Zambeze_em_Marromeu_Sena_Sugar</th>\n",
       "      <th>Chire_em_Vila_Bocage</th>\n",
       "      <th>Chire_em_megaza_Mutamba</th>\n",
       "      <th>Zambeze_em_Caia_SS</th>\n",
       "      <th>Revubue_em_Chingodzi</th>\n",
       "      <th>Zambeze_em_Zumbo</th>\n",
       "      <th>Zambeze_em_Tete</th>\n",
       "      <th>LuenhaLuenha_I</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.88</td>\n",
       "      <td>2.150000</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>1.305</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96.870000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.023333</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.423333</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-01-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.51</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2.160000</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>1.150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96.880000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.063333</td>\n",
       "      <td>2.033333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.383333</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003-01-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2.170000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>1.330</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96.903333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.080000</td>\n",
       "      <td>2.246667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.496667</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003-01-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.53</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2.166667</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>1.610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96.906667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.886667</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003-01-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2.170000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>1.710</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96.920000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.066667</td>\n",
       "      <td>4.290000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.316667</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Limpopo_em_Mapai  Limpopo__Combomune  Limpopo_em_Chokwe  \\\n",
       "0  2003-01-01               NaN                1.50               0.88   \n",
       "1  2003-01-02               NaN                1.51               0.89   \n",
       "2  2003-01-03               NaN                1.52               0.89   \n",
       "3  2003-01-04               NaN                1.53               0.89   \n",
       "4  2003-01-05               NaN                1.54               0.89   \n",
       "\n",
       "   Limpopo_em_Sicacate  Limpopo_em_Mabalane  Limpopo_em_XaiXai  \\\n",
       "0             2.150000             0.930000              1.305   \n",
       "1             2.160000             0.926667              1.150   \n",
       "2             2.170000             0.920000              1.330   \n",
       "3             2.166667             0.920000              1.610   \n",
       "4             2.170000             0.910000              1.710   \n",
       "\n",
       "   Changane_em_Chibuto  Limpopo_em_Macaretane  Zambeze_em_Marromeu_Sena_Sugar  \\\n",
       "0                  NaN              96.870000                             NaN   \n",
       "1                  NaN              96.880000                             NaN   \n",
       "2                  NaN              96.903333                             NaN   \n",
       "3                  NaN              96.906667                             NaN   \n",
       "4                  NaN              96.920000                             NaN   \n",
       "\n",
       "   Chire_em_Vila_Bocage  Chire_em_megaza_Mutamba  Zambeze_em_Caia_SS  \\\n",
       "0                   NaN                      NaN            3.023333   \n",
       "1                   NaN                      NaN            3.063333   \n",
       "2                   NaN                      NaN            3.080000   \n",
       "3                   NaN                      NaN            3.100000   \n",
       "4                   NaN                      NaN            3.066667   \n",
       "\n",
       "   Revubue_em_Chingodzi  Zambeze_em_Zumbo  Zambeze_em_Tete  LuenhaLuenha_I  \n",
       "0              2.100000               NaN         2.423333             NaN  \n",
       "1              2.033333               NaN         2.383333             NaN  \n",
       "2              2.246667               NaN         2.496667             NaN  \n",
       "3              5.250000               NaN         2.886667             NaN  \n",
       "4              4.290000               NaN         3.316667             NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the observed data \n",
    "observed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e840a37-0cc4-47ce-829e-0943e5a67503",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station id</th>\n",
       "      <th>station name</th>\n",
       "      <th>google</th>\n",
       "      <th>river name</th>\n",
       "      <th>river basin</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>lisflood_x</th>\n",
       "      <th>lisflood_y</th>\n",
       "      <th>obs_bankfull</th>\n",
       "      <th>obs_moderate</th>\n",
       "      <th>obs_severe</th>\n",
       "      <th>glofas_bankfull</th>\n",
       "      <th>glofas_moderate</th>\n",
       "      <th>glofas_severe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>Limpopo em Mapai</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Limpopo</td>\n",
       "      <td>Limpopo</td>\n",
       "      <td>-22.850000</td>\n",
       "      <td>31.950000</td>\n",
       "      <td>31.925</td>\n",
       "      <td>-22.825</td>\n",
       "      <td>4.365000</td>\n",
       "      <td>5.550667</td>\n",
       "      <td>6.377333</td>\n",
       "      <td>985.165442</td>\n",
       "      <td>4256.715563</td>\n",
       "      <td>6128.158339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>Limpopo - Combomune</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Limpopo</td>\n",
       "      <td>Limpopo</td>\n",
       "      <td>-23.416667</td>\n",
       "      <td>32.433333</td>\n",
       "      <td>32.425</td>\n",
       "      <td>-23.475</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>7.452000</td>\n",
       "      <td>8.418000</td>\n",
       "      <td>383.495024</td>\n",
       "      <td>4238.880464</td>\n",
       "      <td>5839.625824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35</td>\n",
       "      <td>Limpopo em Chokwe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Limpopo</td>\n",
       "      <td>Limpopo</td>\n",
       "      <td>-24.502778</td>\n",
       "      <td>33.006944</td>\n",
       "      <td>33.025</td>\n",
       "      <td>-24.525</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.110000</td>\n",
       "      <td>7.674667</td>\n",
       "      <td>1187.666264</td>\n",
       "      <td>4797.012057</td>\n",
       "      <td>6356.340579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36</td>\n",
       "      <td>Limpopo em Sicacate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Limpopo</td>\n",
       "      <td>Limpopo</td>\n",
       "      <td>-24.744444</td>\n",
       "      <td>33.543056</td>\n",
       "      <td>33.525</td>\n",
       "      <td>-24.725</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>10.174000</td>\n",
       "      <td>10.632000</td>\n",
       "      <td>426.607727</td>\n",
       "      <td>2594.874032</td>\n",
       "      <td>3953.142422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>Limpopo em Mabalane</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Limpopo</td>\n",
       "      <td>Limpopo</td>\n",
       "      <td>-23.847222</td>\n",
       "      <td>32.583333</td>\n",
       "      <td>32.575</td>\n",
       "      <td>-23.825</td>\n",
       "      <td>5.483333</td>\n",
       "      <td>7.938667</td>\n",
       "      <td>8.404333</td>\n",
       "      <td>668.175784</td>\n",
       "      <td>3046.140923</td>\n",
       "      <td>4815.956713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station id         station name  google river name river basin   latitude  \\\n",
       "0          32     Limpopo em Mapai     NaN    Limpopo     Limpopo -22.850000   \n",
       "1          33  Limpopo - Combomune     NaN    Limpopo     Limpopo -23.416667   \n",
       "2          35    Limpopo em Chokwe     NaN    Limpopo     Limpopo -24.502778   \n",
       "3          36  Limpopo em Sicacate     NaN    Limpopo     Limpopo -24.744444   \n",
       "4          37  Limpopo em Mabalane     NaN    Limpopo     Limpopo -23.847222   \n",
       "\n",
       "   longitude  lisflood_x  lisflood_y  obs_bankfull  obs_moderate  obs_severe  \\\n",
       "0  31.950000      31.925     -22.825      4.365000      5.550667    6.377333   \n",
       "1  32.433333      32.425     -23.475      4.500000      7.452000    8.418000   \n",
       "2  33.006944      33.025     -24.525      5.000000      7.110000    7.674667   \n",
       "3  33.543056      33.525     -24.725      6.000000     10.174000   10.632000   \n",
       "4  32.583333      32.575     -23.825      5.483333      7.938667    8.404333   \n",
       "\n",
       "   glofas_bankfull  glofas_moderate  glofas_severe  \n",
       "0       985.165442      4256.715563    6128.158339  \n",
       "1       383.495024      4238.880464    5839.625824  \n",
       "2      1187.666264      4797.012057    6356.340579  \n",
       "3       426.607727      2594.874032    3953.142422  \n",
       "4       668.175784      3046.140923    4815.956713  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the metadata\n",
    "station_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffde08a8-6f9d-4363-a453-811e25329712",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert the date column in observed_data to pandas timestamps \n",
    "observed_data[\"date\"] = pd.to_datetime(observed_data[\"date\"], format='mixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0bf5c33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load all GloFAS forecast files (there should be 1052 files per gauging station)\n",
    "forecast_files = glob.glob(os.path.join(forecast_data_directory, '*.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8aabcc4-55c8-4749-a696-9a600ed53633",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/s3/scratch/jamie.towner/flood_aa/mozambique/data/forecasts/glofas_reforecasts/Changane_em_Chibuto_2003_10_19.nc',\n",
       " '/s3/scratch/jamie.towner/flood_aa/mozambique/data/forecasts/glofas_reforecasts/Changane_em_Chibuto_2003_10_12.nc',\n",
       " '/s3/scratch/jamie.towner/flood_aa/mozambique/data/forecasts/glofas_reforecasts/Changane_em_Chibuto_2003_03_27.nc',\n",
       " '/s3/scratch/jamie.towner/flood_aa/mozambique/data/forecasts/glofas_reforecasts/Changane_em_Chibuto_2003_10_05.nc',\n",
       " '/s3/scratch/jamie.towner/flood_aa/mozambique/data/forecasts/glofas_reforecasts/Changane_em_Chibuto_2003_10_09.nc',\n",
       " '/s3/scratch/jamie.towner/flood_aa/mozambique/data/forecasts/glofas_reforecasts/Changane_em_Chibuto_2003_10_16.nc',\n",
       " '/s3/scratch/jamie.towner/flood_aa/mozambique/data/forecasts/glofas_reforecasts/Changane_em_Chibuto_2003_10_02.nc',\n",
       " '/s3/scratch/jamie.towner/flood_aa/mozambique/data/forecasts/glofas_reforecasts/Changane_em_Chibuto_2003_03_30.nc',\n",
       " '/s3/scratch/jamie.towner/flood_aa/mozambique/data/forecasts/glofas_reforecasts/Changane_em_Chibuto_2003_10_23.nc',\n",
       " '/s3/scratch/jamie.towner/flood_aa/mozambique/data/forecasts/glofas_reforecasts/Changane_em_Chibuto_2003_10_30.nc']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check forecast files have loaded as expected \n",
    "forecast_files[:10] # check 10 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4700c6e-a83a-494a-84a9-fb99b4753fb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "forecast directory: /s3/scratch/jamie.towner/flood_aa/mozambique/data/forecasts/glofas_reforecasts\n",
      "observed data directory: /s3/scratch/jamie.towner/flood_aa/mozambique/data/observations/gauging_stations/all_stations\n",
      "observed data file: observations_newstations.csv\n",
      "metadata directory: /s3/scratch/jamie.towner/flood_aa/mozambique/data/metadata\n",
      "output directory: /s3/scratch/jamie.towner/flood_aa/mozambique/outputs/triggers\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print data paths to ensure they are set correctly\n",
    "print(f\"\"\"\n",
    "forecast directory: {forecast_data_directory}\n",
    "observed data directory: {observed_data_directory}\n",
    "observed data file: {observed_data_file}\n",
    "metadata directory: {metadata_directory}\n",
    "output directory: {output_directory}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1fb0e2",
   "metadata": {
    "cell_marker": "##############################################################################"
   },
   "source": [
    "Section 2: Process observations and forecasts and define events/non-events \n",
    "\n",
    "In this section we begin by matching the station names in the forecast files from those in the metadata and then process one forecast file at a time. We should have 1052 forecast files per station analysed. To process a station we need the name to be in both the station_info file (i.e., the metadata) and the naming convention of the forecast files. A message will highlight if this is not the case and the code will skip that particular gauging station. We then extract the variable of the forecast data which is river discharge (m3/s) and each ensemble member (we have 11 for GloFAS reforecasts and there will be 51 operationally). After we define the forecast issue date from the forecast netcdf and calculate the forecast end date based on lead time. Remember that due to the indexing of Python from 0 we add 1 to each end date. \n",
    "\n",
    "Looking at the metadata you can see that we have three thresholds for both observations and forecasts. These are bankfull, moderate and severe. The bankfull represents the point at which a gauging station begins to flood and is the readiness phase of the system (i.e., no anticipatory action will take place). The moderate and severe thresholds are based on the 5 and 10 year return periods of the observed data. We then use quantile mapping to map these observed thresholds to the GloFAS reanalysis product over the same time period (2003-2023). This in effect performs a bias correction on the forecasts. \n",
    "\n",
    "Finally, for each threshold we simply identify if each ensemble members of a forecast for each specific lead time exceeds the forecast thresholds and we do the same for the observations. If there is an exceedance we assign a 1 or TRUE and if not we assign a 0 or FALSE. We end this section by creating a dataframe which displays these results called events_df. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "310eb8d3",
   "metadata": {
    "lines_to_next_cell": 1,
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing forecast files: 100%|██████████| 16832/16832 [34:57<00:00,  8.02it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing complete.\n"
     ]
    }
   ],
   "source": [
    "# create an empty list to store events/non-events \n",
    "events = []\n",
    "\n",
    "# filter station_info (i.e., the metadata) to include only stations present in the forecast_files\n",
    "# extract station names from forecast filenames\n",
    "station_names_in_files = [os.path.basename(file).split(\"_\")[0] for file in forecast_files]\n",
    "# get unique station names\n",
    "unique_station_names = list(set(station_names_in_files))\n",
    "# convert station names to lowercase if required\n",
    "filtered_station_info = station_info[station_info[\"station name\"].str.lower().isin([name.lower() for name in unique_station_names])]\n",
    "\n",
    "# loop over each forecast file \n",
    "for forecast_file in tqdm(forecast_files, desc=\"processing forecast files\"):\n",
    "    # load the netcdf file\n",
    "    ds = xr.open_dataset(forecast_file, decode_timedelta=True)\n",
    "    \n",
    "    # extract the station name from the filename\n",
    "    station_name = os.path.basename(forecast_file)[:-14] #os.path.basename(forecast_file).split(\"_\")[0].lower()\n",
    "\n",
    "    # process only the station that matches the current forecast file\n",
    "    #station_row = filtered_station_info[filtered_station_info[\"station name\"].str.lower() == station_name]\n",
    "    station_row = station_info[station_info[\"station name\"] == station_name]\n",
    "\n",
    "    if station_row.empty:\n",
    "        print(f\"Skipping {station_name}: no matching station info found.\")\n",
    "        continue  # skip if station is not found in the metadata\n",
    "\n",
    "    station_row = station_row.iloc[0]  # convert to series since we expect only one row\n",
    "    \n",
    "    # extract all ensemble members \n",
    "    ensemble_data = ds['dis24']  # shape: (number, step)\n",
    "    ensemble_members = ensemble_data['number'].values  # extract ensemble member IDs\n",
    "\n",
    "    # extract the forecast issue date from the file and convert to pandas datetime\n",
    "    forecast_issue_ns = ds['time'].values.item()  \n",
    "    forecast_issue_date = pd.to_datetime(forecast_issue_ns, unit='ns')\n",
    "\n",
    "    # define the lead times up to 46 days ahead (lead time = 0 is actually lead time =1 in reality)\n",
    "    lead_times = list(range(0, 7))  # adjust to match desired lead times\n",
    "\n",
    "    # extract individual station thresholds from metadata file\n",
    "    thresholds = {\n",
    "        \"bankfull\": (station_row[\"obs_bankfull\"], station_row[\"glofas_bankfull\"]),\n",
    "        \"moderate\": (station_row[\"obs_moderate\"], station_row[\"glofas_moderate\"]),\n",
    "        \"severe\": (station_row[\"obs_severe\"], station_row[\"glofas_severe\"]),\n",
    "    }\n",
    "        \n",
    "    # process each lead time\n",
    "    for lead_time in lead_times:\n",
    "            # calculate the forecast end date based on lead time (adds one day due to python indexing from zero)\n",
    "            forecast_end_date = forecast_issue_date + pd.DateOffset(days=lead_time + 1)\n",
    "            \n",
    "            # filter observed data for the matching period\n",
    "            observed_period = observed_data[observed_data[\"date\"] == str(forecast_end_date)[:10]]\n",
    "            \n",
    "            # skip if no observation data is available for this period\n",
    "            if observed_period.empty:\n",
    "                continue\n",
    "\n",
    "            observed_values = observed_period[station_name].values[0]\n",
    "\n",
    "            # skip if there's no observation data (NaN value) for the specific station\n",
    "            if pd.isnull(observed_values):\n",
    "                continue\n",
    "\n",
    "            # extract forecast values for all ensemble members at the current lead time\n",
    "            forecast_data = ensemble_data.isel(step=lead_time).values.squeeze()  # remove extra dimensions\n",
    "            \n",
    "            # **debug check**: ensure the shape of forecast_data is correct\n",
    "            if forecast_data.ndim != 1 or forecast_data.shape[0] != len(ensemble_members):\n",
    "                raise ValueError(f\"unexpected shape for forecast_data: {forecast_data.shape}. expected ({len(ensemble_members)},).\")\n",
    "\n",
    "            # loop over the thresholds\n",
    "            for severity, (obs_threshold, forecast_threshold) in thresholds.items():\n",
    "                # define events and non-events for each ensemble member\n",
    "                observed_event = observed_values > obs_threshold\n",
    "                forecast_event = forecast_data > forecast_threshold     \n",
    "                \n",
    "                # create events dictionaries for all ensemble members at once\n",
    "                for member_idx, ensemble_member in enumerate(ensemble_members):\n",
    "                    events_dict = {\n",
    "                        \"forecast file\": os.path.basename(forecast_file),\n",
    "                        \"lead time\": lead_time,\n",
    "                        \"station name\": station_name,\n",
    "                        \"ensemble member\": ensemble_member,\n",
    "                        \"forecasted date\": forecast_end_date.date(),\n",
    "                        \"threshold\": severity,\n",
    "                        \"observed event\": observed_event,\n",
    "                        \"forecast event\": bool(forecast_event[member_idx]),  \n",
    "                    }\n",
    "                    # append the events dictionary to the list\n",
    "                    events.append(events_dict)\n",
    "\n",
    "# create a data frame from the list of event dictionaries\n",
    "events_df = pd.DataFrame(events)\n",
    "print(\"processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3b9740e-746b-4005-8798-b904fd40a7ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>forecast file</th>\n",
       "      <th>lead time</th>\n",
       "      <th>station name</th>\n",
       "      <th>ensemble member</th>\n",
       "      <th>forecasted date</th>\n",
       "      <th>threshold</th>\n",
       "      <th>observed event</th>\n",
       "      <th>forecast event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Changane_em_Chibuto_2007_10_09.nc</td>\n",
       "      <td>0</td>\n",
       "      <td>Changane_em_Chibuto</td>\n",
       "      <td>0</td>\n",
       "      <td>2007-10-10</td>\n",
       "      <td>bankfull</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Changane_em_Chibuto_2007_10_09.nc</td>\n",
       "      <td>0</td>\n",
       "      <td>Changane_em_Chibuto</td>\n",
       "      <td>1</td>\n",
       "      <td>2007-10-10</td>\n",
       "      <td>bankfull</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Changane_em_Chibuto_2007_10_09.nc</td>\n",
       "      <td>0</td>\n",
       "      <td>Changane_em_Chibuto</td>\n",
       "      <td>2</td>\n",
       "      <td>2007-10-10</td>\n",
       "      <td>bankfull</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Changane_em_Chibuto_2007_10_09.nc</td>\n",
       "      <td>0</td>\n",
       "      <td>Changane_em_Chibuto</td>\n",
       "      <td>3</td>\n",
       "      <td>2007-10-10</td>\n",
       "      <td>bankfull</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Changane_em_Chibuto_2007_10_09.nc</td>\n",
       "      <td>0</td>\n",
       "      <td>Changane_em_Chibuto</td>\n",
       "      <td>4</td>\n",
       "      <td>2007-10-10</td>\n",
       "      <td>bankfull</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       forecast file  lead time         station name  \\\n",
       "0  Changane_em_Chibuto_2007_10_09.nc          0  Changane_em_Chibuto   \n",
       "1  Changane_em_Chibuto_2007_10_09.nc          0  Changane_em_Chibuto   \n",
       "2  Changane_em_Chibuto_2007_10_09.nc          0  Changane_em_Chibuto   \n",
       "3  Changane_em_Chibuto_2007_10_09.nc          0  Changane_em_Chibuto   \n",
       "4  Changane_em_Chibuto_2007_10_09.nc          0  Changane_em_Chibuto   \n",
       "\n",
       "   ensemble member forecasted date threshold  observed event  forecast event  \n",
       "0                0      2007-10-10  bankfull           False           False  \n",
       "1                1      2007-10-10  bankfull           False           False  \n",
       "2                2      2007-10-10  bankfull           False           False  \n",
       "3                3      2007-10-10  bankfull           False           False  \n",
       "4                4      2007-10-10  bankfull           False           False  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print events_df to check output is as expected \n",
    "events_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "033bfcb2-3d29-473a-a60f-93f44fc3c764",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "events_df.to_csv('events_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4e6c24",
   "metadata": {
    "cell_marker": "####################################################################################"
   },
   "source": [
    "Section 3: Create a function to construct contigency table and skill score metrics\n",
    "\n",
    "In this section we create a function called calculate_metrics which counts the number of hits, misses, false alarms and correct rejections before calculating metrics such as the hit and false alarm rate, critical success index (CSI) and f1 score. These metrics will help us determine how good the forecast is at detecting floods events when compared to the observed or reanalysis datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ace3a15",
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to calculate verification metrics \n",
    "def calculate_metrics(df):\n",
    "    hits, false_alarms, misses, correct_rejections = {}, {}, {}, {}\n",
    "    hit_rate, false_alarm_rate, csi, f1_score = {}, {}, {}, {}\n",
    "\n",
    "    # loop through all \"trigger\" columns\n",
    "    for column in [col for col in df.columns if 'trigger' in col]:\n",
    "        obs_1, obs_0 = df['observed event'] == 1, df['observed event'] == 0\n",
    "        fcst_1, fcst_0 = df[column] == 1, df[column] == 0\n",
    "\n",
    "        # calculate contingency table elements\n",
    "        hits[column] = (obs_1 & fcst_1).sum()\n",
    "        false_alarms[column] = (obs_0 & fcst_1).sum()\n",
    "        misses[column] = (obs_1 & fcst_0).sum()\n",
    "        correct_rejections[column] = (obs_0 & fcst_0).sum()\n",
    "\n",
    "        # compute verification metrics\n",
    "        total_observed_events = hits[column] + misses[column]\n",
    "        total_forecasted_events = hits[column] + false_alarms[column]\n",
    "\n",
    "        hit_rate[column] = hits[column] / total_observed_events if total_observed_events > 0 else 0\n",
    "        false_alarm_rate[column] = false_alarms[column] / total_forecasted_events if total_forecasted_events > 0 else 0\n",
    "        csi[column] = hits[column] / (hits[column] + false_alarms[column] + misses[column]) if (hits[column] + false_alarms[column] + misses[column]) > 0 else 0\n",
    "        \n",
    "        precision = hits[column] / total_forecasted_events if total_forecasted_events > 0 else 0\n",
    "        recall = hit_rate[column]\n",
    "        f1_score[column] = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    # convert metrics dictionaries into dataframes\n",
    "    metrics_df = pd.concat([\n",
    "        pd.DataFrame(hits, index=['hits']),\n",
    "        pd.DataFrame(false_alarms, index=['false_alarms']),\n",
    "        pd.DataFrame(misses, index=['misses']),\n",
    "        pd.DataFrame(correct_rejections, index=['correct_rejections']),\n",
    "        pd.DataFrame(hit_rate, index=['hit_rate']),\n",
    "        pd.DataFrame(false_alarm_rate, index=['false_alarm_rate']),\n",
    "        pd.DataFrame(csi, index=['csi']),\n",
    "        pd.DataFrame(f1_score, index=['f1_score']),\n",
    "    ])\n",
    "\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfb2f16",
   "metadata": {
    "cell_marker": "############################################################"
   },
   "source": [
    "Section 4: Grouping by lead-time and calculating metrics\n",
    "\n",
    "In this section we begin by pivoting the events_df created in Section 2 so that the ensemble members are displayed as columns before calculating the probability of each forecast by summing the number of 1's and divding by the total number of ensemble members (11 in our case). Once we have the probability of each forecast we remove any forecasts where there is already a flood observed (i.e., observed event = 1 or TRUE) on the first lead time (i.e., lead time = 0). We do this as we do not want to include forecasts in the analysis where there is already a flood occuring or where a flood is imminent. It's important to note here that if a forecast is removed, we remove all lead times associated with that forecast. We only remove the forecast for the threshold where there is an observed event (i.e., if the bankfull threshold is exceeded on lead_time = 0, but not for the moderate threshold then we keep the forecasts for the moderate and severe thresholds only. We do this in order to see if the flood event gets progressively worse. \n",
    "\n",
    "We then filter our events_df based on the lead-times to a particular grouping which balances the need for 2-3 days lead time for anticipatory action along with the limited period in which we have forecast skill (e.g., 2-5 days, 3-5 days, 3-6 days etc.). We then move onto grouping the large events_df by station, threshold and per forecast file into more managable small dataframes. Here, we have for one station, threshold and forecast file a list of probabilities for each of the lead-times and a binary TRUE or FALSE classification in the observed_event column for each lead-time. We now classify events and non-events based on if there is a 1 or TRUE in any of the lead times filtered to for the observations. While for the forecasted events we take the mean of each probability. When this is complete we finish by adding triggers ranging from 0.01 to 1.0 and run the calcualte_metrics function over each dataframe now groupedby station_name and threshold. We groupby these variables so we account for all forecast files for a given station and threshold. This provides us the contigency scores and verification metrics for each station which we evaluate in the final section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2649da7f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of forecasts removed: 4348\n",
      "number of unique forecasts removed: 631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_284/2068608389.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  events_df_lead['lead'] = f'{lead_start}-{lead_stop} days'\n",
      "/tmp/ipykernel_284/2068608389.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  events_df_lead['lead'] = f'{lead_start}-{lead_stop} days'\n",
      "/tmp/ipykernel_284/2068608389.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  events_df_lead['lead'] = f'{lead_start}-{lead_stop} days'\n",
      "/tmp/ipykernel_284/2068608389.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  events_df_lead['lead'] = f'{lead_start}-{lead_stop} days'\n",
      "/tmp/ipykernel_284/2068608389.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  events_df_lead['lead'] = f'{lead_start}-{lead_stop} days'\n",
      "/tmp/ipykernel_284/2068608389.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  events_df_lead['lead'] = f'{lead_start}-{lead_stop} days'\n"
     ]
    }
   ],
   "source": [
    "# pivot the events_df to list ensemble members as columns\n",
    "events_df = events_df.pivot_table(\n",
    "    index=[\"forecast file\", \"lead time\", \"station name\", \"forecasted date\",\"threshold\", \"observed event\"],\n",
    "    columns=\"ensemble member\",\n",
    ")\n",
    "\n",
    "# reset index to convert the pivoted dataframe to a flat table structure\n",
    "events_df.reset_index(inplace=True)\n",
    "\n",
    "# define the columns corresponding to the forecast ensemble members\n",
    "ensemble_member_columns = [col for col in events_df.columns if col[0] == \"forecast event\"]\n",
    "\n",
    "# calculate the percentage of ensemble members with events (i.e., 1's) for each row\n",
    "events_df[\"probability\"] = events_df[ensemble_member_columns].sum(axis=1) / len(ensemble_member_columns)\n",
    "\n",
    "# check if the columns of events_df have a multi index\n",
    "if isinstance(events_df.columns, pd.MultiIndex):\n",
    "    # flatten the multi index into strings\n",
    "    events_df.columns = [' '.join(map(str, col)).strip() for col in events_df.columns]\n",
    "\n",
    "# remove columns with \"forecast event\" in their names\n",
    "events_df = events_df.loc[:, ~events_df.columns.str.contains('forecast event')]\n",
    "\n",
    "# identify forecasts where lead_time = 0 and observed event = true (i.e., where flooding is already occuring)\n",
    "forecasts_to_remove = events_df[(events_df['lead time'] == 0) & (events_df['observed event'])][['forecast file', 'station name', 'threshold']].drop_duplicates()\n",
    "\n",
    "# filter out all rows with the same forecast file, station name, and threshold for any lead time\n",
    "filtered_events_df = events_df[~events_df[['forecast file', 'station name', 'threshold']].apply(tuple, axis=1).isin(forecasts_to_remove.apply(tuple, axis=1))]\n",
    "\n",
    "# get the number of forecasts removed including lead time\n",
    "removed_forecasts_count = len(events_df) - len(filtered_events_df)\n",
    "print(f\"number of forecasts removed: {removed_forecasts_count}\")\n",
    "\n",
    "# get the number of forecasts removed excluding lead time\n",
    "removed_forecasts_unique_count = forecasts_to_remove.drop_duplicates(subset=['forecast file', 'station name', 'threshold']).shape[0]\n",
    "\n",
    "print(f\"number of unique forecasts removed: {removed_forecasts_unique_count}\")\n",
    "\n",
    "# assign back to the original variable\n",
    "events_df = filtered_events_df\n",
    "\n",
    "all_dfs = []\n",
    "# test multiple lead times\n",
    "for lead_start, lead_stop in [[3,5],[3,6],[3,7],[2,5],[2,6],[2,7]]:\n",
    "    # filter the dataframe to include specific lead times\n",
    "    events_df_lead = events_df[(events_df['lead time'] >=lead_start) & (events_df['lead time'] <=lead_stop)]\n",
    "    events_df_lead['lead'] = f'{lead_start}-{lead_stop} days'\n",
    "    \n",
    "    # group by station name, lead time category, and threshold\n",
    "    grouped = events_df_lead.groupby(['forecast file','station name','threshold','lead'], observed=False)\n",
    "\n",
    "    # create a dictionary to store each group's dataframe\n",
    "    grouped_dfs = {name: group for name, group in grouped}\n",
    "\n",
    "    # dictionary to store processed data\n",
    "    new_grouped_dfs = {}\n",
    "\n",
    "    # calculate events and non-events in the lead time period (i.e., flood event if any observed value in period is a 1, take the mean probability for the forecast data)\n",
    "    for name, df in grouped_dfs.items():\n",
    "        first_row = df.iloc[0]\n",
    "\n",
    "        new_grouped_dfs[name] = pd.DataFrame({\n",
    "            'forecast file': [first_row['forecast file']],\n",
    "            'station name': [first_row['station name']],\n",
    "            'threshold': [first_row['threshold']],\n",
    "            'lead time': [first_row['lead']],\n",
    "            'observed event': [(df['observed event'] == 1).any()],\n",
    "            'probability': [df['probability'].mean()],\n",
    "        })\n",
    "\n",
    "    # combine all resulting dataframe into one \n",
    "    final_df = pd.concat(new_grouped_dfs.values(), ignore_index=True)\n",
    "\n",
    "    # add trigger thresholds ranging from 1-100% \n",
    "    trigger_columns = {}\n",
    "\n",
    "    for trigger in np.arange(0.01, 1.01, 0.01): \n",
    "        event_occurrence = (final_df['probability'] >= trigger).astype(int)\n",
    "        trigger_columns[f'trigger{trigger:.2f}'] = event_occurrence\n",
    "\n",
    "    # concatanate the new trigger columns to the dataframe\n",
    "    final_df = pd.concat([final_df, pd.DataFrame(trigger_columns, index=final_df.index)], axis=1)\n",
    "\n",
    "    # group by station name, lead time category, and threshold\n",
    "    grouped = final_df.groupby(['station name','threshold','lead time'], observed=False)\n",
    "\n",
    "    # create a dictionary to store each group's dataframe\n",
    "    grouped_dfs = {name: group for name, group in grouped}\n",
    "\n",
    "    # iterate through each dataframe in grouped_dfs, apply calculate_metrics, and store the results back into grouped_dfs\n",
    "    for key, df in grouped_dfs.items():\n",
    "        grouped_dfs[key] = calculate_metrics(df)\n",
    "        \n",
    "    all_dfs.append(grouped_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61ebc7fd-e519-4f94-b3fc-5caecc38a819",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_dfs_combine = [pd.concat(dfs, ignore_index=False) for dfs in all_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aae8dc73-da82-45ff-8ade-72e7990d98f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>trigger0.01</th>\n",
       "      <th>trigger0.02</th>\n",
       "      <th>trigger0.03</th>\n",
       "      <th>trigger0.04</th>\n",
       "      <th>trigger0.05</th>\n",
       "      <th>trigger0.06</th>\n",
       "      <th>trigger0.07</th>\n",
       "      <th>trigger0.08</th>\n",
       "      <th>trigger0.09</th>\n",
       "      <th>trigger0.10</th>\n",
       "      <th>...</th>\n",
       "      <th>trigger0.91</th>\n",
       "      <th>trigger0.92</th>\n",
       "      <th>trigger0.93</th>\n",
       "      <th>trigger0.94</th>\n",
       "      <th>trigger0.95</th>\n",
       "      <th>trigger0.96</th>\n",
       "      <th>trigger0.97</th>\n",
       "      <th>trigger0.98</th>\n",
       "      <th>trigger0.99</th>\n",
       "      <th>trigger1.00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Changane_em_Chibuto</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">bankfull</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">3-5 days</th>\n",
       "      <th>hits</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>false_alarms</th>\n",
       "      <td>110.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>misses</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correct_rejections</th>\n",
       "      <td>283.000000</td>\n",
       "      <td>283.000000</td>\n",
       "      <td>283.000000</td>\n",
       "      <td>291.000000</td>\n",
       "      <td>291.000000</td>\n",
       "      <td>291.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>352.000000</td>\n",
       "      <td>352.000000</td>\n",
       "      <td>352.000000</td>\n",
       "      <td>352.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit_rate</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Zambeze_em_Zumbo</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">severe</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">2-7 days</th>\n",
       "      <th>correct_rejections</th>\n",
       "      <td>558.000000</td>\n",
       "      <td>562.000000</td>\n",
       "      <td>562.000000</td>\n",
       "      <td>565.000000</td>\n",
       "      <td>565.000000</td>\n",
       "      <td>565.000000</td>\n",
       "      <td>565.000000</td>\n",
       "      <td>567.000000</td>\n",
       "      <td>567.000000</td>\n",
       "      <td>568.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>573.000000</td>\n",
       "      <td>573.000000</td>\n",
       "      <td>573.000000</td>\n",
       "      <td>573.000000</td>\n",
       "      <td>573.000000</td>\n",
       "      <td>573.000000</td>\n",
       "      <td>573.000000</td>\n",
       "      <td>573.000000</td>\n",
       "      <td>573.000000</td>\n",
       "      <td>573.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit_rate</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>false_alarm_rate</th>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>csi</th>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2304 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          trigger0.01  \\\n",
       "Changane_em_Chibuto bankfull 3-5 days hits                   5.000000   \n",
       "                                      false_alarms         110.000000   \n",
       "                                      misses                 0.000000   \n",
       "                                      correct_rejections   283.000000   \n",
       "                                      hit_rate               1.000000   \n",
       "...                                                               ...   \n",
       "Zambeze_em_Zumbo    severe   2-7 days correct_rejections   558.000000   \n",
       "                                      hit_rate               0.500000   \n",
       "                                      false_alarm_rate       0.944444   \n",
       "                                      csi                    0.052632   \n",
       "                                      f1_score               0.100000   \n",
       "\n",
       "                                                          trigger0.02  \\\n",
       "Changane_em_Chibuto bankfull 3-5 days hits                   5.000000   \n",
       "                                      false_alarms         110.000000   \n",
       "                                      misses                 0.000000   \n",
       "                                      correct_rejections   283.000000   \n",
       "                                      hit_rate               1.000000   \n",
       "...                                                               ...   \n",
       "Zambeze_em_Zumbo    severe   2-7 days correct_rejections   562.000000   \n",
       "                                      hit_rate               0.500000   \n",
       "                                      false_alarm_rate       0.928571   \n",
       "                                      csi                    0.066667   \n",
       "                                      f1_score               0.125000   \n",
       "\n",
       "                                                          trigger0.03  \\\n",
       "Changane_em_Chibuto bankfull 3-5 days hits                   5.000000   \n",
       "                                      false_alarms         110.000000   \n",
       "                                      misses                 0.000000   \n",
       "                                      correct_rejections   283.000000   \n",
       "                                      hit_rate               1.000000   \n",
       "...                                                               ...   \n",
       "Zambeze_em_Zumbo    severe   2-7 days correct_rejections   562.000000   \n",
       "                                      hit_rate               0.500000   \n",
       "                                      false_alarm_rate       0.928571   \n",
       "                                      csi                    0.066667   \n",
       "                                      f1_score               0.125000   \n",
       "\n",
       "                                                          trigger0.04  \\\n",
       "Changane_em_Chibuto bankfull 3-5 days hits                   5.000000   \n",
       "                                      false_alarms         102.000000   \n",
       "                                      misses                 0.000000   \n",
       "                                      correct_rejections   291.000000   \n",
       "                                      hit_rate               1.000000   \n",
       "...                                                               ...   \n",
       "Zambeze_em_Zumbo    severe   2-7 days correct_rejections   565.000000   \n",
       "                                      hit_rate               0.500000   \n",
       "                                      false_alarm_rate       0.909091   \n",
       "                                      csi                    0.083333   \n",
       "                                      f1_score               0.153846   \n",
       "\n",
       "                                                          trigger0.05  \\\n",
       "Changane_em_Chibuto bankfull 3-5 days hits                   5.000000   \n",
       "                                      false_alarms         102.000000   \n",
       "                                      misses                 0.000000   \n",
       "                                      correct_rejections   291.000000   \n",
       "                                      hit_rate               1.000000   \n",
       "...                                                               ...   \n",
       "Zambeze_em_Zumbo    severe   2-7 days correct_rejections   565.000000   \n",
       "                                      hit_rate               0.500000   \n",
       "                                      false_alarm_rate       0.909091   \n",
       "                                      csi                    0.083333   \n",
       "                                      f1_score               0.153846   \n",
       "\n",
       "                                                          trigger0.06  \\\n",
       "Changane_em_Chibuto bankfull 3-5 days hits                   5.000000   \n",
       "                                      false_alarms         102.000000   \n",
       "                                      misses                 0.000000   \n",
       "                                      correct_rejections   291.000000   \n",
       "                                      hit_rate               1.000000   \n",
       "...                                                               ...   \n",
       "Zambeze_em_Zumbo    severe   2-7 days correct_rejections   565.000000   \n",
       "                                      hit_rate               0.500000   \n",
       "                                      false_alarm_rate       0.909091   \n",
       "                                      csi                    0.083333   \n",
       "                                      f1_score               0.153846   \n",
       "\n",
       "                                                          trigger0.07  \\\n",
       "Changane_em_Chibuto bankfull 3-5 days hits                   5.000000   \n",
       "                                      false_alarms          90.000000   \n",
       "                                      misses                 0.000000   \n",
       "                                      correct_rejections   303.000000   \n",
       "                                      hit_rate               1.000000   \n",
       "...                                                               ...   \n",
       "Zambeze_em_Zumbo    severe   2-7 days correct_rejections   565.000000   \n",
       "                                      hit_rate               0.500000   \n",
       "                                      false_alarm_rate       0.909091   \n",
       "                                      csi                    0.083333   \n",
       "                                      f1_score               0.153846   \n",
       "\n",
       "                                                          trigger0.08  \\\n",
       "Changane_em_Chibuto bankfull 3-5 days hits                   5.000000   \n",
       "                                      false_alarms          90.000000   \n",
       "                                      misses                 0.000000   \n",
       "                                      correct_rejections   303.000000   \n",
       "                                      hit_rate               1.000000   \n",
       "...                                                               ...   \n",
       "Zambeze_em_Zumbo    severe   2-7 days correct_rejections   567.000000   \n",
       "                                      hit_rate               0.500000   \n",
       "                                      false_alarm_rate       0.888889   \n",
       "                                      csi                    0.100000   \n",
       "                                      f1_score               0.181818   \n",
       "\n",
       "                                                          trigger0.09  \\\n",
       "Changane_em_Chibuto bankfull 3-5 days hits                   5.000000   \n",
       "                                      false_alarms          90.000000   \n",
       "                                      misses                 0.000000   \n",
       "                                      correct_rejections   303.000000   \n",
       "                                      hit_rate               1.000000   \n",
       "...                                                               ...   \n",
       "Zambeze_em_Zumbo    severe   2-7 days correct_rejections   567.000000   \n",
       "                                      hit_rate               0.500000   \n",
       "                                      false_alarm_rate       0.888889   \n",
       "                                      csi                    0.100000   \n",
       "                                      f1_score               0.181818   \n",
       "\n",
       "                                                          trigger0.10  ...  \\\n",
       "Changane_em_Chibuto bankfull 3-5 days hits                   5.000000  ...   \n",
       "                                      false_alarms          84.000000  ...   \n",
       "                                      misses                 0.000000  ...   \n",
       "                                      correct_rejections   309.000000  ...   \n",
       "                                      hit_rate               1.000000  ...   \n",
       "...                                                               ...  ...   \n",
       "Zambeze_em_Zumbo    severe   2-7 days correct_rejections   568.000000  ...   \n",
       "                                      hit_rate               0.500000  ...   \n",
       "                                      false_alarm_rate       0.875000  ...   \n",
       "                                      csi                    0.111111  ...   \n",
       "                                      f1_score               0.200000  ...   \n",
       "\n",
       "                                                          trigger0.91  \\\n",
       "Changane_em_Chibuto bankfull 3-5 days hits                   4.000000   \n",
       "                                      false_alarms          42.000000   \n",
       "                                      misses                 1.000000   \n",
       "                                      correct_rejections   351.000000   \n",
       "                                      hit_rate               0.800000   \n",
       "...                                                               ...   \n",
       "Zambeze_em_Zumbo    severe   2-7 days correct_rejections   573.000000   \n",
       "                                      hit_rate               0.500000   \n",
       "                                      false_alarm_rate       0.666667   \n",
       "                                      csi                    0.250000   \n",
       "                                      f1_score               0.400000   \n",
       "\n",
       "                                                          trigger0.92  \\\n",
       "Changane_em_Chibuto bankfull 3-5 days hits                   4.000000   \n",
       "                                      false_alarms          42.000000   \n",
       "                                      misses                 1.000000   \n",
       "                                      correct_rejections   351.000000   \n",
       "                                      hit_rate               0.800000   \n",
       "...                                                               ...   \n",
       "Zambeze_em_Zumbo    severe   2-7 days correct_rejections   573.000000   \n",
       "                                      hit_rate               0.500000   \n",
       "                                      false_alarm_rate       0.666667   \n",
       "                                      csi                    0.250000   \n",
       "                                      f1_score               0.400000   \n",
       "\n",
       "                                                          trigger0.93  \\\n",
       "Changane_em_Chibuto bankfull 3-5 days hits                   4.000000   \n",
       "                                      false_alarms          42.000000   \n",
       "                                      misses                 1.000000   \n",
       "                                      correct_rejections   351.000000   \n",
       "                                      hit_rate               0.800000   \n",
       "...                                                               ...   \n",
       "Zambeze_em_Zumbo    severe   2-7 days correct_rejections   573.000000   \n",
       "                                      hit_rate               0.500000   \n",
       "                                      false_alarm_rate       0.666667   \n",
       "                                      csi                    0.250000   \n",
       "                                      f1_score               0.400000   \n",
       "\n",
       "                                                          trigger0.94  \\\n",
       "Changane_em_Chibuto bankfull 3-5 days hits                   4.000000   \n",
       "                                      false_alarms          42.000000   \n",
       "                                      misses                 1.000000   \n",
       "                                      correct_rejections   351.000000   \n",
       "                                      hit_rate               0.800000   \n",
       "...                                                               ...   \n",
       "Zambeze_em_Zumbo    severe   2-7 days correct_rejections   573.000000   \n",
       "                                      hit_rate               0.500000   \n",
       "                                      false_alarm_rate       0.666667   \n",
       "                                      csi                    0.250000   \n",
       "                                      f1_score               0.400000   \n",
       "\n",
       "                                                          trigger0.95  \\\n",
       "Changane_em_Chibuto bankfull 3-5 days hits                   4.000000   \n",
       "                                      false_alarms          42.000000   \n",
       "                                      misses                 1.000000   \n",
       "                                      correct_rejections   351.000000   \n",
       "                                      hit_rate               0.800000   \n",
       "...                                                               ...   \n",
       "Zambeze_em_Zumbo    severe   2-7 days correct_rejections   573.000000   \n",
       "                                      hit_rate               0.500000   \n",
       "                                      false_alarm_rate       0.666667   \n",
       "                                      csi                    0.250000   \n",
       "                                      f1_score               0.400000   \n",
       "\n",
       "                                                          trigger0.96  \\\n",
       "Changane_em_Chibuto bankfull 3-5 days hits                   4.000000   \n",
       "                                      false_alarms          42.000000   \n",
       "                                      misses                 1.000000   \n",
       "                                      correct_rejections   351.000000   \n",
       "                                      hit_rate               0.800000   \n",
       "...                                                               ...   \n",
       "Zambeze_em_Zumbo    severe   2-7 days correct_rejections   573.000000   \n",
       "                                      hit_rate               0.500000   \n",
       "                                      false_alarm_rate       0.666667   \n",
       "                                      csi                    0.250000   \n",
       "                                      f1_score               0.400000   \n",
       "\n",
       "                                                          trigger0.97  \\\n",
       "Changane_em_Chibuto bankfull 3-5 days hits                   4.000000   \n",
       "                                      false_alarms          41.000000   \n",
       "                                      misses                 1.000000   \n",
       "                                      correct_rejections   352.000000   \n",
       "                                      hit_rate               0.800000   \n",
       "...                                                               ...   \n",
       "Zambeze_em_Zumbo    severe   2-7 days correct_rejections   573.000000   \n",
       "                                      hit_rate               0.500000   \n",
       "                                      false_alarm_rate       0.666667   \n",
       "                                      csi                    0.250000   \n",
       "                                      f1_score               0.400000   \n",
       "\n",
       "                                                          trigger0.98  \\\n",
       "Changane_em_Chibuto bankfull 3-5 days hits                   4.000000   \n",
       "                                      false_alarms          41.000000   \n",
       "                                      misses                 1.000000   \n",
       "                                      correct_rejections   352.000000   \n",
       "                                      hit_rate               0.800000   \n",
       "...                                                               ...   \n",
       "Zambeze_em_Zumbo    severe   2-7 days correct_rejections   573.000000   \n",
       "                                      hit_rate               0.500000   \n",
       "                                      false_alarm_rate       0.666667   \n",
       "                                      csi                    0.250000   \n",
       "                                      f1_score               0.400000   \n",
       "\n",
       "                                                          trigger0.99  \\\n",
       "Changane_em_Chibuto bankfull 3-5 days hits                   4.000000   \n",
       "                                      false_alarms          41.000000   \n",
       "                                      misses                 1.000000   \n",
       "                                      correct_rejections   352.000000   \n",
       "                                      hit_rate               0.800000   \n",
       "...                                                               ...   \n",
       "Zambeze_em_Zumbo    severe   2-7 days correct_rejections   573.000000   \n",
       "                                      hit_rate               0.500000   \n",
       "                                      false_alarm_rate       0.666667   \n",
       "                                      csi                    0.250000   \n",
       "                                      f1_score               0.400000   \n",
       "\n",
       "                                                          trigger1.00  \n",
       "Changane_em_Chibuto bankfull 3-5 days hits                   4.000000  \n",
       "                                      false_alarms          41.000000  \n",
       "                                      misses                 1.000000  \n",
       "                                      correct_rejections   352.000000  \n",
       "                                      hit_rate               0.800000  \n",
       "...                                                               ...  \n",
       "Zambeze_em_Zumbo    severe   2-7 days correct_rejections   573.000000  \n",
       "                                      hit_rate               0.500000  \n",
       "                                      false_alarm_rate       0.666667  \n",
       "                                      csi                    0.250000  \n",
       "                                      f1_score               0.400000  \n",
       "\n",
       "[2304 rows x 100 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate all DataFrames in the list\n",
    "combined_df = pd.concat(all_dfs_combine, ignore_index=False)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc2e531e-c489-457b-aacf-4d823b4ff4ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_df.to_csv(f'{country}_combined_output.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b35bc2-075b-4279-b856-2ade74df068d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display an example of one of the grouped_dfs\n",
    "example = grouped_dfs['beitbridge','bankfull']\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9323b6",
   "metadata": {
    "cell_marker": "################################"
   },
   "source": [
    "Section 5: Trigger selection \n",
    "\n",
    "In the final section we evaluate each of the small dataframes in grouped_dfs and evaluate the performance of GloFAS at each gauging station for each of the three thresholds by looking at each of the percentage triggers. Here for each percentage we look at the contigency metrics and skill scores and identify based on previous forecasts how well or not GloFAS can capture flooding. We then pick the best trigger percentage based primarily on the f1 score which balances the hit and false alarm rates. Ideally we want f1 scores above 0.5 and closer to 1.0. The code is set up to filter triggers where the f1 score is greater than 0.45. After we choose the trigger with the highest f1 value. In the event we have more than one trigger left we decide by choosing the one with the highest hit rate, then the lowest false alarm rate and finally the lowest trigger percentage. \n",
    "\n",
    "To finish we save the list of the best performing triggers to a csv which can be found in our output folder. These will be our triggers for the operational flood AA system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cbe0caf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 station threshold lead time best_trigger  f1_score  hit_rate  \\\n",
      "0     Limpopo__Combomune  bankfull  3-5 days  trigger0.85  0.626866  0.617647   \n",
      "1     Limpopo__Combomune    severe  3-5 days  trigger0.10  1.000000  1.000000   \n",
      "2      Limpopo_em_Chokwe  bankfull  3-5 days  trigger0.34  0.450000  0.692308   \n",
      "3      Limpopo_em_Chokwe    severe  3-5 days  trigger0.10  1.000000  1.000000   \n",
      "4       Limpopo_em_Mapai  bankfull  3-5 days  trigger0.10  0.470588  0.666667   \n",
      "5    Limpopo_em_Sicacate  bankfull  3-5 days  trigger0.88  0.615385  0.869565   \n",
      "6    Limpopo_em_Sicacate    severe  3-5 days  trigger0.19  1.000000  1.000000   \n",
      "7   Chire_em_Vila_Bocage  moderate  3-6 days  trigger0.28  0.500000  0.363636   \n",
      "8     Limpopo__Combomune  bankfull  3-6 days  trigger0.78  0.600000  0.552632   \n",
      "9     Limpopo__Combomune    severe  3-6 days  trigger0.10  1.000000  1.000000   \n",
      "10     Limpopo_em_Chokwe  bankfull  3-6 days  trigger0.66  0.500000  0.500000   \n",
      "11     Limpopo_em_Chokwe    severe  3-6 days  trigger0.07  1.000000  1.000000   \n",
      "12      Limpopo_em_Mapai  bankfull  3-6 days  trigger0.19  0.514286  0.692308   \n",
      "13      Limpopo_em_Mapai  moderate  3-6 days  trigger0.12  0.500000  0.500000   \n",
      "14   Limpopo_em_Sicacate  bankfull  3-6 days  trigger0.89  0.704225  0.862069   \n",
      "15   Limpopo_em_Sicacate  moderate  3-6 days  trigger0.85  0.571429  0.400000   \n",
      "16   Limpopo_em_Sicacate    severe  3-6 days  trigger0.23  1.000000  1.000000   \n",
      "17      Zambeze_em_Zumbo  moderate  3-6 days  trigger0.96  0.571429  0.500000   \n",
      "18  Chire_em_Vila_Bocage  moderate  3-7 days  trigger0.28  0.500000  0.363636   \n",
      "19    Limpopo__Combomune  bankfull  3-7 days  trigger0.78  0.600000  0.552632   \n",
      "20    Limpopo__Combomune    severe  3-7 days  trigger0.10  1.000000  1.000000   \n",
      "21     Limpopo_em_Chokwe  bankfull  3-7 days  trigger0.66  0.500000  0.500000   \n",
      "22     Limpopo_em_Chokwe    severe  3-7 days  trigger0.07  1.000000  1.000000   \n",
      "23      Limpopo_em_Mapai  bankfull  3-7 days  trigger0.19  0.514286  0.692308   \n",
      "24      Limpopo_em_Mapai  moderate  3-7 days  trigger0.12  0.500000  0.500000   \n",
      "25   Limpopo_em_Sicacate  bankfull  3-7 days  trigger0.89  0.704225  0.862069   \n",
      "26   Limpopo_em_Sicacate  moderate  3-7 days  trigger0.85  0.571429  0.400000   \n",
      "27   Limpopo_em_Sicacate    severe  3-7 days  trigger0.23  1.000000  1.000000   \n",
      "28      Zambeze_em_Zumbo  moderate  3-7 days  trigger0.96  0.571429  0.500000   \n",
      "29    Limpopo__Combomune  bankfull  2-5 days  trigger0.89  0.636364  0.583333   \n",
      "30    Limpopo__Combomune    severe  2-5 days  trigger0.07  1.000000  1.000000   \n",
      "31     Limpopo_em_Chokwe  bankfull  2-5 days  trigger0.26  0.450000  0.692308   \n",
      "32     Limpopo_em_Chokwe    severe  2-5 days  trigger0.07  1.000000  1.000000   \n",
      "33      Limpopo_em_Mapai  bankfull  2-5 days  trigger0.07  0.514286  0.692308   \n",
      "34   Limpopo_em_Sicacate  bankfull  2-5 days  trigger0.91  0.633333  0.791667   \n",
      "35   Limpopo_em_Sicacate    severe  2-5 days  trigger0.14  0.666667  1.000000   \n",
      "36  Chire_em_Vila_Bocage  moderate  2-6 days  trigger0.39  0.470588  0.363636   \n",
      "37    Limpopo__Combomune  bankfull  2-6 days  trigger0.86  0.608696  0.525000   \n",
      "38    Limpopo__Combomune    severe  2-6 days  trigger0.08  1.000000  1.000000   \n",
      "39     Limpopo_em_Chokwe  bankfull  2-6 days  trigger0.22  0.468085  0.687500   \n",
      "40     Limpopo_em_Chokwe    severe  2-6 days  trigger0.06  1.000000  1.000000   \n",
      "41      Limpopo_em_Mapai  bankfull  2-6 days  trigger0.15  0.555556  0.714286   \n",
      "42   Limpopo_em_Sicacate  bankfull  2-6 days  trigger0.81  0.695652  0.800000   \n",
      "43   Limpopo_em_Sicacate  moderate  2-6 days  trigger0.88  0.571429  0.400000   \n",
      "44   Limpopo_em_Sicacate    severe  2-6 days  trigger0.21  1.000000  1.000000   \n",
      "45      Zambeze_em_Zumbo  moderate  2-6 days  trigger0.79  0.571429  0.500000   \n",
      "46  Chire_em_Vila_Bocage  moderate  2-7 days  trigger0.39  0.470588  0.363636   \n",
      "47    Limpopo__Combomune  bankfull  2-7 days  trigger0.86  0.608696  0.525000   \n",
      "48    Limpopo__Combomune    severe  2-7 days  trigger0.08  1.000000  1.000000   \n",
      "49     Limpopo_em_Chokwe  bankfull  2-7 days  trigger0.22  0.468085  0.687500   \n",
      "50     Limpopo_em_Chokwe    severe  2-7 days  trigger0.06  1.000000  1.000000   \n",
      "51      Limpopo_em_Mapai  bankfull  2-7 days  trigger0.15  0.555556  0.714286   \n",
      "52   Limpopo_em_Sicacate  bankfull  2-7 days  trigger0.81  0.695652  0.800000   \n",
      "53   Limpopo_em_Sicacate  moderate  2-7 days  trigger0.88  0.571429  0.400000   \n",
      "54   Limpopo_em_Sicacate    severe  2-7 days  trigger0.21  1.000000  1.000000   \n",
      "55      Zambeze_em_Zumbo  moderate  2-7 days  trigger0.79  0.571429  0.500000   \n",
      "\n",
      "    false_alarm_rate  \n",
      "0           0.363636  \n",
      "1           0.000000  \n",
      "2           0.666667  \n",
      "3           0.000000  \n",
      "4           0.636364  \n",
      "5           0.523810  \n",
      "6           0.000000  \n",
      "7           0.200000  \n",
      "8           0.343750  \n",
      "9           0.000000  \n",
      "10          0.500000  \n",
      "11          0.000000  \n",
      "12          0.590909  \n",
      "13          0.500000  \n",
      "14          0.404762  \n",
      "15          0.000000  \n",
      "16          0.000000  \n",
      "17          0.333333  \n",
      "18          0.200000  \n",
      "19          0.343750  \n",
      "20          0.000000  \n",
      "21          0.500000  \n",
      "22          0.000000  \n",
      "23          0.590909  \n",
      "24          0.500000  \n",
      "25          0.404762  \n",
      "26          0.000000  \n",
      "27          0.000000  \n",
      "28          0.333333  \n",
      "29          0.300000  \n",
      "30          0.000000  \n",
      "31          0.666667  \n",
      "32          0.000000  \n",
      "33          0.590909  \n",
      "34          0.472222  \n",
      "35          0.500000  \n",
      "36          0.333333  \n",
      "37          0.275862  \n",
      "38          0.000000  \n",
      "39          0.645161  \n",
      "40          0.000000  \n",
      "41          0.545455  \n",
      "42          0.384615  \n",
      "43          0.000000  \n",
      "44          0.000000  \n",
      "45          0.333333  \n",
      "46          0.333333  \n",
      "47          0.275862  \n",
      "48          0.000000  \n",
      "49          0.645161  \n",
      "50          0.000000  \n",
      "51          0.545455  \n",
      "52          0.384615  \n",
      "53          0.000000  \n",
      "54          0.000000  \n",
      "55          0.333333  \n"
     ]
    }
   ],
   "source": [
    "# create an empty dictionary to store the best triggers\n",
    "best_triggers = {}\n",
    "\n",
    "# iterate through each dataframe in grouped_dfs\n",
    "for grouped_dfs in all_dfs:\n",
    "    for key, df in grouped_dfs.items():\n",
    "        # find the column names corresponding to trigger thresholds (i.e., the percentages)\n",
    "        threshold_columns = [col for col in df.columns if col.startswith('trigger')]\n",
    "\n",
    "        # filter triggers based on the f1 score\n",
    "        filtered_columns = [\n",
    "            col for col in threshold_columns\n",
    "            if df.loc['f1_score', col] >= 0.45\n",
    "            # if no triggers fit the criteria, you can get the max f score by uncommenting the next line\n",
    "            #if df.loc['f1_score', col] >= df.loc['f1_score'].max()\n",
    "        ]\n",
    "\n",
    "        # if there are any columns left after filtering, identify the maximum f1 score for each threshold (i.e., bankfull, moderate, severe)\n",
    "        if filtered_columns:\n",
    "            max_f1 = df.loc['f1_score', filtered_columns].max()\n",
    "\n",
    "            # find all thresholds with the maximum f1 score \n",
    "            best_f1_thresholds = df.loc['f1_score', filtered_columns][df.loc['f1_score', filtered_columns] == max_f1].index.tolist()\n",
    "\n",
    "            # if there are multiple thresholds with the same f1 score, proceed to resolve ties\n",
    "            if len(best_f1_thresholds) > 1:\n",
    "                # resolve ties by choosing the highest hit rate\n",
    "                hit_rates = df.loc['hit_rate', best_f1_thresholds]\n",
    "                max_hit_rate = hit_rates.max()\n",
    "                best_f1_thresholds = hit_rates[hit_rates == max_hit_rate].index.tolist()\n",
    "\n",
    "                # if there are still ties, resolve by choosing the lowest false alarm rate\n",
    "                if len(best_f1_thresholds) > 1:\n",
    "                    false_alarm_rates = df.loc['false_alarm_rate', best_f1_thresholds]\n",
    "                    min_false_alarm_rate = false_alarm_rates.min()\n",
    "                    best_f1_thresholds = false_alarm_rates[false_alarm_rates == min_false_alarm_rate].index.tolist()\n",
    "\n",
    "                    # if there are still ties, choose the lowest trigger (threshold)\n",
    "                    if len(best_f1_thresholds) > 1:\n",
    "                        best_threshold = min(best_f1_thresholds, key=lambda x: float(x.split('trigger')[1].split('_')[0]))  # Sorting by numeric threshold\n",
    "                    else:\n",
    "                        best_threshold = best_f1_thresholds[0]\n",
    "                else:\n",
    "                    best_threshold = best_f1_thresholds[0]\n",
    "            else:\n",
    "                best_threshold = best_f1_thresholds[0]\n",
    "\n",
    "            # split key into station and threshold type (if it's a tuple)\n",
    "            station, threshold, lead = key if isinstance(key, tuple) else (key, 'unknown')\n",
    "            # store the best threshold information\n",
    "            best_triggers[(station, threshold, lead)] = {\n",
    "                'station': station,\n",
    "                'threshold': threshold,\n",
    "                'lead time': lead,\n",
    "                'best_trigger': best_threshold,\n",
    "                'f1_score': df.loc['f1_score', best_threshold],\n",
    "                'hit_rate': df.loc['hit_rate', best_threshold],\n",
    "                'false_alarm_rate': df.loc['false_alarm_rate', best_threshold]\n",
    "            }\n",
    "\n",
    "best_triggers_df = pd.DataFrame(best_triggers.values())\n",
    "\n",
    "# print the best triggers\n",
    "print(best_triggers_df)\n",
    "\n",
    "# Save output as a CSV using country name\n",
    "filename = f\"{country.lower()}_triggers_2025_2026.csv\"\n",
    "best_triggers_df.to_csv(os.path.join(output_directory, filename), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aebd1d9d-21f4-4eaa-8cdd-cc5018aa987f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>threshold</th>\n",
       "      <th>lead time</th>\n",
       "      <th>best_trigger</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>false_alarm_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Chire_em_Vila_Bocage</td>\n",
       "      <td>moderate</td>\n",
       "      <td>3-6 days</td>\n",
       "      <td>trigger0.28</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Limpopo__Combomune</td>\n",
       "      <td>bankfull</td>\n",
       "      <td>2-5 days</td>\n",
       "      <td>trigger0.89</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Limpopo__Combomune</td>\n",
       "      <td>severe</td>\n",
       "      <td>3-5 days</td>\n",
       "      <td>trigger0.10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Limpopo_em_Chokwe</td>\n",
       "      <td>bankfull</td>\n",
       "      <td>3-6 days</td>\n",
       "      <td>trigger0.66</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Limpopo_em_Chokwe</td>\n",
       "      <td>severe</td>\n",
       "      <td>3-5 days</td>\n",
       "      <td>trigger0.10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Limpopo_em_Mapai</td>\n",
       "      <td>bankfull</td>\n",
       "      <td>2-6 days</td>\n",
       "      <td>trigger0.15</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Limpopo_em_Mapai</td>\n",
       "      <td>moderate</td>\n",
       "      <td>3-6 days</td>\n",
       "      <td>trigger0.12</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Limpopo_em_Sicacate</td>\n",
       "      <td>bankfull</td>\n",
       "      <td>3-6 days</td>\n",
       "      <td>trigger0.89</td>\n",
       "      <td>0.704225</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.404762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Limpopo_em_Sicacate</td>\n",
       "      <td>moderate</td>\n",
       "      <td>3-6 days</td>\n",
       "      <td>trigger0.85</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Limpopo_em_Sicacate</td>\n",
       "      <td>severe</td>\n",
       "      <td>3-5 days</td>\n",
       "      <td>trigger0.19</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Zambeze_em_Zumbo</td>\n",
       "      <td>moderate</td>\n",
       "      <td>3-6 days</td>\n",
       "      <td>trigger0.96</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 station threshold lead time best_trigger  f1_score  hit_rate  \\\n",
       "7   Chire_em_Vila_Bocage  moderate  3-6 days  trigger0.28  0.500000  0.363636   \n",
       "29    Limpopo__Combomune  bankfull  2-5 days  trigger0.89  0.636364  0.583333   \n",
       "1     Limpopo__Combomune    severe  3-5 days  trigger0.10  1.000000  1.000000   \n",
       "10     Limpopo_em_Chokwe  bankfull  3-6 days  trigger0.66  0.500000  0.500000   \n",
       "3      Limpopo_em_Chokwe    severe  3-5 days  trigger0.10  1.000000  1.000000   \n",
       "41      Limpopo_em_Mapai  bankfull  2-6 days  trigger0.15  0.555556  0.714286   \n",
       "13      Limpopo_em_Mapai  moderate  3-6 days  trigger0.12  0.500000  0.500000   \n",
       "14   Limpopo_em_Sicacate  bankfull  3-6 days  trigger0.89  0.704225  0.862069   \n",
       "15   Limpopo_em_Sicacate  moderate  3-6 days  trigger0.85  0.571429  0.400000   \n",
       "6    Limpopo_em_Sicacate    severe  3-5 days  trigger0.19  1.000000  1.000000   \n",
       "17      Zambeze_em_Zumbo  moderate  3-6 days  trigger0.96  0.571429  0.500000   \n",
       "\n",
       "    false_alarm_rate  \n",
       "7           0.200000  \n",
       "29          0.300000  \n",
       "1           0.000000  \n",
       "10          0.500000  \n",
       "3           0.000000  \n",
       "41          0.545455  \n",
       "13          0.500000  \n",
       "14          0.404762  \n",
       "15          0.000000  \n",
       "6           0.000000  \n",
       "17          0.333333  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_triggers_df.loc[best_triggers_df.groupby(['station','threshold'])['f1_score'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e07b6607-a35f-4fca-b6e3-de93979fed0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_triggers_df_lead= best_triggers_df.loc[best_triggers_df.groupby(['station','threshold'])['f1_score'].idxmax()]\n",
    "best_triggers_df_lead.to_csv(f'{country}_best_triggers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553d8264-bf2d-4945-980b-63a24f6794bb",
   "metadata": {},
   "source": [
    "## calculate # of exceedances of each threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29215c9d-d5c4-42e4-93b4-8e79a14beb97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>bankfull_exceedance</th>\n",
       "      <th>moderate_exceedance</th>\n",
       "      <th>severe_exceedance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Limpopo_em_Mapai</td>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Limpopo__Combomune</td>\n",
       "      <td>203</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Limpopo_em_Chokwe</td>\n",
       "      <td>102</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Limpopo_em_Sicacate</td>\n",
       "      <td>302</td>\n",
       "      <td>27</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Limpopo_em_Mabalane</td>\n",
       "      <td>79</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Limpopo_em_XaiXai</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Changane_em_Chibuto</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Limpopo_em_Macaretane</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Zambeze_em_Marromeu_Sena_Sugar</td>\n",
       "      <td>163</td>\n",
       "      <td>46</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Chire_em_Vila_Bocage</td>\n",
       "      <td>497</td>\n",
       "      <td>74</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Chire_em_megaza_Mutamba</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Zambeze_em_Caia_SS</td>\n",
       "      <td>228</td>\n",
       "      <td>46</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Revubue_em_Chingodzi</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Zambeze_em_Zumbo</td>\n",
       "      <td>78</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Zambeze_em_Tete</td>\n",
       "      <td>47</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LuenhaLuenha_I</td>\n",
       "      <td>106</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           station  bankfull_exceedance  moderate_exceedance  \\\n",
       "0                 Limpopo_em_Mapai                   47                    2   \n",
       "1               Limpopo__Combomune                  203                    9   \n",
       "2                Limpopo_em_Chokwe                  102                    8   \n",
       "3              Limpopo_em_Sicacate                  302                   27   \n",
       "4              Limpopo_em_Mabalane                   79                   15   \n",
       "5                Limpopo_em_XaiXai                   13                    0   \n",
       "6              Changane_em_Chibuto                  155                    0   \n",
       "7            Limpopo_em_Macaretane                   56                    2   \n",
       "8   Zambeze_em_Marromeu_Sena_Sugar                  163                   46   \n",
       "9             Chire_em_Vila_Bocage                  497                   74   \n",
       "10         Chire_em_megaza_Mutamba                    0                    0   \n",
       "11              Zambeze_em_Caia_SS                  228                   46   \n",
       "12            Revubue_em_Chingodzi                   15                   10   \n",
       "13                Zambeze_em_Zumbo                   78                   18   \n",
       "14                 Zambeze_em_Tete                   47                   10   \n",
       "15                  LuenhaLuenha_I                  106                   10   \n",
       "\n",
       "    severe_exceedance  \n",
       "0                   0  \n",
       "1                   2  \n",
       "2                   4  \n",
       "3                  10  \n",
       "4                   0  \n",
       "5                   0  \n",
       "6                   0  \n",
       "7                   0  \n",
       "8                  23  \n",
       "9                  22  \n",
       "10                  0  \n",
       "11                 13  \n",
       "12                  2  \n",
       "13                 14  \n",
       "14                  5  \n",
       "15                  4  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exc = []\n",
    "for c in observed_data.columns:\n",
    "    if c=='date':\n",
    "        continue\n",
    "    info = station_info[station_info['station name']==c]\n",
    "    bank = info['obs_bankfull'].item()\n",
    "    mod = info['obs_moderate'].item()\n",
    "    sev = info['obs_severe'].item()\n",
    "    count_b = (observed_data[c]>bank).sum()\n",
    "    count_m = (observed_data[c]>mod).sum()\n",
    "    count_s = (observed_data[c]>sev).sum()\n",
    "    exc.append({\n",
    "            'station': c,\n",
    "            'bankfull_exceedance': count_b,\n",
    "            'moderate_exceedance': count_m,\n",
    "            'severe_exceedance': count_s,\n",
    "        })\n",
    "exc_df = pd.DataFrame(exc)\n",
    "exc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357cf2d8-4e52-422c-9d24-f0de8732bbaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "hdc",
   "language": "python",
   "name": "conda-env-hdc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
