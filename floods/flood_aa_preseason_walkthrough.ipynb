{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3549cdc8-4dde-43ea-b719-4f516c17734c",
   "metadata": {},
   "source": [
    "Preseason flood anticipatory action script to process observed/reanalysis and \n",
    "forecast data, calculate contigency metrics and choose the 'best' trigger for \n",
    "operational use based on quality criteria (e.g., f1 score, hit rate and false alarm rates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f64bf4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries \n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03603971",
   "metadata": {
    "cell_marker": "#######################################################"
   },
   "source": [
    "Section 1: Define variables, paths and read in data \n",
    "\n",
    "In this section we begin by defining our country of interest, our main working directory and whether we are using observed or reanalysis data. We then define the paths to load the data and perform visual checks to ensure the data is what we are expecting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fa48348-a091-4695-aaae-65aa18fae052",
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'mozambique'  # define country of interest\n",
    "directory = '/s3/scratch/jamie.towner/flood_aa'  # define main working directory\n",
    "benchmark = 'observations'  # choose 'observations' or 'glofas_reanalysis' as the benchmark\n",
    "\n",
    "# define paths to data\n",
    "forecast_data_directory = os.path.join(directory, country, \"data/forecasts/glofas_reforecasts\")\n",
    "metadata_directory = os.path.join(directory, country, \"data/metadata\")\n",
    "output_directory = os.path.join(directory, country, \"outputs\")\n",
    "\n",
    "# create output directory if it does not exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# set observed data and metadata directory and filenames based on benchmark choice\n",
    "if benchmark == 'observations':\n",
    "    observed_data_directory = os.path.join(directory, country, \"data/observations/gauging_stations/all_stations\")\n",
    "    observed_data_file = \"observations.csv\"\n",
    "    station_info_file = \"metadata_observations.csv\"\n",
    "elif benchmark == 'glofas_reanalysis':\n",
    "    observed_data_directory = os.path.join(directory, country, \"data/forecasts/glofas_reanalysis/all_stations\")\n",
    "    observed_data_file = \"glofas_reanalysis_moz.csv\"\n",
    "    station_info_file = 'metadata_glofas_reanalysis.csv'\n",
    "else:\n",
    "    raise ValueError(\"invalid benchmark choice. choose 'observations' or 'glofas_reanalysis'.\")\n",
    "\n",
    "# load the observed or reanalysis data and gauging stations metadata\n",
    "observed_data_path = os.path.join(observed_data_directory, observed_data_file)\n",
    "station_info_path = os.path.join(metadata_directory, station_info_file)\n",
    "\n",
    "observed_data = pd.read_csv(observed_data_path)\n",
    "station_info = pd.read_csv(station_info_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be228527-43db-49c2-800a-1883d30ccec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>gurue</th>\n",
       "      <th>goonda</th>\n",
       "      <th>messalo</th>\n",
       "      <th>revue</th>\n",
       "      <th>franca</th>\n",
       "      <th>mocuba</th>\n",
       "      <th>nairoto</th>\n",
       "      <th>massangena</th>\n",
       "      <th>espungabera</th>\n",
       "      <th>dombe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/01/2003</td>\n",
       "      <td>2.593333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.013333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.173333</td>\n",
       "      <td>1.633333</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02/01/2003</td>\n",
       "      <td>4.446667</td>\n",
       "      <td>2.040000</td>\n",
       "      <td>3.953333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.993333</td>\n",
       "      <td>1.563333</td>\n",
       "      <td>0.736667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03/01/2003</td>\n",
       "      <td>3.706667</td>\n",
       "      <td>2.030000</td>\n",
       "      <td>4.076667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.243333</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04/01/2003</td>\n",
       "      <td>3.293333</td>\n",
       "      <td>2.016667</td>\n",
       "      <td>3.810000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.380000</td>\n",
       "      <td>1.870000</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05/01/2003</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.060000</td>\n",
       "      <td>3.876667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.090000</td>\n",
       "      <td>2.066667</td>\n",
       "      <td>0.646667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date     gurue    goonda   messalo  revue  franca    mocuba  \\\n",
       "0  01/01/2003  2.593333       NaN  4.013333    NaN     NaN  4.173333   \n",
       "1  02/01/2003  4.446667  2.040000  3.953333    NaN     NaN  4.993333   \n",
       "2  03/01/2003  3.706667  2.030000  4.076667    NaN     NaN  6.243333   \n",
       "3  04/01/2003  3.293333  2.016667  3.810000    NaN     NaN  6.380000   \n",
       "4  05/01/2003  4.300000  2.060000  3.876667    NaN     NaN  7.090000   \n",
       "\n",
       "    nairoto  massangena  espungabera  dombe  \n",
       "0  1.633333    0.760000          NaN    NaN  \n",
       "1  1.563333    0.736667          NaN    NaN  \n",
       "2  1.800000    0.706667          NaN    NaN  \n",
       "3  1.870000    0.670000          NaN    NaN  \n",
       "4  2.066667    0.646667          NaN    2.5  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the observed data \n",
    "observed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e840a37-0cc4-47ce-829e-0943e5a67503",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station id</th>\n",
       "      <th>station name</th>\n",
       "      <th>google</th>\n",
       "      <th>river name</th>\n",
       "      <th>river basin</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>lisflood_x</th>\n",
       "      <th>lisflood_y</th>\n",
       "      <th>obs_bankfull</th>\n",
       "      <th>obs_moderate</th>\n",
       "      <th>obs_severe</th>\n",
       "      <th>glofas_bankfull</th>\n",
       "      <th>glofas_moderate</th>\n",
       "      <th>glofas_severe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>456</td>\n",
       "      <td>goonda</td>\n",
       "      <td>hybas_1121510590</td>\n",
       "      <td>Buzi</td>\n",
       "      <td>Buzi</td>\n",
       "      <td>-19.9167</td>\n",
       "      <td>33.8667</td>\n",
       "      <td>33.875</td>\n",
       "      <td>-19.875</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.20</td>\n",
       "      <td>8.44</td>\n",
       "      <td>2428</td>\n",
       "      <td>3003</td>\n",
       "      <td>5845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>246</td>\n",
       "      <td>dombe</td>\n",
       "      <td>hybas_1121513190</td>\n",
       "      <td>Lucite</td>\n",
       "      <td>Buzi</td>\n",
       "      <td>-19.9667</td>\n",
       "      <td>33.4000</td>\n",
       "      <td>33.425</td>\n",
       "      <td>-19.975</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.37</td>\n",
       "      <td>9.79</td>\n",
       "      <td>385</td>\n",
       "      <td>1549</td>\n",
       "      <td>1877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84</td>\n",
       "      <td>espungabera</td>\n",
       "      <td>hybas_1121524840</td>\n",
       "      <td>Buzi</td>\n",
       "      <td>Buzi</td>\n",
       "      <td>-20.4458</td>\n",
       "      <td>32.8603</td>\n",
       "      <td>32.825</td>\n",
       "      <td>-20.475</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.80</td>\n",
       "      <td>4.21</td>\n",
       "      <td>182</td>\n",
       "      <td>194</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>654</td>\n",
       "      <td>revue</td>\n",
       "      <td>hybas_1122238390</td>\n",
       "      <td>Revue</td>\n",
       "      <td>Buzi</td>\n",
       "      <td>-19.7653</td>\n",
       "      <td>33.8461</td>\n",
       "      <td>33.825</td>\n",
       "      <td>-19.775</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.87</td>\n",
       "      <td>819</td>\n",
       "      <td>150</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90</td>\n",
       "      <td>gurue</td>\n",
       "      <td>hybas_1122203010</td>\n",
       "      <td>Licungo</td>\n",
       "      <td>Licungo</td>\n",
       "      <td>-15.4667</td>\n",
       "      <td>37.0167</td>\n",
       "      <td>36.925</td>\n",
       "      <td>-15.475</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.55</td>\n",
       "      <td>4.88</td>\n",
       "      <td>24</td>\n",
       "      <td>69</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station id station name            google river name river basin      lat  \\\n",
       "0         456       goonda  hybas_1121510590       Buzi       Buzi  -19.9167   \n",
       "1         246        dombe  hybas_1121513190     Lucite       Buzi  -19.9667   \n",
       "2          84  espungabera  hybas_1121524840       Buzi       Buzi  -20.4458   \n",
       "3         654        revue  hybas_1122238390      Revue       Buzi  -19.7653   \n",
       "4          90        gurue  hybas_1122203010    Licungo     Licungo -15.4667   \n",
       "\n",
       "       lon  lisflood_x  lisflood_y  obs_bankfull  obs_moderate  obs_severe  \\\n",
       "0  33.8667      33.875     -19.875           6.0          7.20        8.44   \n",
       "1  33.4000      33.425     -19.975           6.0          9.37        9.79   \n",
       "2  32.8603      32.825     -20.475           3.7          3.80        4.21   \n",
       "3  33.8461      33.825     -19.775           4.5          2.65        2.87   \n",
       "4  37.0167      36.925     -15.475           3.5          4.55        4.88   \n",
       "\n",
       "   glofas_bankfull  glofas_moderate  glofas_severe  \n",
       "0             2428             3003           5845  \n",
       "1              385             1549           1877  \n",
       "2              182              194            272  \n",
       "3              819              150            338  \n",
       "4               24               69            108  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the metadata\n",
    "station_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffde08a8-6f9d-4363-a453-811e25329712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the date column in observed_data to pandas timestamps \n",
    "observed_data[\"date\"] = pd.to_datetime(observed_data[\"date\"], format='mixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0bf5c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all GloFAS forecast files (there should be 1052 files per gauging station)\n",
    "forecast_files = glob.glob(os.path.join(forecast_data_directory, '*.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8aabcc4-55c8-4749-a696-9a600ed53633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/s3/scratch/jamie.towner/flood_aa/mozambique/data/forecasts/glofas_reforecasts/Dombe_2003-10-19.nc',\n",
       " '/s3/scratch/jamie.towner/flood_aa/mozambique/data/forecasts/glofas_reforecasts/Dombe_2003-03-30.nc',\n",
       " '/s3/scratch/jamie.towner/flood_aa/mozambique/data/forecasts/glofas_reforecasts/Dombe_2003-10-26.nc',\n",
       " '/s3/scratch/jamie.towner/flood_aa/mozambique/data/forecasts/glofas_reforecasts/Dombe_2003-10-02.nc',\n",
       " '/s3/scratch/jamie.towner/flood_aa/mozambique/data/forecasts/glofas_reforecasts/Dombe_2003-10-09.nc',\n",
       " '/s3/scratch/jamie.towner/flood_aa/mozambique/data/forecasts/glofas_reforecasts/Dombe_2003-10-12.nc',\n",
       " '/s3/scratch/jamie.towner/flood_aa/mozambique/data/forecasts/glofas_reforecasts/Dombe_2003-03-27.nc',\n",
       " '/s3/scratch/jamie.towner/flood_aa/mozambique/data/forecasts/glofas_reforecasts/Dombe_2003-10-16.nc',\n",
       " '/s3/scratch/jamie.towner/flood_aa/mozambique/data/forecasts/glofas_reforecasts/Dombe_2003-10-23.nc',\n",
       " '/s3/scratch/jamie.towner/flood_aa/mozambique/data/forecasts/glofas_reforecasts/Dombe_2003-10-05.nc']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check forecast files have loaded as expected \n",
    "forecast_files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4700c6e-a83a-494a-84a9-fb99b4753fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "forecast directory: /s3/scratch/jamie.towner/flood_aa/mozambique/data/forecasts/glofas_reforecasts\n",
      "observed data directory: /s3/scratch/jamie.towner/flood_aa/mozambique/data/observations/gauging_stations/all_stations\n",
      "observed data file: observations.csv\n",
      "metadata directory: /s3/scratch/jamie.towner/flood_aa/mozambique/data/metadata\n",
      "output directory: /s3/scratch/jamie.towner/flood_aa/mozambique/outputs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print data paths to ensure they are set correctly\n",
    "print(f\"\"\"\n",
    "forecast directory: {forecast_data_directory}\n",
    "observed data directory: {observed_data_directory}\n",
    "observed data file: {observed_data_file}\n",
    "metadata directory: {metadata_directory}\n",
    "output directory: {output_directory}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1fb0e2",
   "metadata": {
    "cell_marker": "##############################################################################"
   },
   "source": [
    "Section 2: Process observations and forecasts and define events/non-events \n",
    "\n",
    "In this section we begin by matching the station names in the forecast files from those in the metadata and then process one forecast file at a time. We should have 1052 forecast files per station analysed. To process a station we need the name to be in both the station_info file (i.e., the metadata) and the naming convention of the forecast files. A message will highlight if this is not the case and the code will skip that particular gauging station. We then extract the variable of the forecast data which is river discharge (m3/s) and each ensemble member (we have 11 for GloFAS reforecasts and there will be 51 operationally). After we define the forecast issue date from the forecast netcdf and calculate the forecast end date based on lead time. Remember that due to the indexing of Python from 0 we add 1 to each end date. \n",
    "\n",
    "Looking at the metadata you can see that we have three thresholds for both observations and forecasts. These are bankfull, moderate and observed. The bankfull represents the point at which a gauging station begins to flood and is the readiness phase of the system (i.e., no anticipatory action will take place). The moderate and severe thresholds are based on the 5 and 10 year return periods of the observed data. We then use quantile mapping to map these observed thresholds to the GloFAS reanalysis product over the same time period (2003-2023). This in effect performs a bias correction on the forecasts. \n",
    "\n",
    "Finally, for each threshold we simply identify if each ensemble members of a forecast for each specific lead time exceeds the forecast thresholds and we do the same for the observations. If there is an exceedance we assign a 1 or TRUE and if not we assign a 0 or FALSE. We end this section by creating a dataframe which displays these results called events_df. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "310eb8d3",
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing forecast files: 100%|██████████| 10520/10520 [41:18<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing complete.\n"
     ]
    }
   ],
   "source": [
    "# create an empty list to store events/non-events \n",
    "events = []\n",
    "\n",
    "# filter station_info (i.e., the metadata) to include only stations present in the forecast_files\n",
    "# extract station names from forecast filenames\n",
    "station_names_in_files = [os.path.basename(file).split(\"_\")[0] for file in forecast_files]\n",
    "# get unique station names\n",
    "unique_station_names = list(set(station_names_in_files))\n",
    "# convert station names to lowercase if required\n",
    "filtered_station_info = station_info[station_info[\"station name\"].str.lower().isin([name.lower() for name in unique_station_names])]\n",
    "\n",
    "# loop over each forecast file \n",
    "for forecast_file in tqdm(forecast_files, desc=\"processing forecast files\"):\n",
    "    # load the netcdf file\n",
    "    ds = xr.open_dataset(forecast_file, decode_timedelta=True)\n",
    "    \n",
    "    # extract the station name from the filename\n",
    "    station_name = os.path.basename(forecast_file).split(\"_\")[0].lower()\n",
    "\n",
    "    # process only the station that matches the current forecast file\n",
    "    station_row = filtered_station_info[filtered_station_info[\"station name\"].str.lower() == station_name]\n",
    "\n",
    "    if station_row.empty:\n",
    "        print(f\"Skipping {station_name}: no matching station info found.\")\n",
    "        continue  # skip if station is not found in the metadata\n",
    "\n",
    "    station_row = station_row.iloc[0]  # convert to series since we expect only one row\n",
    "    \n",
    "    # extract all ensemble members \n",
    "    ensemble_data = ds['dis24']  # shape: (number, step)\n",
    "    ensemble_members = ensemble_data['number'].values  # extract ensemble member IDs\n",
    "\n",
    "    # extract the forecast issue date from the file and convert to pandas datetime\n",
    "    forecast_issue_ns = ds['time'].values.item()  \n",
    "    forecast_issue_date = pd.to_datetime(forecast_issue_ns, unit='ns')\n",
    "\n",
    "    # define the lead times up to 46 days ahead (lead time = 0 is actually lead time =1 in reality)\n",
    "    lead_times = list(range(0, 15))  # adjust to match desired lead times\n",
    "\n",
    "    # extract individual station thresholds from metadata file\n",
    "    thresholds = {\n",
    "        \"bankfull\": (station_row[\"obs_bankfull\"], station_row[\"glofas_bankfull\"]),\n",
    "        \"moderate\": (station_row[\"obs_moderate\"], station_row[\"glofas_moderate\"]),\n",
    "        \"severe\": (station_row[\"obs_severe\"], station_row[\"glofas_severe\"]),\n",
    "    }\n",
    "        \n",
    "    # process each lead time\n",
    "    for lead_time in lead_times:\n",
    "            # calculate the forecast end date based on lead time (adds one day due to python indexing from zero)\n",
    "            forecast_end_date = forecast_issue_date + pd.DateOffset(days=lead_time + 1)\n",
    "            \n",
    "            # filter observed data for the matching period\n",
    "            observed_period = observed_data[observed_data[\"date\"] == forecast_end_date]\n",
    "            \n",
    "            # skip if no observation data is available for this period\n",
    "            if observed_period.empty:\n",
    "                continue\n",
    "\n",
    "            observed_values = observed_period[station_name].values[0]\n",
    "\n",
    "            # skip if there's no observation data (NaN value) for the specific station\n",
    "            if pd.isnull(observed_values):\n",
    "                continue\n",
    "\n",
    "            # extract forecast values for all ensemble members at the current lead time\n",
    "            forecast_data = ensemble_data.isel(step=lead_time).values.squeeze()  # remove extra dimensions\n",
    "            \n",
    "            # **debug check**: ensure the shape of forecast_data is correct\n",
    "            if forecast_data.ndim != 1 or forecast_data.shape[0] != len(ensemble_members):\n",
    "                raise ValueError(f\"unexpected shape for forecast_data: {forecast_data.shape}. expected ({len(ensemble_members)},).\")\n",
    "\n",
    "            # loop over the thresholds\n",
    "            for severity, (obs_threshold, forecast_threshold) in thresholds.items():\n",
    "                # define events and non-events for each ensemble member\n",
    "                observed_event = observed_values > obs_threshold\n",
    "                forecast_event = forecast_data > forecast_threshold     \n",
    "                \n",
    "                # create events dictionaries for all ensemble members at once\n",
    "                for member_idx, ensemble_member in enumerate(ensemble_members):\n",
    "                    events_dict = {\n",
    "                        \"forecast file\": os.path.basename(forecast_file),\n",
    "                        \"lead time\": lead_time,\n",
    "                        \"station name\": station_name,\n",
    "                        \"ensemble member\": ensemble_member,\n",
    "                        \"forecasted date\": forecast_end_date.date(),\n",
    "                        \"threshold\": severity,\n",
    "                        \"observed event\": observed_event,\n",
    "                        \"forecast event\": bool(forecast_event[member_idx]),  \n",
    "                    }\n",
    "                    # append the events dictionary to the list\n",
    "                    events.append(events_dict)\n",
    "\n",
    "# create a data frame from the list of event dictionaries\n",
    "events_df = pd.DataFrame(events)\n",
    "print(\"processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3b9740e-746b-4005-8798-b904fd40a7ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>forecast file</th>\n",
       "      <th>lead time</th>\n",
       "      <th>station name</th>\n",
       "      <th>ensemble member</th>\n",
       "      <th>forecasted date</th>\n",
       "      <th>threshold</th>\n",
       "      <th>observed event</th>\n",
       "      <th>forecast event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dombe_2003-10-19.nc</td>\n",
       "      <td>0</td>\n",
       "      <td>dombe</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2003-10-20</td>\n",
       "      <td>bankfull</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dombe_2003-10-19.nc</td>\n",
       "      <td>0</td>\n",
       "      <td>dombe</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2003-10-20</td>\n",
       "      <td>bankfull</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dombe_2003-10-19.nc</td>\n",
       "      <td>0</td>\n",
       "      <td>dombe</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2003-10-20</td>\n",
       "      <td>bankfull</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dombe_2003-10-19.nc</td>\n",
       "      <td>0</td>\n",
       "      <td>dombe</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2003-10-20</td>\n",
       "      <td>bankfull</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dombe_2003-10-19.nc</td>\n",
       "      <td>0</td>\n",
       "      <td>dombe</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2003-10-20</td>\n",
       "      <td>bankfull</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         forecast file  lead time station name  ensemble member  \\\n",
       "0  Dombe_2003-10-19.nc          0        dombe              0.0   \n",
       "1  Dombe_2003-10-19.nc          0        dombe              1.0   \n",
       "2  Dombe_2003-10-19.nc          0        dombe              2.0   \n",
       "3  Dombe_2003-10-19.nc          0        dombe              3.0   \n",
       "4  Dombe_2003-10-19.nc          0        dombe              4.0   \n",
       "\n",
       "  forecasted date threshold  observed event  forecast event  \n",
       "0      2003-10-20  bankfull           False           False  \n",
       "1      2003-10-20  bankfull           False           False  \n",
       "2      2003-10-20  bankfull           False           False  \n",
       "3      2003-10-20  bankfull           False           False  \n",
       "4      2003-10-20  bankfull           False           False  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print events_df to check output is as expected \n",
    "events_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4e6c24",
   "metadata": {
    "cell_marker": "####################################################################################"
   },
   "source": [
    "Section 3: Create a function to construct contigency table and skill score metrics\n",
    "\n",
    "In this section we create a function called calculate_metrics which counts the number of hits, misses, false alarms and correct rejections before calculating metrics such as the hit and false alarm rate, critical success index (CSI) and f1 score. These metrics will help us determine how good the forecast is at detecting floods events when compared to the observed or reanalysis datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ace3a15",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# function to calculate verification metrics \n",
    "def calculate_metrics(df):\n",
    "    hits, false_alarms, misses, correct_rejections = {}, {}, {}, {}\n",
    "    hit_rate, false_alarm_rate, csi, f1_score = {}, {}, {}, {}\n",
    "\n",
    "    # loop through all \"trigger\" columns\n",
    "    for column in [col for col in df.columns if 'trigger' in col]:\n",
    "        obs_1, obs_0 = df['observed event'] == 1, df['observed event'] == 0\n",
    "        fcst_1, fcst_0 = df[column] == 1, df[column] == 0\n",
    "\n",
    "        # calculate contingency table elements\n",
    "        hits[column] = (obs_1 & fcst_1).sum()\n",
    "        false_alarms[column] = (obs_0 & fcst_1).sum()\n",
    "        misses[column] = (obs_1 & fcst_0).sum()\n",
    "        correct_rejections[column] = (obs_0 & fcst_0).sum()\n",
    "\n",
    "        # compute verification metrics\n",
    "        total_observed_events = hits[column] + misses[column]\n",
    "        total_forecasted_events = hits[column] + false_alarms[column]\n",
    "\n",
    "        hit_rate[column] = hits[column] / total_observed_events if total_observed_events > 0 else 0\n",
    "        false_alarm_rate[column] = false_alarms[column] / total_forecasted_events if total_forecasted_events > 0 else 0\n",
    "        csi[column] = hits[column] / (hits[column] + false_alarms[column] + misses[column]) if (hits[column] + false_alarms[column] + misses[column]) > 0 else 0\n",
    "        \n",
    "        precision = hits[column] / total_forecasted_events if total_forecasted_events > 0 else 0\n",
    "        recall = hit_rate[column]\n",
    "        f1_score[column] = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    # convert metrics dictionaries into dataframes\n",
    "    metrics_df = pd.concat([\n",
    "        pd.DataFrame(hits, index=['hits']),\n",
    "        pd.DataFrame(false_alarms, index=['false_alarms']),\n",
    "        pd.DataFrame(misses, index=['misses']),\n",
    "        pd.DataFrame(correct_rejections, index=['correct_rejections']),\n",
    "        pd.DataFrame(hit_rate, index=['hit_rate']),\n",
    "        pd.DataFrame(false_alarm_rate, index=['false_alarm_rate']),\n",
    "        pd.DataFrame(csi, index=['csi']),\n",
    "        pd.DataFrame(f1_score, index=['f1_score']),\n",
    "    ])\n",
    "\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfb2f16",
   "metadata": {
    "cell_marker": "############################################################"
   },
   "source": [
    "Section 4: Grouping by lead-time and calculating metrics\n",
    "\n",
    "In this section we begin by pivoting the events_df created in Section 2 so that the ensemble members are displayed as columns before calculating the probability of each forecast by summing the number of 1's and divding by the total number of ensemble members (11 in our case). Once we have the probability of each forecast we remove any forecasts where there is already a flood observed (i.e., observed event = 1 or TRUE) on the first lead time (i.e., lead time = 0). We do this as we do not want to include forecasts in the analysis where there is already a flood occuring or where a flood is imminent. It's important to note here that if a forecast is removed, we remove all lead times associated with that forecast. We only remove the forecast for the threshold where there is an observed event (i.e., if the bankfull threshold is exceeded on lead_time = 0, but not for the moderate threshold then we keep the forecasts for the moderate and severe thresholds only. We do this in order to see if the flood event gets progressively worse. \n",
    "\n",
    "We then filter our events_df based on the lead-times to a particular grouping which balances the need for 2-3 days lead time for anticipatory action along with the limited period in which we have forecast skill (e.g., 2-5 days, 3-5 days, 3-6 days etc.). We then move onto grouping the large events_df by station, threshold and per forecast file into more managable small dataframes. Here, we have for one station, threshold and forecast file a list of probabilities for each of the lead-times and a binary TRUE or FALSE classification in the observed_event column for each lead-time. We now classify events and non-events based on if there is a 1 or TRUE in any of the lead times filtered to for the observations. While for the forecasted events we take the mean of each probability. When this is complete we finish by adding triggers ranging from 0.01 to 1.0 and run the calcualte_metrics function over each dataframe now groupedby station_name and threshold. We groupby these variables so we account for all forecast files for a given station and threshold. This provides us the contigency scores and verification metrics for each station which we evaluate in the final section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2649da7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of forecasts removed: 6417\n",
      "number of unique forecasts removed: 493\n"
     ]
    }
   ],
   "source": [
    "# pivot the events_df to list ensemble members as columns\n",
    "events_df = events_df.pivot_table(index=[\"forecast file\", \"lead time\", \"station name\", \"forecasted date\",\"threshold\", \"observed event\"],\n",
    "                                        columns=\"ensemble member\",)\n",
    "\n",
    "# reset index to convert the pivoted dataframe to a flat table structure\n",
    "events_df.reset_index(inplace=True)\n",
    "\n",
    "# define the columns corresponding to the forecast ensemble members\n",
    "ensemble_member_columns = [col for col in events_df.columns if col[0] == \"forecast event\"]\n",
    "\n",
    "# calculate the percentage of ensemble members with events (i.e., 1's) for each row\n",
    "events_df[\"probability\"] = events_df[ensemble_member_columns].sum(axis=1) / len(ensemble_members)\n",
    "\n",
    "# check if the columns of events_df have a multi index\n",
    "if isinstance(events_df.columns, pd.MultiIndex):\n",
    "    # flatten the multi index into strings\n",
    "    events_df.columns = [' '.join(map(str, col)).strip() for col in events_df.columns]\n",
    "\n",
    "# remove columns with \"forecast event\" in their names\n",
    "events_df = events_df.loc[:, ~events_df.columns.str.contains('forecast event')]\n",
    "\n",
    "# identify forecasts where lead_time = 0 and observed event = true (i.e., where flooding is already occuring)\n",
    "forecasts_to_remove = events_df[(events_df['lead time'] == 0) & (events_df['observed event'])][['forecast file', 'station name', 'threshold']].drop_duplicates()\n",
    "\n",
    "# filter out all rows with the same forecast file, station name, and threshold for any lead time\n",
    "filtered_events_df = events_df[~events_df[['forecast file', 'station name', 'threshold']].apply(tuple, axis=1).isin(forecasts_to_remove.apply(tuple, axis=1))]\n",
    "\n",
    "# get the number of forecasts removed including lead time\n",
    "removed_forecasts_count = len(events_df) - len(filtered_events_df)\n",
    "print(f\"number of forecasts removed: {removed_forecasts_count}\")\n",
    "\n",
    "# get the number of forecasts removed excluding lead time\n",
    "removed_forecasts_unique_count = forecasts_to_remove.drop_duplicates(subset=['forecast file', 'station name', 'threshold']).shape[0]\n",
    "\n",
    "print(f\"number of unique forecasts removed: {removed_forecasts_unique_count}\")\n",
    "\n",
    "# assign back to the original variable\n",
    "events_df = filtered_events_df\n",
    "\n",
    "# filter the dataframe to include specific lead times\n",
    "events_df = events_df[(events_df['lead time'] >=2) & (events_df['lead time'] <=4)]\n",
    "\n",
    "# group by station name, lead time category, and threshold\n",
    "grouped = events_df.groupby(['forecast file','station name','threshold'], observed=False)\n",
    "\n",
    "# create a dictionary to store each group's dataframe\n",
    "grouped_dfs = {name: group for name, group in grouped}\n",
    "\n",
    "# dictionary to store processed data\n",
    "new_grouped_dfs = {}\n",
    "\n",
    "# calculate events and non-events in the lead time period (i.e., flood event if any observed value in period is a 1, take the mean probability for the forecast data)\n",
    "for name, df in grouped_dfs.items():\n",
    "    first_row = df.iloc[0]\n",
    "\n",
    "    new_grouped_dfs[name] = pd.DataFrame({\n",
    "        'forecast file': [first_row['forecast file']],\n",
    "        'station name': [first_row['station name']],\n",
    "        'threshold': [first_row['threshold']],\n",
    "        'observed event': [(df['observed event'] == 1).any()],\n",
    "        'probability': [df['probability'].mean()]\n",
    "    })\n",
    "\n",
    "# combine all resulting dataframe into one \n",
    "final_df = pd.concat(new_grouped_dfs.values(), ignore_index=True)\n",
    "\n",
    "# add trigger thresholds ranging from 1-100% \n",
    "trigger_columns = {}\n",
    "\n",
    "for trigger in np.arange(0.01, 1.01, 0.01): \n",
    "    event_occurrence = (final_df['probability'] >= trigger).astype(int)\n",
    "    trigger_columns[f'trigger{trigger:.2f}'] = event_occurrence\n",
    "\n",
    "# concatanate the new trigger columns to the dataframe\n",
    "final_df = pd.concat([final_df, pd.DataFrame(trigger_columns, index=final_df.index)], axis=1)\n",
    "\n",
    "# group by station name, lead time category, and threshold\n",
    "grouped = final_df.groupby(['station name','threshold'], observed=False)\n",
    "\n",
    "# create a dictionary to store each group's dataframe\n",
    "grouped_dfs = {name: group for name, group in grouped}\n",
    "\n",
    "# iterate through each dataframe in grouped_dfs, apply calculate_metrics, and store the results back into grouped_dfs\n",
    "for key, df in grouped_dfs.items():\n",
    "    grouped_dfs[key] = calculate_metrics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27b35bc2-075b-4279-b856-2ade74df068d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trigger0.01</th>\n",
       "      <th>trigger0.02</th>\n",
       "      <th>trigger0.03</th>\n",
       "      <th>trigger0.04</th>\n",
       "      <th>trigger0.05</th>\n",
       "      <th>trigger0.06</th>\n",
       "      <th>trigger0.07</th>\n",
       "      <th>trigger0.08</th>\n",
       "      <th>trigger0.09</th>\n",
       "      <th>trigger0.10</th>\n",
       "      <th>...</th>\n",
       "      <th>trigger0.91</th>\n",
       "      <th>trigger0.92</th>\n",
       "      <th>trigger0.93</th>\n",
       "      <th>trigger0.94</th>\n",
       "      <th>trigger0.95</th>\n",
       "      <th>trigger0.96</th>\n",
       "      <th>trigger0.97</th>\n",
       "      <th>trigger0.98</th>\n",
       "      <th>trigger0.99</th>\n",
       "      <th>trigger1.00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hits</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>false_alarms</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>misses</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correct_rejections</th>\n",
       "      <td>988.000000</td>\n",
       "      <td>988.000000</td>\n",
       "      <td>988.000000</td>\n",
       "      <td>991.000000</td>\n",
       "      <td>992.000000</td>\n",
       "      <td>992.000000</td>\n",
       "      <td>993.000000</td>\n",
       "      <td>993.000000</td>\n",
       "      <td>993.000000</td>\n",
       "      <td>995.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>1001.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit_rate</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>false_alarm_rate</th>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>csi</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    trigger0.01  trigger0.02  trigger0.03  trigger0.04  \\\n",
       "hits                   5.000000     5.000000     5.000000     5.000000   \n",
       "false_alarms          13.000000    13.000000    13.000000    10.000000   \n",
       "misses                 2.000000     2.000000     2.000000     2.000000   \n",
       "correct_rejections   988.000000   988.000000   988.000000   991.000000   \n",
       "hit_rate               0.714286     0.714286     0.714286     0.714286   \n",
       "false_alarm_rate       0.722222     0.722222     0.722222     0.666667   \n",
       "csi                    0.250000     0.250000     0.250000     0.294118   \n",
       "f1_score               0.400000     0.400000     0.400000     0.454545   \n",
       "\n",
       "                    trigger0.05  trigger0.06  trigger0.07  trigger0.08  \\\n",
       "hits                   5.000000     5.000000     5.000000     5.000000   \n",
       "false_alarms           9.000000     9.000000     8.000000     8.000000   \n",
       "misses                 2.000000     2.000000     2.000000     2.000000   \n",
       "correct_rejections   992.000000   992.000000   993.000000   993.000000   \n",
       "hit_rate               0.714286     0.714286     0.714286     0.714286   \n",
       "false_alarm_rate       0.642857     0.642857     0.615385     0.615385   \n",
       "csi                    0.312500     0.312500     0.333333     0.333333   \n",
       "f1_score               0.476190     0.476190     0.500000     0.500000   \n",
       "\n",
       "                    trigger0.09  trigger0.10  ...  trigger0.91  trigger0.92  \\\n",
       "hits                   5.000000     5.000000  ...          0.0          0.0   \n",
       "false_alarms           8.000000     6.000000  ...          0.0          0.0   \n",
       "misses                 2.000000     2.000000  ...          7.0          7.0   \n",
       "correct_rejections   993.000000   995.000000  ...       1001.0       1001.0   \n",
       "hit_rate               0.714286     0.714286  ...          0.0          0.0   \n",
       "false_alarm_rate       0.615385     0.545455  ...          0.0          0.0   \n",
       "csi                    0.333333     0.384615  ...          0.0          0.0   \n",
       "f1_score               0.500000     0.555556  ...          0.0          0.0   \n",
       "\n",
       "                    trigger0.93  trigger0.94  trigger0.95  trigger0.96  \\\n",
       "hits                        0.0          0.0          0.0          0.0   \n",
       "false_alarms                0.0          0.0          0.0          0.0   \n",
       "misses                      7.0          7.0          7.0          7.0   \n",
       "correct_rejections       1001.0       1001.0       1001.0       1001.0   \n",
       "hit_rate                    0.0          0.0          0.0          0.0   \n",
       "false_alarm_rate            0.0          0.0          0.0          0.0   \n",
       "csi                         0.0          0.0          0.0          0.0   \n",
       "f1_score                    0.0          0.0          0.0          0.0   \n",
       "\n",
       "                    trigger0.97  trigger0.98  trigger0.99  trigger1.00  \n",
       "hits                        0.0          0.0          0.0          0.0  \n",
       "false_alarms                0.0          0.0          0.0          0.0  \n",
       "misses                      7.0          7.0          7.0          7.0  \n",
       "correct_rejections       1001.0       1001.0       1001.0       1001.0  \n",
       "hit_rate                    0.0          0.0          0.0          0.0  \n",
       "false_alarm_rate            0.0          0.0          0.0          0.0  \n",
       "csi                         0.0          0.0          0.0          0.0  \n",
       "f1_score                    0.0          0.0          0.0          0.0  \n",
       "\n",
       "[8 rows x 100 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display an example of one of the grouped_dfs\n",
    "example = grouped_dfs['mocuba','moderate']\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9323b6",
   "metadata": {
    "cell_marker": "################################"
   },
   "source": [
    "Section 5: Trigger selection \n",
    "\n",
    "In the final section we evaluate each of the small dataframes in grouped_dfs and evaluate the performance of GloFAS at each gauging station for each of the three thresholds by looking at each of the percentage triggers. Here for each percentage we look at the contigency metrics and skill scores and identify based on previous forecasts how well or not GloFAS can capture flooding. We then pick the best trigger percentage based primarily on the f1 score which balances the hit and false alarm rates. Ideally we want f1 scores above 0.5 and closer to 1.0. The code is set up to filter triggers where the f1 score is greater than 0.45. After we choose the trigger with the highest f1 value. In the event we have more than one trigger left we decide by choosing the one with the highest hit rate, then the lowest false alarm rate and finally the lowest trigger percentage. \n",
    "\n",
    "To finish we save the list of the best performing triggers to a csv which can be found in our output folder. These will be our triggers for the operational flood AA system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cbe0caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     best_threshold  f1_score  hit_rate false_alarm_rate\n",
      "espungabera bankfull    trigger0.14  0.666667       0.5              0.0\n",
      "            moderate    trigger0.14       0.8  0.666667              0.0\n",
      "            severe      trigger0.10  0.666667       1.0              0.5\n",
      "franca      bankfull    trigger0.01  0.612245  0.555556         0.318182\n",
      "goonda      severe      trigger0.10  0.666667       0.5              0.0\n",
      "gurue       moderate    trigger0.07   0.47619  0.555556         0.583333\n",
      "massangena  moderate    trigger0.04       0.5       1.0         0.666667\n",
      "mocuba      moderate    trigger0.25  0.666667  0.714286            0.375\n",
      "            severe      trigger0.16  0.571429  0.666667              0.5\n"
     ]
    }
   ],
   "source": [
    "# create an empty dictionary to store the best triggers\n",
    "best_triggers = {}\n",
    "\n",
    "# iterate through each dataframe in grouped_dfs\n",
    "for key, df in grouped_dfs.items():\n",
    "    # find the column names corresponding to trigger thresholds (i.e., the percentages)\n",
    "    threshold_columns = [col for col in df.columns if col.startswith('trigger')]\n",
    "\n",
    "    # filter triggers based on the f1 score\n",
    "    filtered_columns = [\n",
    "        col for col in threshold_columns\n",
    "        if df.loc['f1_score', col] >= 0.45\n",
    "    ]\n",
    "    \n",
    "    # if there are any columns left after filtering, identify the maximum f1 score for each threshold (i.e., bankfull, moderate, severe)\n",
    "    if filtered_columns:\n",
    "        max_f1 = df.loc['f1_score', filtered_columns].max()\n",
    "        \n",
    "        # find all thresholds with the maximum f1 score \n",
    "        best_f1_thresholds = df.loc['f1_score', filtered_columns][df.loc['f1_score', filtered_columns] == max_f1].index.tolist()\n",
    "        \n",
    "        # if there are multiple thresholds with the same f1 score, proceed to resolve ties\n",
    "        if len(best_f1_thresholds) > 1:\n",
    "            # resolve ties by choosing the highest hit rate\n",
    "            hit_rates = df.loc['hit_rate', best_f1_thresholds]\n",
    "            max_hit_rate = hit_rates.max()\n",
    "            best_f1_thresholds = hit_rates[hit_rates == max_hit_rate].index.tolist()\n",
    "\n",
    "            # if there are still ties, resolve by choosing the lowest false alarm rate\n",
    "            if len(best_f1_thresholds) > 1:\n",
    "                false_alarm_rates = df.loc['false_alarm_rate', best_f1_thresholds]\n",
    "                min_false_alarm_rate = false_alarm_rates.min()\n",
    "                best_f1_thresholds = false_alarm_rates[false_alarm_rates == min_false_alarm_rate].index.tolist()\n",
    "\n",
    "                # if there are still ties, choose the lowest trigger (threshold)\n",
    "                if len(best_f1_thresholds) > 1:\n",
    "                    best_threshold = min(best_f1_thresholds, key=lambda x: float(x.split('trigger')[1]))  # Sorting by numeric threshold\n",
    "                else:\n",
    "                    best_threshold = best_f1_thresholds[0]\n",
    "            else:\n",
    "                best_threshold = best_f1_thresholds[0]\n",
    "        else:\n",
    "            best_threshold = best_f1_thresholds[0]\n",
    "        \n",
    "        # store the best threshold information\n",
    "        best_triggers[key] = {\n",
    "            'best_threshold': best_threshold,\n",
    "            'f1_score': df.loc['f1_score', best_threshold],\n",
    "            'hit_rate': df.loc['hit_rate', best_threshold],\n",
    "            'false_alarm_rate': df.loc['false_alarm_rate', best_threshold]\n",
    "        }\n",
    "\n",
    "# convert the best_triggers dictionary to a dataframe\n",
    "best_triggers_df = pd.DataFrame(best_triggers).T\n",
    "\n",
    "# print the best triggers\n",
    "print(best_triggers_df)\n",
    "\n",
    "# define the full output file path\n",
    "#output_file = os.path.join(output_directory, \"best_triggers.csv\")\n",
    "\n",
    "# save the results as a CSV file\n",
    "#best_triggers_df.to_csv(output_file, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5008342-dd6a-402b-83fc-c01f179ab97c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "hdc",
   "language": "python",
   "name": "conda-env-hdc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
