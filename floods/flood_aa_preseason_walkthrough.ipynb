{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3549cdc8-4dde-43ea-b719-4f516c17734c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Preseason flood anticipatory action script to process observed/reanalysis and \n",
    "forecast data, calculate contigency metrics and choose the 'best' trigger for \n",
    "operational use based on quality criteria (e.g., f1 score, hit rate and false alarm rates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f64bf4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant packages \n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03603971",
   "metadata": {
    "cell_marker": "#######################################################"
   },
   "source": [
    "Section 1: Define variables, paths and read in data \n",
    "\n",
    "In this section we begin by defining our country of interest, our main working directory and whether we are using observed or reanalysis data. We then define the paths to load the data and perform visual checks to ensure the data is what we are expecting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fa48348-a091-4695-aaae-65aa18fae052",
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'zimbabwe'  # define country of interest\n",
    "directory = '/s3/scratch/jamie.towner/flood_aa'  # define main working directory\n",
    "benchmark = 'observations'  # choose 'observations' or 'glofas_reanalysis' as the benchmark\n",
    "\n",
    "# define paths to data\n",
    "forecast_data_directory = os.path.join(directory, country, \"data/forecasts/glofas_reforecasts\")\n",
    "metadata_directory = os.path.join(directory, country, \"data/metadata\")\n",
    "output_directory = os.path.join(directory, country, \"outputs/triggers\")\n",
    "\n",
    "# create output directory if it does not exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# set observed data and metadata directory and filenames based on benchmark choice\n",
    "if benchmark == 'observations':\n",
    "    observed_data_directory = os.path.join(directory, country, \"data/observations/gauging_stations/all_stations\")\n",
    "    observed_data_file = \"observations.csv\"\n",
    "    station_info_file = \"metadata_observations.csv\"\n",
    "elif benchmark == 'glofas_reanalysis':\n",
    "    observed_data_directory = os.path.join(directory, country, \"data/forecasts/glofas_reanalysis/all_stations\")\n",
    "    observed_data_file = \"glofas_reanalysis_moz.csv\"\n",
    "    station_info_file = 'metadata_glofas_reanalysis.csv'\n",
    "else:\n",
    "    raise ValueError(\"invalid benchmark choice. choose 'observations' or 'glofas_reanalysis'.\")\n",
    "\n",
    "# load the observed or reanalysis data and gauging stations metadata\n",
    "observed_data_path = os.path.join(observed_data_directory, observed_data_file)\n",
    "station_info_path = os.path.join(metadata_directory, station_info_file)\n",
    "\n",
    "observed_data = pd.read_csv(observed_data_path)\n",
    "station_info = pd.read_csv(station_info_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be228527-43db-49c2-800a-1883d30ccec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ruware</th>\n",
       "      <th>malapati</th>\n",
       "      <th>mutirikwi</th>\n",
       "      <th>makwe</th>\n",
       "      <th>runde</th>\n",
       "      <th>beitbridge</th>\n",
       "      <th>manyuchi</th>\n",
       "      <th>kwalu</th>\n",
       "      <th>bangala</th>\n",
       "      <th>ingwesi</th>\n",
       "      <th>tokwane</th>\n",
       "      <th>tokwe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-01-01</td>\n",
       "      <td>1.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>256.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-01-02</td>\n",
       "      <td>1.23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>235.23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003-01-03</td>\n",
       "      <td>1.17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>231.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003-01-04</td>\n",
       "      <td>1.08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>229.51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003-01-05</td>\n",
       "      <td>1.09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>227.82</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  ruware  malapati  mutirikwi  makwe  runde  beitbridge  \\\n",
       "0  2003-01-01    1.21       NaN       0.20    0.0   0.75         0.0   \n",
       "1  2003-01-02    1.23       NaN       0.18    0.0   1.02         0.0   \n",
       "2  2003-01-03    1.17       NaN       0.18    0.0   1.01         0.0   \n",
       "3  2003-01-04    1.08       NaN       0.17    0.0   1.00         0.0   \n",
       "4  2003-01-05    1.09       NaN       0.16    0.0   1.00         0.0   \n",
       "\n",
       "   manyuchi  kwalu  bangala  ingwesi  tokwane  tokwe  \n",
       "0       0.1    NaN   256.02      0.0     1.28   0.53  \n",
       "1       0.1    NaN   235.23      0.0     1.10   0.39  \n",
       "2       0.1    NaN   231.20      0.0     0.98   0.71  \n",
       "3       0.1    NaN   229.51      0.0     1.00   1.53  \n",
       "4       0.1    NaN   227.82      0.0     1.03   1.34  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the observed data \n",
    "observed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e840a37-0cc4-47ce-829e-0943e5a67503",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station id</th>\n",
       "      <th>full station name</th>\n",
       "      <th>station name</th>\n",
       "      <th>river</th>\n",
       "      <th>province</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>obs_bankfull</th>\n",
       "      <th>obs_moderate</th>\n",
       "      <th>obs_severe</th>\n",
       "      <th>glofas_bankfull</th>\n",
       "      <th>glofas_moderate</th>\n",
       "      <th>glofas_severe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B35</td>\n",
       "      <td>BEIT BRIDGE PUMP STATION</td>\n",
       "      <td>beitbridge</td>\n",
       "      <td>Limpopo</td>\n",
       "      <td>Mt South</td>\n",
       "      <td>-22.22</td>\n",
       "      <td>29.98</td>\n",
       "      <td>951.0</td>\n",
       "      <td>2425.0</td>\n",
       "      <td>2755.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>2108.0</td>\n",
       "      <td>2462.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B37</td>\n",
       "      <td>MALAPATI BRIDGE</td>\n",
       "      <td>malapati</td>\n",
       "      <td>Mwenzi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-22.03</td>\n",
       "      <td>31.45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B68</td>\n",
       "      <td>Makwe Dam D/S G/W</td>\n",
       "      <td>makwe</td>\n",
       "      <td>Thuli</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-20.97</td>\n",
       "      <td>28.80</td>\n",
       "      <td>12.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B73</td>\n",
       "      <td>Ingwesi Dam D/S</td>\n",
       "      <td>ingwesi</td>\n",
       "      <td>Ingwesi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-21.07</td>\n",
       "      <td>27.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B79</td>\n",
       "      <td>Manyuchi</td>\n",
       "      <td>manyuchi</td>\n",
       "      <td>Mwenezi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-21.07</td>\n",
       "      <td>30.38</td>\n",
       "      <td>0.7</td>\n",
       "      <td>102.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>803.0</td>\n",
       "      <td>906.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  station id         full station name station name    river  province  \\\n",
       "0        B35  BEIT BRIDGE PUMP STATION   beitbridge  Limpopo  Mt South   \n",
       "1        B37           MALAPATI BRIDGE     malapati   Mwenzi       NaN   \n",
       "2        B68         Makwe Dam D/S G/W        makwe    Thuli       NaN   \n",
       "3        B73           Ingwesi Dam D/S      ingwesi  Ingwesi       NaN   \n",
       "4        B79                  Manyuchi     manyuchi  Mwenezi       NaN   \n",
       "\n",
       "   latitude  longitude  obs_bankfull  obs_moderate  obs_severe  \\\n",
       "0    -22.22      29.98         951.0        2425.0      2755.0   \n",
       "1    -22.03      31.45           NaN           NaN         NaN   \n",
       "2    -20.97      28.80          12.0          18.0        30.0   \n",
       "3    -21.07      27.92           0.0           0.0         3.0   \n",
       "4    -21.07      30.38           0.7         102.0       112.0   \n",
       "\n",
       "   glofas_bankfull  glofas_moderate  glofas_severe  \n",
       "0            232.0           2108.0         2462.0  \n",
       "1              NaN              NaN            NaN  \n",
       "2             63.0             76.0          160.0  \n",
       "3              0.0              0.0           16.0  \n",
       "4             40.0            803.0          906.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the metadata\n",
    "station_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffde08a8-6f9d-4363-a453-811e25329712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the date column in observed_data to pandas timestamps \n",
    "observed_data[\"date\"] = pd.to_datetime(observed_data[\"date\"], format='mixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0bf5c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all GloFAS forecast files (there should be 1052 files per gauging station)\n",
    "forecast_files = glob.glob(os.path.join(forecast_data_directory, '*.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8aabcc4-55c8-4749-a696-9a600ed53633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/s3/scratch/jamie.towner/flood_aa/zimbabwe/data/forecasts/glofas_reforecasts/bangala_2003_03_30.nc',\n",
       " '/s3/scratch/jamie.towner/flood_aa/zimbabwe/data/forecasts/glofas_reforecasts/bangala_2003_10_12.nc',\n",
       " '/s3/scratch/jamie.towner/flood_aa/zimbabwe/data/forecasts/glofas_reforecasts/bangala_2003_03_27.nc',\n",
       " '/s3/scratch/jamie.towner/flood_aa/zimbabwe/data/forecasts/glofas_reforecasts/bangala_2003_10_02.nc',\n",
       " '/s3/scratch/jamie.towner/flood_aa/zimbabwe/data/forecasts/glofas_reforecasts/bangala_2003_10_16.nc',\n",
       " '/s3/scratch/jamie.towner/flood_aa/zimbabwe/data/forecasts/glofas_reforecasts/bangala_2003_10_09.nc',\n",
       " '/s3/scratch/jamie.towner/flood_aa/zimbabwe/data/forecasts/glofas_reforecasts/bangala_2003_10_26.nc',\n",
       " '/s3/scratch/jamie.towner/flood_aa/zimbabwe/data/forecasts/glofas_reforecasts/bangala_2003_10_23.nc',\n",
       " '/s3/scratch/jamie.towner/flood_aa/zimbabwe/data/forecasts/glofas_reforecasts/bangala_2003_10_19.nc',\n",
       " '/s3/scratch/jamie.towner/flood_aa/zimbabwe/data/forecasts/glofas_reforecasts/bangala_2003_10_05.nc']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check forecast files have loaded as expected \n",
    "forecast_files[:10] # check 10 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4700c6e-a83a-494a-84a9-fb99b4753fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "forecast directory: /s3/scratch/jamie.towner/flood_aa/zimbabwe/data/forecasts/glofas_reforecasts\n",
      "observed data directory: /s3/scratch/jamie.towner/flood_aa/zimbabwe/data/observations/gauging_stations/all_stations\n",
      "observed data file: observations.csv\n",
      "metadata directory: /s3/scratch/jamie.towner/flood_aa/zimbabwe/data/metadata\n",
      "output directory: /s3/scratch/jamie.towner/flood_aa/zimbabwe/outputs/triggers\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print data paths to ensure they are set correctly\n",
    "print(f\"\"\"\n",
    "forecast directory: {forecast_data_directory}\n",
    "observed data directory: {observed_data_directory}\n",
    "observed data file: {observed_data_file}\n",
    "metadata directory: {metadata_directory}\n",
    "output directory: {output_directory}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1fb0e2",
   "metadata": {
    "cell_marker": "##############################################################################"
   },
   "source": [
    "Section 2: Process observations and forecasts and define events/non-events \n",
    "\n",
    "In this section we begin by matching the station names in the forecast files from those in the metadata and then process one forecast file at a time. We should have 1052 forecast files per station analysed. To process a station we need the name to be in both the station_info file (i.e., the metadata) and the naming convention of the forecast files. A message will highlight if this is not the case and the code will skip that particular gauging station. We then extract the variable of the forecast data which is river discharge (m3/s) and each ensemble member (we have 11 for GloFAS reforecasts and there will be 51 operationally). After we define the forecast issue date from the forecast netcdf and calculate the forecast end date based on lead time. Remember that due to the indexing of Python from 0 we add 1 to each end date. \n",
    "\n",
    "Looking at the metadata you can see that we have three thresholds for both observations and forecasts. These are bankfull, moderate and severe. The bankfull represents the point at which a gauging station begins to flood and is the readiness phase of the system (i.e., no anticipatory action will take place). The moderate and severe thresholds are based on the 5 and 10 year return periods of the observed data. We then use quantile mapping to map these observed thresholds to the GloFAS reanalysis product over the same time period (2003-2023). This in effect performs a bias correction on the forecasts. \n",
    "\n",
    "Finally, for each threshold we simply identify if each ensemble members of a forecast for each specific lead time exceeds the forecast thresholds and we do the same for the observations. If there is an exceedance we assign a 1 or TRUE and if not we assign a 0 or FALSE. We end this section by creating a dataframe which displays these results called events_df. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "310eb8d3",
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing forecast files:  32%|███▏      | 4004/12624 [19:01<40:57,  3.51it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# loop over each forecast file \u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m forecast_file \u001b[38;5;129;01min\u001b[39;00m tqdm(forecast_files, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessing forecast files\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# load the netcdf file\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforecast_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_timedelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# extract the station name from the filename\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     station_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(forecast_file)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlower()\n",
      "File \u001b[0;32m/envs/shared/hdc/lib/python3.10/site-packages/xarray/backends/api.py:652\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    649\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(backend_kwargs)\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 652\u001b[0m     engine \u001b[38;5;241m=\u001b[39m \u001b[43mplugins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguess_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_array_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    655\u001b[0m     from_array_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/envs/shared/hdc/lib/python3.10/site-packages/xarray/backends/plugins.py:147\u001b[0m, in \u001b[0;36mguess_engine\u001b[0;34m(store_spec)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m engine, backend \u001b[38;5;129;01min\u001b[39;00m engines\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguess_can_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore_spec\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    148\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m engine\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mPermissionError\u001b[39;00m:\n",
      "File \u001b[0;32m/envs/shared/hdc/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:633\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.guess_can_open\u001b[0;34m(self, filename_or_obj)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filename_or_obj, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_remote_uri(filename_or_obj):\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m magic_number \u001b[38;5;241m=\u001b[39m \u001b[43mtry_read_magic_number_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic_number \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;66;03m# netcdf 3 or HDF5\u001b[39;00m\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m magic_number\u001b[38;5;241m.\u001b[39mstartswith((\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCDF\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\211\u001b[39;00m\u001b[38;5;124mHDF\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\032\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/envs/shared/hdc/lib/python3.10/site-packages/xarray/core/utils.py:649\u001b[0m, in \u001b[0;36mtry_read_magic_number_from_path\u001b[0;34m(pathlike, count)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 649\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mread_magic_number_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mFileNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/envs/shared/hdc/lib/python3.10/site-packages/xarray/core/utils.py:630\u001b[0m, in \u001b[0;36mread_magic_number_from_file\u001b[0;34m(filename_or_obj, count)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Finds URLs of the form protocol:// or protocol::\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \n\u001b[1;32m    624\u001b[0m \u001b[38;5;124;03m    This also matches for http[s]://, which were the only remote URLs\u001b[39;00m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;124;03m    supported in <=v0.16.2.\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^[a-z][a-z0-9]*(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m://|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m:)\u001b[39m\u001b[38;5;124m\"\u001b[39m, path))\n\u001b[0;32m--> 630\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_magic_number_from_file\u001b[39m(filename_or_obj, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbytes\u001b[39m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# check byte header to determine file type\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filename_or_obj, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m    633\u001b[0m         magic_number \u001b[38;5;241m=\u001b[39m filename_or_obj[:count]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create an empty list to store events/non-events \n",
    "events = []\n",
    "\n",
    "# filter station_info (i.e., the metadata) to include only stations present in the forecast_files\n",
    "# extract station names from forecast filenames\n",
    "station_names_in_files = [os.path.basename(file).split(\"_\")[0] for file in forecast_files]\n",
    "# get unique station names\n",
    "unique_station_names = list(set(station_names_in_files))\n",
    "# convert station names to lowercase if required\n",
    "filtered_station_info = station_info[station_info[\"station name\"].str.lower().isin([name.lower() for name in unique_station_names])]\n",
    "\n",
    "# loop over each forecast file \n",
    "for forecast_file in tqdm(forecast_files, desc=\"processing forecast files\"):\n",
    "    # load the netcdf file\n",
    "    ds = xr.open_dataset(forecast_file, decode_timedelta=True)\n",
    "    \n",
    "    # extract the station name from the filename\n",
    "    station_name = os.path.basename(forecast_file).split(\"_\")[0].lower()\n",
    "\n",
    "    # process only the station that matches the current forecast file\n",
    "    station_row = filtered_station_info[filtered_station_info[\"station name\"].str.lower() == station_name]\n",
    "\n",
    "    if station_row.empty:\n",
    "        print(f\"Skipping {station_name}: no matching station info found.\")\n",
    "        continue  # skip if station is not found in the metadata\n",
    "\n",
    "    station_row = station_row.iloc[0]  # convert to series since we expect only one row\n",
    "    \n",
    "    # extract all ensemble members \n",
    "    ensemble_data = ds['dis24']  # shape: (number, step)\n",
    "    ensemble_members = ensemble_data['number'].values  # extract ensemble member IDs\n",
    "\n",
    "    # extract the forecast issue date from the file and convert to pandas datetime\n",
    "    forecast_issue_ns = ds['time'].values.item()  \n",
    "    forecast_issue_date = pd.to_datetime(forecast_issue_ns, unit='ns')\n",
    "\n",
    "    # define the lead times up to 46 days ahead (lead time = 0 is actually lead time =1 in reality)\n",
    "    lead_times = list(range(0, 7))  # adjust to match desired lead times\n",
    "\n",
    "    # extract individual station thresholds from metadata file\n",
    "    thresholds = {\n",
    "        \"bankfull\": (station_row[\"obs_bankfull\"], station_row[\"glofas_bankfull\"]),\n",
    "        \"moderate\": (station_row[\"obs_moderate\"], station_row[\"glofas_moderate\"]),\n",
    "        \"severe\": (station_row[\"obs_severe\"], station_row[\"glofas_severe\"]),\n",
    "    }\n",
    "        \n",
    "    # process each lead time\n",
    "    for lead_time in lead_times:\n",
    "            # calculate the forecast end date based on lead time (adds one day due to python indexing from zero)\n",
    "            forecast_end_date = forecast_issue_date + pd.DateOffset(days=lead_time + 1)\n",
    "            \n",
    "            # filter observed data for the matching period\n",
    "            observed_period = observed_data[observed_data[\"date\"] == forecast_end_date]\n",
    "            \n",
    "            # skip if no observation data is available for this period\n",
    "            if observed_period.empty:\n",
    "                continue\n",
    "\n",
    "            observed_values = observed_period[station_name].values[0]\n",
    "\n",
    "            # skip if there's no observation data (NaN value) for the specific station\n",
    "            if pd.isnull(observed_values):\n",
    "                continue\n",
    "\n",
    "            # extract forecast values for all ensemble members at the current lead time\n",
    "            forecast_data = ensemble_data.isel(step=lead_time).values.squeeze()  # remove extra dimensions\n",
    "            \n",
    "            # **debug check**: ensure the shape of forecast_data is correct\n",
    "            if forecast_data.ndim != 1 or forecast_data.shape[0] != len(ensemble_members):\n",
    "                raise ValueError(f\"unexpected shape for forecast_data: {forecast_data.shape}. expected ({len(ensemble_members)},).\")\n",
    "\n",
    "            # loop over the thresholds\n",
    "            for severity, (obs_threshold, forecast_threshold) in thresholds.items():\n",
    "                # define events and non-events for each ensemble member\n",
    "                observed_event = observed_values > obs_threshold\n",
    "                forecast_event = forecast_data > forecast_threshold     \n",
    "                \n",
    "                # create events dictionaries for all ensemble members at once\n",
    "                for member_idx, ensemble_member in enumerate(ensemble_members):\n",
    "                    events_dict = {\n",
    "                        \"forecast file\": os.path.basename(forecast_file),\n",
    "                        \"lead time\": lead_time,\n",
    "                        \"station name\": station_name,\n",
    "                        \"ensemble member\": ensemble_member,\n",
    "                        \"forecasted date\": forecast_end_date.date(),\n",
    "                        \"threshold\": severity,\n",
    "                        \"observed event\": observed_event,\n",
    "                        \"forecast event\": bool(forecast_event[member_idx]),  \n",
    "                    }\n",
    "                    # append the events dictionary to the list\n",
    "                    events.append(events_dict)\n",
    "\n",
    "# create a data frame from the list of event dictionaries\n",
    "events_df = pd.DataFrame(events)\n",
    "print(\"processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4219246-418e-49c6-8eab-b7e99ec0a0a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>forecast file</th>\n",
       "      <th>lead time</th>\n",
       "      <th>station name</th>\n",
       "      <th>ensemble member</th>\n",
       "      <th>forecasted date</th>\n",
       "      <th>threshold</th>\n",
       "      <th>observed event</th>\n",
       "      <th>forecast event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bangala_2003_03_30.nc</td>\n",
       "      <td>0</td>\n",
       "      <td>bangala</td>\n",
       "      <td>0</td>\n",
       "      <td>2003-03-31</td>\n",
       "      <td>bankfull</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bangala_2003_03_30.nc</td>\n",
       "      <td>0</td>\n",
       "      <td>bangala</td>\n",
       "      <td>1</td>\n",
       "      <td>2003-03-31</td>\n",
       "      <td>bankfull</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bangala_2003_03_30.nc</td>\n",
       "      <td>0</td>\n",
       "      <td>bangala</td>\n",
       "      <td>2</td>\n",
       "      <td>2003-03-31</td>\n",
       "      <td>bankfull</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bangala_2003_03_30.nc</td>\n",
       "      <td>0</td>\n",
       "      <td>bangala</td>\n",
       "      <td>3</td>\n",
       "      <td>2003-03-31</td>\n",
       "      <td>bankfull</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bangala_2003_03_30.nc</td>\n",
       "      <td>0</td>\n",
       "      <td>bangala</td>\n",
       "      <td>4</td>\n",
       "      <td>2003-03-31</td>\n",
       "      <td>bankfull</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663426</th>\n",
       "      <td>tokwe_2016_03_30.nc</td>\n",
       "      <td>6</td>\n",
       "      <td>tokwe</td>\n",
       "      <td>6</td>\n",
       "      <td>2016-04-06</td>\n",
       "      <td>severe</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663427</th>\n",
       "      <td>tokwe_2016_03_30.nc</td>\n",
       "      <td>6</td>\n",
       "      <td>tokwe</td>\n",
       "      <td>7</td>\n",
       "      <td>2016-04-06</td>\n",
       "      <td>severe</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663428</th>\n",
       "      <td>tokwe_2016_03_30.nc</td>\n",
       "      <td>6</td>\n",
       "      <td>tokwe</td>\n",
       "      <td>8</td>\n",
       "      <td>2016-04-06</td>\n",
       "      <td>severe</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663429</th>\n",
       "      <td>tokwe_2016_03_30.nc</td>\n",
       "      <td>6</td>\n",
       "      <td>tokwe</td>\n",
       "      <td>9</td>\n",
       "      <td>2016-04-06</td>\n",
       "      <td>severe</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663430</th>\n",
       "      <td>tokwe_2016_03_30.nc</td>\n",
       "      <td>6</td>\n",
       "      <td>tokwe</td>\n",
       "      <td>10</td>\n",
       "      <td>2016-04-06</td>\n",
       "      <td>severe</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1663431 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 forecast file  lead time station name  ensemble member  \\\n",
       "0        bangala_2003_03_30.nc          0      bangala                0   \n",
       "1        bangala_2003_03_30.nc          0      bangala                1   \n",
       "2        bangala_2003_03_30.nc          0      bangala                2   \n",
       "3        bangala_2003_03_30.nc          0      bangala                3   \n",
       "4        bangala_2003_03_30.nc          0      bangala                4   \n",
       "...                        ...        ...          ...              ...   \n",
       "1663426    tokwe_2016_03_30.nc          6        tokwe                6   \n",
       "1663427    tokwe_2016_03_30.nc          6        tokwe                7   \n",
       "1663428    tokwe_2016_03_30.nc          6        tokwe                8   \n",
       "1663429    tokwe_2016_03_30.nc          6        tokwe                9   \n",
       "1663430    tokwe_2016_03_30.nc          6        tokwe               10   \n",
       "\n",
       "        forecasted date threshold  observed event  forecast event  \n",
       "0            2003-03-31  bankfull           False           False  \n",
       "1            2003-03-31  bankfull           False           False  \n",
       "2            2003-03-31  bankfull           False           False  \n",
       "3            2003-03-31  bankfull           False           False  \n",
       "4            2003-03-31  bankfull           False           False  \n",
       "...                 ...       ...             ...             ...  \n",
       "1663426      2016-04-06    severe           False           False  \n",
       "1663427      2016-04-06    severe           False           False  \n",
       "1663428      2016-04-06    severe           False           False  \n",
       "1663429      2016-04-06    severe           False           False  \n",
       "1663430      2016-04-06    severe           False           False  \n",
       "\n",
       "[1663431 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3b9740e-746b-4005-8798-b904fd40a7ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>forecast file</th>\n",
       "      <th>lead time</th>\n",
       "      <th>station name</th>\n",
       "      <th>ensemble member</th>\n",
       "      <th>forecasted date</th>\n",
       "      <th>threshold</th>\n",
       "      <th>observed event</th>\n",
       "      <th>forecast event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bangala_2003_03_30.nc</td>\n",
       "      <td>0</td>\n",
       "      <td>bangala</td>\n",
       "      <td>0</td>\n",
       "      <td>2003-03-31</td>\n",
       "      <td>bankfull</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bangala_2003_03_30.nc</td>\n",
       "      <td>0</td>\n",
       "      <td>bangala</td>\n",
       "      <td>1</td>\n",
       "      <td>2003-03-31</td>\n",
       "      <td>bankfull</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bangala_2003_03_30.nc</td>\n",
       "      <td>0</td>\n",
       "      <td>bangala</td>\n",
       "      <td>2</td>\n",
       "      <td>2003-03-31</td>\n",
       "      <td>bankfull</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bangala_2003_03_30.nc</td>\n",
       "      <td>0</td>\n",
       "      <td>bangala</td>\n",
       "      <td>3</td>\n",
       "      <td>2003-03-31</td>\n",
       "      <td>bankfull</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bangala_2003_03_30.nc</td>\n",
       "      <td>0</td>\n",
       "      <td>bangala</td>\n",
       "      <td>4</td>\n",
       "      <td>2003-03-31</td>\n",
       "      <td>bankfull</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           forecast file  lead time station name  ensemble member  \\\n",
       "0  bangala_2003_03_30.nc          0      bangala                0   \n",
       "1  bangala_2003_03_30.nc          0      bangala                1   \n",
       "2  bangala_2003_03_30.nc          0      bangala                2   \n",
       "3  bangala_2003_03_30.nc          0      bangala                3   \n",
       "4  bangala_2003_03_30.nc          0      bangala                4   \n",
       "\n",
       "  forecasted date threshold  observed event  forecast event  \n",
       "0      2003-03-31  bankfull           False           False  \n",
       "1      2003-03-31  bankfull           False           False  \n",
       "2      2003-03-31  bankfull           False           False  \n",
       "3      2003-03-31  bankfull           False           False  \n",
       "4      2003-03-31  bankfull           False           False  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print events_df to check output is as expected \n",
    "events_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4e6c24",
   "metadata": {
    "cell_marker": "####################################################################################"
   },
   "source": [
    "Section 3: Create a function to construct contigency table and skill score metrics\n",
    "\n",
    "In this section we create a function called calculate_metrics which counts the number of hits, misses, false alarms and correct rejections before calculating metrics such as the hit and false alarm rate, critical success index (CSI) and f1 score. These metrics will help us determine how good the forecast is at detecting floods events when compared to the observed or reanalysis datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ace3a15",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# function to calculate verification metrics \n",
    "def calculate_metrics(df):\n",
    "    hits, false_alarms, misses, correct_rejections = {}, {}, {}, {}\n",
    "    hit_rate, false_alarm_rate, csi, f1_score = {}, {}, {}, {}\n",
    "\n",
    "    # loop through all \"trigger\" columns\n",
    "    for column in [col for col in df.columns if 'trigger' in col]:\n",
    "        obs_1, obs_0 = df['observed event'] == 1, df['observed event'] == 0\n",
    "        fcst_1, fcst_0 = df[column] == 1, df[column] == 0\n",
    "\n",
    "        # calculate contingency table elements\n",
    "        hits[column] = (obs_1 & fcst_1).sum()\n",
    "        false_alarms[column] = (obs_0 & fcst_1).sum()\n",
    "        misses[column] = (obs_1 & fcst_0).sum()\n",
    "        correct_rejections[column] = (obs_0 & fcst_0).sum()\n",
    "\n",
    "        # compute verification metrics\n",
    "        total_observed_events = hits[column] + misses[column]\n",
    "        total_forecasted_events = hits[column] + false_alarms[column]\n",
    "\n",
    "        hit_rate[column] = hits[column] / total_observed_events if total_observed_events > 0 else 0\n",
    "        false_alarm_rate[column] = false_alarms[column] / total_forecasted_events if total_forecasted_events > 0 else 0\n",
    "        csi[column] = hits[column] / (hits[column] + false_alarms[column] + misses[column]) if (hits[column] + false_alarms[column] + misses[column]) > 0 else 0\n",
    "        \n",
    "        precision = hits[column] / total_forecasted_events if total_forecasted_events > 0 else 0\n",
    "        recall = hit_rate[column]\n",
    "        f1_score[column] = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    # convert metrics dictionaries into dataframes\n",
    "    metrics_df = pd.concat([\n",
    "        pd.DataFrame(hits, index=['hits']),\n",
    "        pd.DataFrame(false_alarms, index=['false_alarms']),\n",
    "        pd.DataFrame(misses, index=['misses']),\n",
    "        pd.DataFrame(correct_rejections, index=['correct_rejections']),\n",
    "        pd.DataFrame(hit_rate, index=['hit_rate']),\n",
    "        pd.DataFrame(false_alarm_rate, index=['false_alarm_rate']),\n",
    "        pd.DataFrame(csi, index=['csi']),\n",
    "        pd.DataFrame(f1_score, index=['f1_score']),\n",
    "    ])\n",
    "\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfb2f16",
   "metadata": {
    "cell_marker": "############################################################"
   },
   "source": [
    "Section 4: Grouping by lead-time and calculating metrics\n",
    "\n",
    "In this section we begin by pivoting the events_df created in Section 2 so that the ensemble members are displayed as columns before calculating the probability of each forecast by summing the number of 1's and divding by the total number of ensemble members (11 in our case). Once we have the probability of each forecast we remove any forecasts where there is already a flood observed (i.e., observed event = 1 or TRUE) on the first lead time (i.e., lead time = 0). We do this as we do not want to include forecasts in the analysis where there is already a flood occuring or where a flood is imminent. It's important to note here that if a forecast is removed, we remove all lead times associated with that forecast. We only remove the forecast for the threshold where there is an observed event (i.e., if the bankfull threshold is exceeded on lead_time = 0, but not for the moderate threshold then we keep the forecasts for the moderate and severe thresholds only. We do this in order to see if the flood event gets progressively worse. \n",
    "\n",
    "We then filter our events_df based on the lead-times to a particular grouping which balances the need for 2-3 days lead time for anticipatory action along with the limited period in which we have forecast skill (e.g., 2-5 days, 3-5 days, 3-6 days etc.). We then move onto grouping the large events_df by station, threshold and per forecast file into more managable small dataframes. Here, we have for one station, threshold and forecast file a list of probabilities for each of the lead-times and a binary TRUE or FALSE classification in the observed_event column for each lead-time. We now classify events and non-events based on if there is a 1 or TRUE in any of the lead times filtered to for the observations. While for the forecasted events we take the mean of each probability. When this is complete we finish by adding triggers ranging from 0.01 to 1.0 and run the calcualte_metrics function over each dataframe now groupedby station_name and threshold. We groupby these variables so we account for all forecast files for a given station and threshold. This provides us the contigency scores and verification metrics for each station which we evaluate in the final section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2649da7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of forecasts removed: 2396\n",
      "number of unique forecasts removed: 344\n"
     ]
    }
   ],
   "source": [
    "# pivot the events_df to list ensemble members as columns\n",
    "events_df = events_df.pivot_table(index=[\"forecast file\", \"lead time\", \"station name\", \"forecasted date\",\"threshold\", \"observed event\"],\n",
    "                                        columns=\"ensemble member\",)\n",
    "\n",
    "# reset index to convert the pivoted dataframe to a flat table structure\n",
    "events_df.reset_index(inplace=True)\n",
    "\n",
    "# define the columns corresponding to the forecast ensemble members\n",
    "ensemble_member_columns = [col for col in events_df.columns if col[0] == \"forecast event\"]\n",
    "\n",
    "# calculate the percentage of ensemble members with events (i.e., 1's) for each row\n",
    "events_df[\"probability\"] = events_df[ensemble_member_columns].sum(axis=1) / len(ensemble_members)\n",
    "\n",
    "# check if the columns of events_df have a multi index\n",
    "if isinstance(events_df.columns, pd.MultiIndex):\n",
    "    # flatten the multi index into strings\n",
    "    events_df.columns = [' '.join(map(str, col)).strip() for col in events_df.columns]\n",
    "\n",
    "# remove columns with \"forecast event\" in their names\n",
    "events_df = events_df.loc[:, ~events_df.columns.str.contains('forecast event')]\n",
    "\n",
    "# identify forecasts where lead_time = 0 and observed event = true (i.e., where flooding is already occuring)\n",
    "forecasts_to_remove = events_df[(events_df['lead time'] == 0) & (events_df['observed event'])][['forecast file', 'station name', 'threshold']].drop_duplicates()\n",
    "\n",
    "# filter out all rows with the same forecast file, station name, and threshold for any lead time\n",
    "filtered_events_df = events_df[~events_df[['forecast file', 'station name', 'threshold']].apply(tuple, axis=1).isin(forecasts_to_remove.apply(tuple, axis=1))]\n",
    "\n",
    "# get the number of forecasts removed including lead time\n",
    "removed_forecasts_count = len(events_df) - len(filtered_events_df)\n",
    "print(f\"number of forecasts removed: {removed_forecasts_count}\")\n",
    "\n",
    "# get the number of forecasts removed excluding lead time\n",
    "removed_forecasts_unique_count = forecasts_to_remove.drop_duplicates(subset=['forecast file', 'station name', 'threshold']).shape[0]\n",
    "\n",
    "print(f\"number of unique forecasts removed: {removed_forecasts_unique_count}\")\n",
    "\n",
    "# assign back to the original variable\n",
    "events_df = filtered_events_df\n",
    "\n",
    "# filter the dataframe to include specific lead times\n",
    "events_df = events_df[(events_df['lead time'] >=2) & (events_df['lead time'] <=5)]\n",
    "\n",
    "# group by station name, lead time category, and threshold\n",
    "grouped = events_df.groupby(['forecast file','station name','threshold'], observed=False)\n",
    "\n",
    "# create a dictionary to store each group's dataframe\n",
    "grouped_dfs = {name: group for name, group in grouped}\n",
    "\n",
    "# dictionary to store processed data\n",
    "new_grouped_dfs = {}\n",
    "\n",
    "# calculate events and non-events in the lead time period (i.e., flood event if any observed value in period is a 1, take the mean probability for the forecast data)\n",
    "for name, df in grouped_dfs.items():\n",
    "    first_row = df.iloc[0]\n",
    "\n",
    "    new_grouped_dfs[name] = pd.DataFrame({\n",
    "        'forecast file': [first_row['forecast file']],\n",
    "        'station name': [first_row['station name']],\n",
    "        'threshold': [first_row['threshold']],\n",
    "        'observed event': [(df['observed event'] == 1).any()],\n",
    "        'probability': [df['probability'].mean()]\n",
    "    })\n",
    "\n",
    "# combine all resulting dataframe into one \n",
    "final_df = pd.concat(new_grouped_dfs.values(), ignore_index=True)\n",
    "\n",
    "# add trigger thresholds ranging from 1-100% \n",
    "trigger_columns = {}\n",
    "\n",
    "for trigger in np.arange(0.01, 1.01, 0.01): \n",
    "    event_occurrence = (final_df['probability'] >= trigger).astype(int)\n",
    "    trigger_columns[f'trigger{trigger:.2f}'] = event_occurrence\n",
    "\n",
    "# concatanate the new trigger columns to the dataframe\n",
    "final_df = pd.concat([final_df, pd.DataFrame(trigger_columns, index=final_df.index)], axis=1)\n",
    "\n",
    "# group by station name, lead time category, and threshold\n",
    "grouped = final_df.groupby(['station name','threshold'], observed=False)\n",
    "\n",
    "# create a dictionary to store each group's dataframe\n",
    "grouped_dfs = {name: group for name, group in grouped}\n",
    "\n",
    "# iterate through each dataframe in grouped_dfs, apply calculate_metrics, and store the results back into grouped_dfs\n",
    "for key, df in grouped_dfs.items():\n",
    "    grouped_dfs[key] = calculate_metrics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27b35bc2-075b-4279-b856-2ade74df068d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trigger0.01</th>\n",
       "      <th>trigger0.02</th>\n",
       "      <th>trigger0.03</th>\n",
       "      <th>trigger0.04</th>\n",
       "      <th>trigger0.05</th>\n",
       "      <th>trigger0.06</th>\n",
       "      <th>trigger0.07</th>\n",
       "      <th>trigger0.08</th>\n",
       "      <th>trigger0.09</th>\n",
       "      <th>trigger0.10</th>\n",
       "      <th>...</th>\n",
       "      <th>trigger0.91</th>\n",
       "      <th>trigger0.92</th>\n",
       "      <th>trigger0.93</th>\n",
       "      <th>trigger0.94</th>\n",
       "      <th>trigger0.95</th>\n",
       "      <th>trigger0.96</th>\n",
       "      <th>trigger0.97</th>\n",
       "      <th>trigger0.98</th>\n",
       "      <th>trigger0.99</th>\n",
       "      <th>trigger1.00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hits</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>false_alarms</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>misses</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correct_rejections</th>\n",
       "      <td>678.0</td>\n",
       "      <td>678.0</td>\n",
       "      <td>678.0</td>\n",
       "      <td>678.0</td>\n",
       "      <td>678.0</td>\n",
       "      <td>678.0</td>\n",
       "      <td>678.0</td>\n",
       "      <td>678.0</td>\n",
       "      <td>678.0</td>\n",
       "      <td>680.0</td>\n",
       "      <td>...</td>\n",
       "      <td>680.0</td>\n",
       "      <td>680.0</td>\n",
       "      <td>680.0</td>\n",
       "      <td>680.0</td>\n",
       "      <td>680.0</td>\n",
       "      <td>680.0</td>\n",
       "      <td>680.0</td>\n",
       "      <td>680.0</td>\n",
       "      <td>680.0</td>\n",
       "      <td>680.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit_rate</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>false_alarm_rate</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>csi</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    trigger0.01  trigger0.02  trigger0.03  trigger0.04  \\\n",
       "hits                        0.0          0.0          0.0          0.0   \n",
       "false_alarms                2.0          2.0          2.0          2.0   \n",
       "misses                      2.0          2.0          2.0          2.0   \n",
       "correct_rejections        678.0        678.0        678.0        678.0   \n",
       "hit_rate                    0.0          0.0          0.0          0.0   \n",
       "false_alarm_rate            1.0          1.0          1.0          1.0   \n",
       "csi                         0.0          0.0          0.0          0.0   \n",
       "f1_score                    0.0          0.0          0.0          0.0   \n",
       "\n",
       "                    trigger0.05  trigger0.06  trigger0.07  trigger0.08  \\\n",
       "hits                        0.0          0.0          0.0          0.0   \n",
       "false_alarms                2.0          2.0          2.0          2.0   \n",
       "misses                      2.0          2.0          2.0          2.0   \n",
       "correct_rejections        678.0        678.0        678.0        678.0   \n",
       "hit_rate                    0.0          0.0          0.0          0.0   \n",
       "false_alarm_rate            1.0          1.0          1.0          1.0   \n",
       "csi                         0.0          0.0          0.0          0.0   \n",
       "f1_score                    0.0          0.0          0.0          0.0   \n",
       "\n",
       "                    trigger0.09  trigger0.10  ...  trigger0.91  trigger0.92  \\\n",
       "hits                        0.0          0.0  ...          0.0          0.0   \n",
       "false_alarms                2.0          0.0  ...          0.0          0.0   \n",
       "misses                      2.0          2.0  ...          2.0          2.0   \n",
       "correct_rejections        678.0        680.0  ...        680.0        680.0   \n",
       "hit_rate                    0.0          0.0  ...          0.0          0.0   \n",
       "false_alarm_rate            1.0          0.0  ...          0.0          0.0   \n",
       "csi                         0.0          0.0  ...          0.0          0.0   \n",
       "f1_score                    0.0          0.0  ...          0.0          0.0   \n",
       "\n",
       "                    trigger0.93  trigger0.94  trigger0.95  trigger0.96  \\\n",
       "hits                        0.0          0.0          0.0          0.0   \n",
       "false_alarms                0.0          0.0          0.0          0.0   \n",
       "misses                      2.0          2.0          2.0          2.0   \n",
       "correct_rejections        680.0        680.0        680.0        680.0   \n",
       "hit_rate                    0.0          0.0          0.0          0.0   \n",
       "false_alarm_rate            0.0          0.0          0.0          0.0   \n",
       "csi                         0.0          0.0          0.0          0.0   \n",
       "f1_score                    0.0          0.0          0.0          0.0   \n",
       "\n",
       "                    trigger0.97  trigger0.98  trigger0.99  trigger1.00  \n",
       "hits                        0.0          0.0          0.0          0.0  \n",
       "false_alarms                0.0          0.0          0.0          0.0  \n",
       "misses                      2.0          2.0          2.0          2.0  \n",
       "correct_rejections        680.0        680.0        680.0        680.0  \n",
       "hit_rate                    0.0          0.0          0.0          0.0  \n",
       "false_alarm_rate            0.0          0.0          0.0          0.0  \n",
       "csi                         0.0          0.0          0.0          0.0  \n",
       "f1_score                    0.0          0.0          0.0          0.0  \n",
       "\n",
       "[8 rows x 100 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display an example of one of the grouped_dfs\n",
    "example = grouped_dfs['tokwe','moderate']\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9323b6",
   "metadata": {
    "cell_marker": "################################"
   },
   "source": [
    "Section 5: Trigger selection \n",
    "\n",
    "In the final section we evaluate each of the small dataframes in grouped_dfs and evaluate the performance of GloFAS at each gauging station for each of the three thresholds by looking at each of the percentage triggers. Here for each percentage we look at the contigency metrics and skill scores and identify based on previous forecasts how well or not GloFAS can capture flooding. We then pick the best trigger percentage based primarily on the f1 score which balances the hit and false alarm rates. Ideally we want f1 scores above 0.5 and closer to 1.0. The code is set up to filter triggers where the f1 score is greater than 0.45. After we choose the trigger with the highest f1 value. In the event we have more than one trigger left we decide by choosing the one with the highest hit rate, then the lowest false alarm rate and finally the lowest trigger percentage. \n",
    "\n",
    "To finish we save the list of the best performing triggers to a csv which can be found in our output folder. These will be our triggers for the operational flood AA system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cbe0caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               best_threshold f1_score hit_rate false_alarm_rate\n",
      "ingwesi severe    trigger0.31      0.5      0.5              0.5\n"
     ]
    }
   ],
   "source": [
    "# create an empty dictionary to store the best triggers\n",
    "best_triggers = {}\n",
    "\n",
    "# iterate through each dataframe in grouped_dfs\n",
    "for key, df in grouped_dfs.items():\n",
    "    # find the column names corresponding to trigger thresholds (i.e., the percentages)\n",
    "    threshold_columns = [col for col in df.columns if col.startswith('trigger')]\n",
    "\n",
    "    # filter triggers based on the f1 score\n",
    "    filtered_columns = [\n",
    "        col for col in threshold_columns\n",
    "        if df.loc['f1_score', col] >= 0.45\n",
    "    ]\n",
    "    \n",
    "    # if there are any columns left after filtering, identify the maximum f1 score for each threshold (i.e., bankfull, moderate, severe)\n",
    "    if filtered_columns:\n",
    "        max_f1 = df.loc['f1_score', filtered_columns].max()\n",
    "        \n",
    "        # find all thresholds with the maximum f1 score \n",
    "        best_f1_thresholds = df.loc['f1_score', filtered_columns][df.loc['f1_score', filtered_columns] == max_f1].index.tolist()\n",
    "        \n",
    "        # if there are multiple thresholds with the same f1 score, proceed to resolve ties\n",
    "        if len(best_f1_thresholds) > 1:\n",
    "            # resolve ties by choosing the highest hit rate\n",
    "            hit_rates = df.loc['hit_rate', best_f1_thresholds]\n",
    "            max_hit_rate = hit_rates.max()\n",
    "            best_f1_thresholds = hit_rates[hit_rates == max_hit_rate].index.tolist()\n",
    "\n",
    "            # if there are still ties, resolve by choosing the lowest false alarm rate\n",
    "            if len(best_f1_thresholds) > 1:\n",
    "                false_alarm_rates = df.loc['false_alarm_rate', best_f1_thresholds]\n",
    "                min_false_alarm_rate = false_alarm_rates.min()\n",
    "                best_f1_thresholds = false_alarm_rates[false_alarm_rates == min_false_alarm_rate].index.tolist()\n",
    "\n",
    "                # if there are still ties, choose the lowest trigger (threshold)\n",
    "                if len(best_f1_thresholds) > 1:\n",
    "                    best_threshold = min(best_f1_thresholds, key=lambda x: float(x.split('trigger')[1]))  # Sorting by numeric threshold\n",
    "                else:\n",
    "                    best_threshold = best_f1_thresholds[0]\n",
    "            else:\n",
    "                best_threshold = best_f1_thresholds[0]\n",
    "        else:\n",
    "            best_threshold = best_f1_thresholds[0]\n",
    "        \n",
    "        # store the best threshold information\n",
    "        best_triggers[key] = {\n",
    "            'best_threshold': best_threshold,\n",
    "            'f1_score': df.loc['f1_score', best_threshold],\n",
    "            'hit_rate': df.loc['hit_rate', best_threshold],\n",
    "            'false_alarm_rate': df.loc['false_alarm_rate', best_threshold]\n",
    "        }\n",
    "\n",
    "# convert the best_triggers dictionary to a dataframe\n",
    "best_triggers_df = pd.DataFrame(best_triggers).T\n",
    "\n",
    "# print the best triggers\n",
    "print(best_triggers_df)\n",
    "\n",
    "# Save output as a CSV using country name\n",
    "filename = f\"{country.lower()}_triggers_2025_2026.csv\"\n",
    "best_triggers_df.to_csv(os.path.join(output_directory, filename), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fb4ad9-6a65-4bbb-af3f-24e361c43797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "hdc",
   "language": "python",
   "name": "conda-env-hdc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
