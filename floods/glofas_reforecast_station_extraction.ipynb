{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49878cdb-9641-4730-8b1c-4e055b7ff377",
   "metadata": {},
   "source": [
    "Script to extract GloFAS reforecast data at station locations stored in an s3 bucket. Metadata file is used to identify which station points to extract (use Lisflood x and y coordinates if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a2f3091-fffe-4fca-971d-c7cd953b0962",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import dask\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20bae077-d6b2-4d65-90ac-ed55e3903e20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "country = 'mozambique'  # define country of interest\n",
    "directory = '/s3/scratch/jamie.towner/flood_aa'  # define main working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd13594b-de40-4cfc-b677-236ed8b29e77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the S3 path for the Zarr files\n",
    "store = f\"s3://wfp-seasmon/input/cds/glofas-reforecast/saf/*/*.zarr\"\n",
    "\n",
    "# Set up connection to s3 store\n",
    "s3 = s3fs.S3FileSystem.current()\n",
    "\n",
    "# Define mapper object for multiple files\n",
    "remote_files = s3.glob(store)\n",
    "store = [s3fs.S3Map(root=f\"s3://{file}\", s3=s3, check=False) for file in remote_files]\n",
    "\n",
    "# Configure dask to avoid creating large chunks and open the dataset\n",
    "with dask.config.set(**{\"array.slicing.split_large_chunks\": True}):\n",
    "    ds = xr.open_mfdataset(store, decode_coords=\"all\", engine=\"zarr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b53067d4-f09d-484b-9a6e-640dc57e1cc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Data:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data at point dimensions: ('number', 'time', 'step')\n",
      "Data at point shape: (11, 1052, 46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Data:  10%|â–ˆ         | 1/10 [29:20<4:24:01, 1760.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data at point dimensions: ('number', 'time', 'step')\n",
      "Data at point shape: (11, 1052, 46)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 44\u001b[0m\n\u001b[1;32m     39\u001b[0m steps \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Create a new xarray dataset for storing data in NetCDF format\u001b[39;00m\n\u001b[1;32m     42\u001b[0m station_ds \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mDataset(\n\u001b[1;32m     43\u001b[0m     {\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdis24\u001b[39m\u001b[38;5;124m\"\u001b[39m: ([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[43mdata_at_point\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m)\n\u001b[1;32m     45\u001b[0m     },\n\u001b[1;32m     46\u001b[0m     coords\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m\"\u001b[39m: ensemble_members,\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m: dates,\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m: steps\n\u001b[1;32m     50\u001b[0m     },\n\u001b[1;32m     51\u001b[0m     attrs\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRiver discharge forecasts for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpoint_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munits\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm^3/s\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     54\u001b[0m     }\n\u001b[1;32m     55\u001b[0m )\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Save the dataset as a NetCDF file\u001b[39;00m\n\u001b[1;32m     58\u001b[0m nc_file_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(out_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_forecast.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/envs/user/aa-env/lib/python3.10/site-packages/xarray/core/dataarray.py:801\u001b[0m, in \u001b[0;36mDataArray.values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    790\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    791\u001b[0m \u001b[38;5;124;03m    The array's data converted to numpy.ndarray.\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;124;03m    to this array may be reflected in the DataArray as well.\u001b[39;00m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 801\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\n",
      "File \u001b[0;32m/envs/user/aa-env/lib/python3.10/site-packages/xarray/core/variable.py:541\u001b[0m, in \u001b[0;36mVariable.values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    540\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The variable's data as a numpy.ndarray\"\"\"\u001b[39;00m\n\u001b[0;32m--> 541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_as_array_or_item\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/envs/user/aa-env/lib/python3.10/site-packages/xarray/core/variable.py:339\u001b[0m, in \u001b[0;36m_as_array_or_item\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_as_array_or_item\u001b[39m(data):\n\u001b[1;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the given values as a numpy array, or as an individual item if\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03m    it's a 0d datetime64 or timedelta64 array.\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m    TODO: remove this (replace with np.asarray) once these issues are fixed\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    341\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/envs/user/aa-env/lib/python3.10/site-packages/dask/array/core.py:1694\u001b[0m, in \u001b[0;36mArray.__array__\u001b[0;34m(self, dtype, **kwargs)\u001b[0m\n\u001b[1;32m   1693\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1694\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1695\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m dtype:\n\u001b[1;32m   1696\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mastype(dtype)\n",
      "File \u001b[0;32m/envs/user/aa-env/lib/python3.10/site-packages/dask/base.py:376\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    353\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 376\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/envs/user/aa-env/lib/python3.10/site-packages/dask/base.py:664\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    661\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 664\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m/envs/user/aa-env/lib/python3.10/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/envs/user/aa-env/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load the CSV file containing station information (i.e., station name, lat, lon)\n",
    "# define paths to data\n",
    "metadata_directory = os.path.join(directory, country, \"data/metadata\")\n",
    "station_info_file = \"metadata_observations.csv\"\n",
    "station_info_path = os.path.join(metadata_directory, station_info_file)\n",
    "station_info = pd.read_csv(station_info_path)\n",
    "\n",
    "# Define the output directory\n",
    "out_dir = \"./GloFAS_forecasts_moz\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "# Initialize tqdm progress bar\n",
    "pbar = tqdm(total=len(station_info), desc=\"Extracting Data\")\n",
    "\n",
    "# Loop over each station in the metadata file\n",
    "for index, row in station_info.iterrows():\n",
    "    point_name = row['station name']\n",
    "    latitude = row['lisflood_y']\n",
    "    longitude = row['lisflood_x']\n",
    "\n",
    "    # Sanitize the station name for use in a file path (e.g., remove spaces, special characters)\n",
    "    station_name = \"\".join(c for c in point_name if c.isalnum() or c in (' ', '_')).replace(' ', '_')\n",
    "\n",
    "    # Find the nearest latitude and longitude in the dataset\n",
    "    lat_index = ds['latitude'].sel(latitude=latitude, method='nearest')\n",
    "    lon_index = ds['longitude'].sel(longitude=longitude, method='nearest')\n",
    "\n",
    "    # Extract river discharge data for the nearest point and for all ensemble members\n",
    "    data_at_point = ds['dis24'].sel(latitude=lat_index, longitude=lon_index)\n",
    "\n",
    "    # Print the actual dimensions for diagnostics\n",
    "    print(\"Data at point dimensions:\", data_at_point.dims)\n",
    "    print(\"Data at point shape:\", data_at_point.shape)\n",
    "\n",
    "    # Extract dates, ensemble member, and step\n",
    "    dates = ds.time.values\n",
    "    ensemble_members = ds['dis24'].coords['number'].values\n",
    "    steps = ds.step.values\n",
    "\n",
    "    # Create a new xarray dataset for storing data in NetCDF format\n",
    "    station_ds = xr.Dataset(\n",
    "        {\n",
    "            \"dis24\": ([\"time\", \"number\", \"step\"], data_at_point.values)\n",
    "        },\n",
    "        coords={\n",
    "            \"time\": dates,\n",
    "            \"number\": ensemble_members,\n",
    "            \"step\": steps\n",
    "        },\n",
    "        attrs={\n",
    "            \"description\": f\"River discharge forecasts for {point_name}\",\n",
    "            \"units\": \"m^3/s\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Save the dataset as a NetCDF file\n",
    "    nc_file_name = os.path.join(out_dir, f\"{station_name}_forecast.nc\")\n",
    "    station_ds.to_netcdf(nc_file_name)\n",
    "\n",
    "    # Update the progress bar\n",
    "    pbar.update(1)\n",
    "\n",
    "# Close the progress bar\n",
    "pbar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa-env",
   "language": "python",
   "name": "conda-env-aa-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
