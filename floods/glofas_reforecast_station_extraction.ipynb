{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49878cdb-9641-4730-8b1c-4e055b7ff377",
   "metadata": {},
   "source": [
    "Script to extract GloFAS reforecast data at station locations stored in an s3 bucket. Metadata file is used to identify which station points to extract (use Lisflood x and y coordinates if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a2f3091-fffe-4fca-971d-c7cd953b0962",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import dask\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20bae077-d6b2-4d65-90ac-ed55e3903e20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "country = 'zimbabwe'  # define country of interest\n",
    "directory = '/s3/scratch/jamie.towner/flood_aa'  # define main working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd13594b-de40-4cfc-b677-236ed8b29e77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the S3 path for the Zarr files\n",
    "store = f\"s3://wfp-seasmon/input/cds/glofas-reforecast/saf/*/*.zarr\"\n",
    "\n",
    "# Set up connection to s3 store\n",
    "s3 = s3fs.S3FileSystem.current()\n",
    "\n",
    "# Define mapper object for multiple files\n",
    "remote_files = s3.glob(store)\n",
    "store = [s3fs.S3Map(root=f\"s3://{file}\", s3=s3, check=False) for file in remote_files]\n",
    "\n",
    "# Configure dask to avoid creating large chunks and open the dataset\n",
    "with dask.config.set(**{\"array.slicing.split_large_chunks\": True}):\n",
    "    ds = xr.open_mfdataset(store, decode_coords=\"all\", engine=\"zarr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b53067d4-f09d-484b-9a6e-640dc57e1cc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Data:   8%|▊         | 1/12 [03:23<37:15, 203.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data at point dimensions: ('number', 'time', 'step')\n",
      "Data at point shape: (11, 1052, 46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Data: 100%|██████████| 12/12 [18:38<00:00, 93.23s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file containing station information (i.e., station name, lat, lon)\n",
    "# define paths to data\n",
    "metadata_directory = os.path.join(directory, country, \"data/metadata\")\n",
    "station_info_file = \"zim_metadata.csv\"\n",
    "station_info_path = os.path.join(metadata_directory, station_info_file)\n",
    "station_info = pd.read_csv(station_info_path)\n",
    "\n",
    "# Define the output directory\n",
    "out_dir = \"./glofas_forecasts_zim\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "# Initialize tqdm progress bar\n",
    "pbar = tqdm(total=len(station_info), desc=\"Extracting Data\")\n",
    "\n",
    "# Loop over each station in the metadata file\n",
    "for index, row in station_info.iterrows():\n",
    "    point_name = row['Station Name']\n",
    "    latitude = row['Latitude']\n",
    "    longitude = row['Longitude']\n",
    "\n",
    "    # Sanitize the station name for use in a file path (e.g., remove spaces, special characters)\n",
    "    station_name = \"\".join(c for c in point_name if c.isalnum() or c in (' ', '_')).replace(' ', '_')\n",
    "    \n",
    "    # Define expected output file path\n",
    "    #nc_file_name = os.path.join(out_dir, f\"{station_name}.nc\")\n",
    "\n",
    "    # Skip if file already exists\n",
    "    if os.path.exists(nc_file_name):\n",
    "        print(f\"Skipping {station_name} (already exists)\")\n",
    "        pbar.update(1)\n",
    "        continue\n",
    "\n",
    "    # Find the nearest latitude and longitude in the dataset\n",
    "    lat_index = ds['latitude'].sel(latitude=latitude, method='nearest')\n",
    "    lon_index = ds['longitude'].sel(longitude=longitude, method='nearest')\n",
    "\n",
    "    # Extract river discharge data for the nearest point and for all ensemble members\n",
    "    data_at_point = ds['dis24'].sel(latitude=lat_index, longitude=lon_index)\n",
    "\n",
    "    # Print the actual dimensions for diagnostics\n",
    "    print(\"Data at point dimensions:\", data_at_point.dims)\n",
    "    print(\"Data at point shape:\", data_at_point.shape)\n",
    "\n",
    "    # Extract dates, ensemble member, and step\n",
    "    dates = ds.time.values\n",
    "    ensemble_members = ds['dis24'].coords['number'].values\n",
    "    steps = ds.step.values\n",
    "\n",
    "    # Create a new xarray dataset for storing data in NetCDF format\n",
    "    station_ds = xr.Dataset(\n",
    "        {\n",
    "            \"dis24\": ([\"number\", \"time\", \"step\"], data_at_point.values)\n",
    "        },\n",
    "        coords={\n",
    "            \"number\": ensemble_members,\n",
    "            \"time\": dates,\n",
    "            \"step\": steps\n",
    "        },\n",
    "        attrs={\n",
    "            \"description\": f\"River discharge forecasts for {point_name}\",\n",
    "            \"units\": \"m^3/s\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Save the dataset as a NetCDF file\n",
    "    nc_file_name = os.path.join(out_dir, f\"{station_name}.nc\")\n",
    "    station_ds.to_netcdf(nc_file_name)\n",
    "\n",
    "    # Update the progress bar\n",
    "    pbar.update(1)\n",
    "\n",
    "# Close the progress bar\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c48688d-f481-4dae-a4c1-62abd8e20101",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa-env",
   "language": "python",
   "name": "conda-env-aa-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
