{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amine.barkaoui\\OneDrive - World Food Programme\\Documents\\GitHub\\anticipatory-action\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amine.barkaoui\\AppData\\Local\\miniconda3\\envs\\aa-env\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "%cd ../\n",
    "import logging\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from hip.analysis.analyses.drought import get_accumulation_periods\n",
    "\n",
    "from config.params import Params\n",
    "from dask.distributed import Client\n",
    "from triggers import read_aggregated_obs, read_aggregated_probs, merge_un_biased_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:distributed.http.proxy:To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "INFO:distributed.scheduler:State start\n",
      "INFO:distributed.diskutils:Found stale lock file and directory 'C:\\\\Users\\\\AMINE~1.BAR\\\\AppData\\\\Local\\\\Temp\\\\dask-scratch-space\\\\scheduler-yqghn7_w', purging\n",
      "INFO:distributed.diskutils:Found stale lock file and directory 'C:\\\\Users\\\\AMINE~1.BAR\\\\AppData\\\\Local\\\\Temp\\\\dask-scratch-space\\\\worker-1bj2gv2m', purging\n",
      "INFO:distributed.diskutils:Found stale lock file and directory 'C:\\\\Users\\\\AMINE~1.BAR\\\\AppData\\\\Local\\\\Temp\\\\dask-scratch-space\\\\worker-cabhbe8i', purging\n",
      "INFO:distributed.diskutils:Found stale lock file and directory 'C:\\\\Users\\\\AMINE~1.BAR\\\\AppData\\\\Local\\\\Temp\\\\dask-scratch-space\\\\worker-i8wye5b1', purging\n",
      "INFO:distributed.diskutils:Found stale lock file and directory 'C:\\\\Users\\\\AMINE~1.BAR\\\\AppData\\\\Local\\\\Temp\\\\dask-scratch-space\\\\worker-y3kctyac', purging\n",
      "INFO:distributed.scheduler:  Scheduler at:     tcp://127.0.0.1:52660\n",
      "INFO:distributed.scheduler:  dashboard at:  http://127.0.0.1:8787/status\n",
      "INFO:distributed.scheduler:Registering Worker plugin shuffle\n",
      "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:52665'\n",
      "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:52664'\n",
      "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:52663'\n",
      "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:52666'\n",
      "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:52675', name: 1, status: init, memory: 0, processing: 0>\n",
      "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:52675\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:52683\n",
      "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:52681', name: 2, status: init, memory: 0, processing: 0>\n",
      "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:52681\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:52688\n",
      "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:52680', name: 0, status: init, memory: 0, processing: 0>\n",
      "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:52680\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:52686\n",
      "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:52684', name: 3, status: init, memory: 0, processing: 0>\n",
      "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:52684\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:52690\n",
      "INFO:distributed.scheduler:Receive client connection: Client-48717d45-bb88-11ee-b650-78af08c80c61\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:52693\n"
     ]
    }
   ],
   "source": [
    "client = Client()\n",
    "\n",
    "params = Params(iso='MOZ', index='SPI')\n",
    "\n",
    "rfh = xr.DataArray(\n",
    "    np.arange(1, 9),\n",
    "    coords=dict(\n",
    "        time=(\n",
    "            [\"time\"],\n",
    "            pd.date_range(\n",
    "                f\"{params.start_season}/1/1990\",\n",
    "                f\"{params.end_season + 1}/28/1991\",\n",
    "                freq=\"M\",\n",
    "            ),\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "periods = get_accumulation_periods(\n",
    "    rfh, 0, 0, params.min_index_period, params.max_index_period\n",
    ")\n",
    "\n",
    "obs = read_aggregated_obs(\n",
    "    f\"data/{params.iso}/outputs/zarr/obs/2022\",\n",
    "    params,\n",
    ")\n",
    "obs = obs.assign_coords(\n",
    "    lead_time=(\"index\", [periods[i.split(\" \")[-1]][0] for i in obs.index.values])\n",
    ")\n",
    "obs = obs.assign_coords(\n",
    "    vulnerability=(\n",
    "        \"district\",\n",
    "        [params.districts_vulnerability[d] for d in obs.district.values],\n",
    "    )\n",
    ")\n",
    "logging.info(\n",
    "    f\"Completed reading of aggregated observations for the whole {params.iso} country\"\n",
    ")\n",
    "\n",
    "probs_ds = read_aggregated_probs(\n",
    "    f\"data/{params.iso}/outputs/zarr/2022\",\n",
    "    params,\n",
    ")\n",
    "probs = xr.concat(\n",
    "    [\n",
    "        merge_un_biased_probs(probs_ds, probs_ds, params, i.split(\" \")[-1])\n",
    "        for i in probs_ds.index.values\n",
    "    ],\n",
    "    dim=\"index\",\n",
    ")\n",
    "logging.info(\n",
    "    f\"Completed reading of aggregated probabilities for the whole {params.iso} country\"\n",
    ")\n",
    "\n",
    "# Filter year/time dimension: temporary before harmonization with analytical script\n",
    "obs = obs.sel(year=probs.year.values).load()\n",
    "obs = obs.sel(time=probs.year.values).load()\n",
    "\n",
    "# Trick to align couples of issue months inside apply_ufunc\n",
    "probs_ready = probs.sel(\n",
    "    issue=np.uint8(params.issue)[:-1]\n",
    ").load()  # use start/end season here\n",
    "probs_set = probs.sel(issue=np.uint8(params.issue)[1:]).load()\n",
    "probs_set[\"issue\"] = [i - 1 if i != 1 else 12 for i in probs_set.issue.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numba-optimized way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba.core import types\n",
    "from numba.typed import Dict\n",
    "\n",
    "# Define some constants\n",
    "# The Dict.empty() constructs a typed dictionary.\n",
    "TOLERANCE = Dict.empty(\n",
    "    key_type=types.unicode_type,\n",
    "    value_type=types.f8,\n",
    ")\n",
    "TOLERANCE['Leve'] = 0; TOLERANCE['Moderado'] = -0.44; TOLERANCE['Severo'] = -0.68\n",
    "\n",
    "GENERAL_T = Dict.empty(\n",
    "    key_type=types.unicode_type,\n",
    "    value_type=types.f8,\n",
    ")\n",
    "GENERAL_T['HR'] = 0.5; GENERAL_T['SR'] = 0.65; GENERAL_T['FR'] = 0.35; GENERAL_T['RP'] = 4.\n",
    "\n",
    "NON_REGRET_T = Dict.empty(\n",
    "    key_type=types.unicode_type,\n",
    "    value_type=types.f8,\n",
    ")\n",
    "NON_REGRET_T['HR']=0.65; NON_REGRET_T['SR']=0.55; NON_REGRET_T['FR']=0.45; NON_REGRET_T['RP'] = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "\n",
    "@jit(nopython=True, cache=True)\n",
    "def _compute_confusion_matrix(true, pred):\n",
    "  '''\n",
    "  Computes a confusion matrix using numpy for two np.arrays\n",
    "  true and pred.\n",
    "\n",
    "  Results are identical (and similar in computation time) to: \n",
    "    \"from sklearn.metrics import confusion_matrix\"\n",
    "\n",
    "  However, this function avoids the dependency on sklearn and \n",
    "  allows to use numba in nopython mode.\n",
    "  '''\n",
    "\n",
    "  K = len(np.unique(true)) # Number of classes \n",
    "  result = np.zeros((K, K))\n",
    "\n",
    "  for i in range(len(true)):\n",
    "    result[true[i]][pred[i]] += 1\n",
    "\n",
    "  return result\n",
    "\n",
    "@jit(\n",
    "    nopython=True, \n",
    "    cache=True,\n",
    ")\n",
    "def objective_numba(\n",
    "    t,\n",
    "    obs_val,\n",
    "    obs_bool,\n",
    "    prob_issue0,\n",
    "    prob_issue1,\n",
    "    leadtime,\n",
    "    issue,\n",
    "    category,\n",
    "    vulnerability,\n",
    "    tolerance,\n",
    "    general_req,\n",
    "    non_regret_req,\n",
    "    end_season=5,\n",
    "    penalty=1e6,\n",
    "    alpha=10e-3,\n",
    "    sorting=False,\n",
    "):\n",
    "    if leadtime <= end_season:\n",
    "        obs_val = obs_val[1:]\n",
    "        obs_bool = obs_bool[1:]\n",
    "        prob_issue0 = prob_issue0[:-1]\n",
    "        prob_issue1 = prob_issue1[:-1]\n",
    "    \n",
    "    prediction = np.logical_and(prob_issue0 > t[0], prob_issue1 > t[1]).astype(np.int16)\n",
    "\n",
    "    cm = _compute_confusion_matrix(obs_bool, prediction)\n",
    "    _, false, fn, hits = cm.astype(np.int16).ravel()\n",
    "\n",
    "    number_actions = np.sum(prediction)\n",
    "\n",
    "    if hits + false == 0: # avoid divisions by zero\n",
    "       return [penalty]\n",
    "    \n",
    "    far = false / (false + hits)\n",
    "    false_tol = np.sum(prediction & (obs_val > tolerance[category]))\n",
    "    hit_rate = hits / (hits + fn)\n",
    "    success_rate = hits + false - false_tol\n",
    "    failure_rate = false_tol\n",
    "    \n",
    "    freq = number_actions / len(obs_val)\n",
    "    return_period = np.round(1 / freq if freq != 0 else 0, 0)\n",
    "    \n",
    "    requirements = general_req if vulnerability == \"GT\" else non_regret_req\n",
    "    req_RP = requirements['RP'] + 1 * (category[0]=='M') + 3 * (category[0]=='S')\n",
    "    \n",
    "    constraints = np.array([\n",
    "        hit_rate >= requirements[\"HR\"],\n",
    "        success_rate >= (requirements[\"SR\"] * number_actions),\n",
    "        failure_rate <= (requirements[\"FR\"] * number_actions),\n",
    "        return_period >= req_RP,\n",
    "        (leadtime - (issue + 1)) % 12 > 1,\n",
    "    ]).astype(np.int16)\n",
    "    \n",
    "    if sorting:\n",
    "        return [-hit_rate, failure_rate / number_actions]\n",
    "    else:\n",
    "      if np.all(constraints):\n",
    "          return [-hit_rate + alpha * far]\n",
    "      else:\n",
    "          return [penalty]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def _make_grid(arraylist):\n",
    "    n = len(arraylist)\n",
    "    k = arraylist[0].shape[0]\n",
    "    a2d = np.zeros((n, k, k))\n",
    "    for i in range(n):\n",
    "        a2d[i] = arraylist[i]\n",
    "    return(a2d)\n",
    "\n",
    "@jit(nopython=True)\n",
    "def _meshxy(x, y):\n",
    "    xx = np.empty(shape=(x.size, y.size), dtype=x.dtype)\n",
    "    yy = np.empty(shape=(x.size, y.size), dtype=y.dtype)\n",
    "    for i in range(y.size):\n",
    "        for j in range(x.size):\n",
    "            xx[i,j] = x[i]  # change to x[j] if indexing xy\n",
    "            yy[i,j] = y[j]  # change to y[i] if indexing xy\n",
    "    return xx, yy\n",
    "\n",
    "@jit(nopython=True)\n",
    "def brute_numba(func, ranges, args=()):\n",
    "    \"\"\"\n",
    "    Numba-compatible implementation of scipy.optimize.brute designed only for 2d minimizations. \n",
    "    Minimize a function over a given range by brute force.\n",
    "\n",
    "    Uses the “brute force” method, i.e., computes the function's value at each point of a \n",
    "    multidimensional grid of points, to find the global minimum of the function.\n",
    "\n",
    "    Args:\n",
    "        func: callable, objective function to be minimized. Must be in the form f(x, *args),\n",
    "                        where x is the argument in the form of a 1-D array and args is a tuple \n",
    "                        of any additional fixed parameters needed to completely specify the \n",
    "                        function.\n",
    "        ranges: tuple,  each component of the ranges tuple must be a numpy.array. The program uses\n",
    "                        these to create the grid of points on which the objective function will be\n",
    "                        computed.\n",
    "        args: tuple, optional, any additional fixed parameters needed to completely specify the \n",
    "                        function.\n",
    "    Returns:\n",
    "        xmin: numpy.ndarray, a 1-D array containing the coordinates of a point at which the \n",
    "                        objective function had its minimum value.\n",
    "        Jmin: float, function values at the point xmin.\n",
    "    \"\"\"\n",
    "    assert len(ranges) == 2\n",
    "    \n",
    "    x, y = _meshxy(*ranges)\n",
    "    grid = _make_grid([x, y])\n",
    "    \n",
    "    # obtain an array of parameters that is iterable by a map-like callable\n",
    "    inpt_shape = np.array(grid.shape)\n",
    "    grid = np.reshape(grid, (inpt_shape[0], np.prod(inpt_shape[1:]))).T\n",
    "    \n",
    "    # iterate over input arrays\n",
    "    Jout = np.array([\n",
    "        func(np.asarray(candidate).flatten(), *args)[0]\n",
    "        for candidate in grid\n",
    "    ])\n",
    "    \n",
    "    indx = np.argmin(Jout)\n",
    "\n",
    "    Jout = np.reshape(Jout, (inpt_shape[1], inpt_shape[2]))\n",
    "    grid = np.reshape(grid.T, (inpt_shape[0], inpt_shape[1], inpt_shape[2]))\n",
    "    \n",
    "    Nshape = np.shape(Jout)\n",
    "    Nindx = np.empty(2, dtype=np.uint8)    \n",
    "    Nindx[1] = indx % Nshape[1]\n",
    "    indx = indx // Nshape[1]\n",
    "    Nindx[0] = indx % Nshape[0]\n",
    "    indx = indx // Nshape[0]\n",
    "    \n",
    "    xmin = np.array([grid[k][Nindx[0], Nindx[1]] for k in range(2)])\n",
    "\n",
    "    Jmin = Jout[Nindx[0], Nindx[1]]\n",
    "\n",
    "    return xmin, Jmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_triggers_numba(\n",
    "    observations_bool,\n",
    "    observations_val,\n",
    "    prob_ready,\n",
    "    prob_set,\n",
    "    lead_time,\n",
    "    issue,\n",
    "    category,\n",
    "    vulnerability,\n",
    "):\n",
    "    \"\"\"\n",
    "    Find the optimal triggers pair by evaluating the objective function on each couple of \n",
    "    values of a 100 * 100 grid and selecting the minimizer.\n",
    "\n",
    "    Args:\n",
    "        observations_bool: np.array, time series of categorical observations for the \n",
    "                        specified category\n",
    "        observations_val: np.array, time series of the observed rainfall values (or SPI) \n",
    "        prob_ready: np.array, time series of forecasts probabilities for the ready month\n",
    "        prob_ready: np.array, time series of forecasts probabilities for the set month\n",
    "        lead_time: int, lead time month\n",
    "        issue: int, issue month\n",
    "        category: str, intensity level\n",
    "        vulnerability: str, should be either 'GT' for General Triggers or 'NRT' for 'Non-\n",
    "                        Regret Triggers'\n",
    "    Returns:\n",
    "        best_triggers: np.array, array of size 2 containing best triggers for Ready / Set\n",
    "        best_score: int, score (mainly hit rate) corresponding to the best triggers\n",
    "    \"\"\"\n",
    "\n",
    "    # Define grid\n",
    "    threshold_range = (0.0, 1.0)\n",
    "    grid = (\n",
    "        np.arange(threshold_range[0], threshold_range[1], step=0.01),\n",
    "        np.arange(threshold_range[0], threshold_range[1], step=0.01),\n",
    "    )\n",
    "    \n",
    "    # Launch research\n",
    "    best_triggers, best_score = brute_numba(\n",
    "        objective_numba,\n",
    "        grid,\n",
    "        args=(\n",
    "            observations_val,\n",
    "            observations_bool,\n",
    "            prob_ready,\n",
    "            prob_set,\n",
    "            lead_time,\n",
    "            issue,\n",
    "            category,\n",
    "            vulnerability,\n",
    "            TOLERANCE,\n",
    "            GENERAL_T,\n",
    "            NON_REGRET_T,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return best_triggers, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 25s\n",
      "Wall time: 2min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Distribute computation of triggers\n",
    "trigs, score = xr.apply_ufunc(\n",
    "    find_optimal_triggers_numba,\n",
    "    obs.bool,\n",
    "    obs.val,\n",
    "    probs_ready.prob,\n",
    "    probs_set.prob,\n",
    "    obs.lead_time,\n",
    "    probs.issue,\n",
    "    obs.category,\n",
    "    obs.vulnerability,\n",
    "    vectorize=True,\n",
    "    join=\"outer\",\n",
    "    input_core_dims=[[\"year\"], [\"time\"], [\"year\"], [\"year\"], [], [], [], []],\n",
    "    output_core_dims=[[\"trigger\"], []],\n",
    "    dask=\"parallelized\",\n",
    "    keep_attrs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation of numba implementation: comparison with previous Python results (older version of objective function and scipy brute function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigs_ref = xr.open_zarr('data/MOZ/outputs/Plots/triggers_spi_2022_GT.zarr').bool\n",
    "score_ref = xr.open_zarr('data/MOZ/outputs/Plots/score_spi_2022_GT.zarr').bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.testing.assert_equal(trigs_ref, trigs.assign_coords({'trigger': trigs_ref.trigger.values}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.testing.assert_equal(score_ref, score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
